(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias', 'quantizer.codevectors', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 16170
  Number of trainable parameters = 311286969
  0%|                                                                                                                                               | 0/16170 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 18.2935, 'learning_rate': 4.2e-06, 'epoch': 0.01}                                                                                                                    
{'loss': 25.7754, 'learning_rate': 1.02e-05, 'epoch': 0.01}                                                                                                                   
{'loss': 30.6869, 'learning_rate': 1.6199999999999997e-05, 'epoch': 0.02}                                                                                                     
{'loss': 33.3907, 'learning_rate': 2.1599999999999996e-05, 'epoch': 0.02}                                                                                                     
{'loss': 29.8809, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.03}                                                                                                     
{'loss': 15.9997, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.04}                                                                                                     
{'loss': 18.2285, 'learning_rate': 3.9e-05, 'epoch': 0.04}                                                                                                                    
{'loss': 14.5987, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.05}                                                                                                     
{'loss': 10.1298, 'learning_rate': 5.1e-05, 'epoch': 0.06}                                                                                                                    
{'loss': 7.8712, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.06}                                                                                                      
  1%|‚ñä                                                                                                                                  | 100/16170 [01:14<2:00:16,  2.23it/s]***** Running Evaluation *****
  Num examples = 9578
  Batch size = 8

Traceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 1125/1198 [04:56<00:27,  2.67it/s]
  File "/home/or/Desktop/wav2vec2/main.py", line 316, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.70 GiB (GPU 0; 15.75 GiB total capacity; 8.91 GiB already allocated; 819.81 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  1%|‚ñä                                                                                                                                 | 100/16170 [06:11<16:35:35,  3.72s/it]

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.weight', 'project_hid.bias', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 16170
  Number of trainable parameters = 311286969
  0%|                                                                                                                                               | 0/16170 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 18.3519, 'learning_rate': 5.399999999999999e-06, 'epoch': 0.01}                                                                                                      
{'loss': 25.9906, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.01}                                                                                                     
{'loss': 30.8717, 'learning_rate': 1.68e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 33.6851, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.02}                                                                                                     
{'loss': 29.905, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.03}                                                                                                      
{'loss': 15.8954, 'learning_rate': 3.36e-05, 'epoch': 0.04}                                                                                                                   
{'loss': 18.1031, 'learning_rate': 3.96e-05, 'epoch': 0.04}                                                                                                                   
{'loss': 15.0147, 'learning_rate': 4.56e-05, 'epoch': 0.05}                                                                                                                   
{'loss': 10.8393, 'learning_rate': 5.1599999999999994e-05, 'epoch': 0.06}                                                                                                     
{'loss': 8.5095, 'learning_rate': 5.76e-05, 'epoch': 0.06}                                                                                                                    
  1%|‚ñä                                                                                                                                  | 100/16170 [01:12<1:54:50,  2.33it/s]***** Running Evaluation *****
  Num examples = 9578
  Batch size = 8

Traceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ        | 1125/1198 [04:52<00:26,  2.71it/s]
  File "/home/or/Desktop/wav2vec2/main.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.70 GiB (GPU 0; 15.75 GiB total capacity; 8.91 GiB already allocated; 819.81 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  1%|‚ñä                                                                                                                                 | 100/16170 [06:05<16:19:42,  3.66s/it]

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.codevectors', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 2
  Total optimization steps = 32330
  Number of trainable parameters = 311286969
  0%|                                                                                                                                               | 0/32330 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 19.1922, 'learning_rate': 4.8e-06, 'epoch': 0.0}                                                                                                                     
{'loss': 27.9827, 'learning_rate': 1.02e-05, 'epoch': 0.01}                                                                                                                   
{'loss': 31.4161, 'learning_rate': 1.6199999999999997e-05, 'epoch': 0.01}                                                                                                     
{'loss': 32.1499, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.01}                                                                                                     
{'loss': 30.9676, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.02}                                                                                                     
{'loss': 15.8903, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.02}                                                                                                     
{'loss': 18.9516, 'learning_rate': 3.9e-05, 'epoch': 0.02}                                                                                                                    
{'loss': 18.5124, 'learning_rate': 4.4399999999999995e-05, 'epoch': 0.02}                                                                                                     
{'loss': 14.5135, 'learning_rate': 5.04e-05, 'epoch': 0.03}                                                                                                                   
{'loss': 9.4053, 'learning_rate': 5.6399999999999995e-05, 'epoch': 0.03}                                                                                                      
  0%|‚ñç                                                                                                                                  | 100/32330 [00:46<2:40:53,  3.34it/s]***** Running Evaluation *****
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 559.0574, 'eval_samples_per_second': 17.132, 'eval_steps_per_second': 2.143, 'epoch': 0.03}                                                                                                                                                              
  0%|‚ñç                                                                                                                                  | 100/32330 [10:05<2:40:53,  3.34it/s]
Saving model checkpoint to ./turkish_clean/checkpoint-100                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-100/config.json
Model weights saved in ./turkish_clean/checkpoint-100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-100/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 5.1125, 'learning_rate': 6.239999999999999e-05, 'epoch': 0.03}                                                                                                       
{'loss': 5.1383, 'learning_rate': 6.84e-05, 'epoch': 0.04}                                                                                                                    
  0%|‚ñç                                                                                                                                  | 120/32330 [10:18<5:32:24,  1.62it/s]^CTraceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1749, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2508, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2540, in compute_loss
    outputs = model(**inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1693, in forward
    if labels.max() >= self.config.vocab_size:
KeyboardInterrupt
  0%|‚ñç                                                                                                                                 | 120/32330 [10:19<46:09:51,  5.16s/it]

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight', 'project_hid.bias', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-100.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 2
  Total optimization steps = 32330
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 100
  Will skip the first 0 epochs then the first 200 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/200 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:10<00:00, 19.66it/s]
{'loss': 5.1137, 'learning_rate': 6.239999999999999e-05, 'epoch': 0.03}
{'loss': 5.141, 'learning_rate': 6.84e-05, 'epoch': 0.04}                                                                                                                     
{'loss': 4.5527, 'learning_rate': 7.439999999999999e-05, 'epoch': 0.04}                                                                                                       
{'loss': 4.0949, 'learning_rate': 8.04e-05, 'epoch': 0.04}                                                                                                                    
{'loss': 4.0124, 'learning_rate': 8.639999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 3.5102, 'learning_rate': 9.24e-05, 'epoch': 0.05}                                                                                                                    
{'loss': 3.6384, 'learning_rate': 9.839999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 3.4719, 'learning_rate': 0.00010439999999999999, 'epoch': 0.06}                                                                                                      
{'loss': 3.3705, 'learning_rate': 0.00011039999999999999, 'epoch': 0.06}                                                                                                      
{'loss': 3.3643, 'learning_rate': 0.0001164, 'epoch': 0.06}                                                                                                                   
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 200/32330 [00:54<2:44:31,  3.25it/s]
  Num examples = 9578
  Batch size = 8
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1193/1198 [05:18<00:01,  2.73it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.93 GiB (GPU 0; 15.75 GiB total capacity; 9.16 GiB already allocated; 3.93 GiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  1%|‚ñä                                                                                                                                 | 200/32330 [06:13<16:40:35,  1.87s/it]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'project_hid.bias', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-100.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 2
  Total optimization steps = 32330
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 100
  Will skip the first 0 epochs then the first 200 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/200 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:10<00:00, 19.97it/s]
{'loss': 5.1137, 'learning_rate': 6.239999999999999e-05, 'epoch': 0.03}
{'loss': 5.141, 'learning_rate': 6.84e-05, 'epoch': 0.04}                                                                                                                     
{'loss': 4.5527, 'learning_rate': 7.439999999999999e-05, 'epoch': 0.04}                                                                                                       
{'loss': 4.0949, 'learning_rate': 8.04e-05, 'epoch': 0.04}                                                                                                                    
{'loss': 4.0124, 'learning_rate': 8.639999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 3.5102, 'learning_rate': 9.24e-05, 'epoch': 0.05}                                                                                                                    
{'loss': 3.638, 'learning_rate': 9.839999999999999e-05, 'epoch': 0.05}                                                                                                        
{'loss': 3.4719, 'learning_rate': 0.00010439999999999999, 'epoch': 0.06}                                                                                                      
{'loss': 3.371, 'learning_rate': 0.00011039999999999999, 'epoch': 0.06}                                                                                                       
{'loss': 3.3645, 'learning_rate': 0.0001164, 'epoch': 0.06}                                                                                                                   
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 200/32330 [00:54<2:44:55,  3.25it/s]
  Num examples = 9578
  Batch size = 8
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1193/1198 [05:17<00:01,  2.74it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.93 GiB (GPU 0; 15.75 GiB total capacity; 9.16 GiB already allocated; 3.93 GiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  1%|‚ñä                                                                                                                                 | 200/32330 [06:12<16:37:24,  1.86s/it]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.bias', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-100.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 2
  Total optimization steps = 32330
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 100
  Will skip the first 0 epochs then the first 200 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/200 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:10<00:00, 19.93it/s]
{'loss': 5.115, 'learning_rate': 6.239999999999999e-05, 'epoch': 0.03}
{'loss': 5.1437, 'learning_rate': 6.84e-05, 'epoch': 0.04}                                                                                                                    
{'loss': 4.5534, 'learning_rate': 7.439999999999999e-05, 'epoch': 0.04}                                                                                                       
{'loss': 4.0949, 'learning_rate': 8.04e-05, 'epoch': 0.04}                                                                                                                    
{'loss': 4.0086, 'learning_rate': 8.639999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 3.5102, 'learning_rate': 9.24e-05, 'epoch': 0.05}                                                                                                                    
{'loss': 3.6257, 'learning_rate': 9.839999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 3.4699, 'learning_rate': 0.00010439999999999999, 'epoch': 0.06}                                                                                                      
{'loss': 3.369, 'learning_rate': 0.00011039999999999999, 'epoch': 0.06}                                                                                                       
{'loss': 3.371, 'learning_rate': 0.0001164, 'epoch': 0.06}                                                                                                                    
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 200/32330 [00:54<2:44:00,  3.27it/s]
  Num examples = 9578
  Batch size = 8
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1193/1198 [05:16<00:01,  2.76it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.93 GiB (GPU 0; 15.75 GiB total capacity; 9.16 GiB already allocated; 3.93 GiB free; 10.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  1%|‚ñä                                                                                                                                 | 200/32330 [06:11<16:34:20,  1.86s/it]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_q.bias', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-100.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 100
  Will skip the first 0 epochs then the first 200 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/200 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:04<00:00, 40.28it/s]
{'loss': 4.8868, 'learning_rate': 6.239999999999999e-05, 'epoch': 0.02}
{'loss': 4.5339, 'learning_rate': 6.84e-05, 'epoch': 0.02}                                                                                                                    
{'loss': 4.6104, 'learning_rate': 7.439999999999999e-05, 'epoch': 0.02}                                                                                                       
{'loss': 4.3882, 'learning_rate': 8.04e-05, 'epoch': 0.02}                                                                                                                    
{'loss': 3.8872, 'learning_rate': 8.639999999999999e-05, 'epoch': 0.02}                                                                                                       
{'loss': 3.4514, 'learning_rate': 9.24e-05, 'epoch': 0.02}                                                                                                                    
{'loss': 3.5859, 'learning_rate': 9.839999999999999e-05, 'epoch': 0.03}                                                                                                       
{'loss': 3.5181, 'learning_rate': 0.00010439999999999999, 'epoch': 0.03}                                                                                                      
{'loss': 3.4402, 'learning_rate': 0.00011039999999999999, 'epoch': 0.03}                                                                                                      
{'loss': 3.3978, 'learning_rate': 0.0001164, 'epoch': 0.03}                                                                                                                   
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 200/64670 [00:37<3:59:35,  4.48it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 556.4648, 'eval_samples_per_second': 17.212, 'eval_steps_per_second': 2.153, 'epoch': 0.03}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-200                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-200/config.json                                                                       | 200/64670 [09:53<3:59:35,  4.48it/s]
Model weights saved in ./turkish_clean/checkpoint-200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-200/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.3259, 'learning_rate': 0.0001224, 'epoch': 0.03}
{'loss': 3.2375, 'learning_rate': 0.00012839999999999998, 'epoch': 0.03}                                                                                                      
{'loss': 3.309, 'learning_rate': 0.0001344, 'epoch': 0.04}                                                                                                                    
{'loss': 3.3614, 'learning_rate': 0.0001404, 'epoch': 0.04}                                                                                                                   
{'loss': 3.3536, 'learning_rate': 0.00014639999999999998, 'epoch': 0.04}                                                                                                      
{'loss': 3.2824, 'learning_rate': 0.0001524, 'epoch': 0.04}                                                                                                                   
{'loss': 3.4737, 'learning_rate': 0.0001584, 'epoch': 0.04}                                                                                                                   
{'loss': 3.2642, 'learning_rate': 0.0001644, 'epoch': 0.04}                                                                                                                   
{'loss': 3.2348, 'learning_rate': 0.00017039999999999997, 'epoch': 0.04}                                                                                                      
{'loss': 3.2867, 'learning_rate': 0.00017639999999999998, 'epoch': 0.05}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 300/64670 [10:26<3:58:13,  4.50it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 527.7295, 'eval_samples_per_second': 18.149, 'eval_steps_per_second': 2.27, 'epoch': 0.05}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-300                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-300/config.json                                                                       | 300/64670 [19:14<3:58:13,  4.50it/s]
Model weights saved in ./turkish_clean/checkpoint-300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-300/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.2717, 'learning_rate': 0.0001824, 'epoch': 0.05}
{'loss': 3.4301, 'learning_rate': 0.00018839999999999997, 'epoch': 0.05}                                                                                                      
{'loss': 3.3522, 'learning_rate': 0.00019439999999999998, 'epoch': 0.05}                                                                                                      
{'loss': 3.2393, 'learning_rate': 0.0002004, 'epoch': 0.05}                                                                                                                   
{'loss': 3.3632, 'learning_rate': 0.00020639999999999998, 'epoch': 0.05}                                                                                                      
{'loss': 3.2034, 'learning_rate': 0.00021239999999999996, 'epoch': 0.06}                                                                                                      
{'loss': 3.4998, 'learning_rate': 0.00021839999999999997, 'epoch': 0.06}                                                                                                      
{'loss': 3.3175, 'learning_rate': 0.00022439999999999998, 'epoch': 0.06}                                                                                                      
{'loss': 3.2915, 'learning_rate': 0.0002304, 'epoch': 0.06}                                                                                                                   
{'loss': 3.278, 'learning_rate': 0.0002364, 'epoch': 0.06}                                                                                                                    
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 400/64670 [19:47<4:13:20,  4.23it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 530.0043, 'eval_samples_per_second': 18.072, 'eval_steps_per_second': 2.26, 'epoch': 0.06}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-400                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-400/config.json                                                                       | 400/64670 [28:37<4:13:20,  4.23it/s]
Model weights saved in ./turkish_clean/checkpoint-400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-400/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.2569, 'learning_rate': 0.00024239999999999998, 'epoch': 0.06}
{'loss': 3.2472, 'learning_rate': 0.00024839999999999997, 'epoch': 0.06}                                                                                                      
{'loss': 3.2874, 'learning_rate': 0.00025439999999999995, 'epoch': 0.07}                                                                                                      
{'loss': 3.2237, 'learning_rate': 0.0002604, 'epoch': 0.07}                                                                                                                   
{'loss': 3.2882, 'learning_rate': 0.00026639999999999997, 'epoch': 0.07}                                                                                                      
{'loss': 3.2182, 'learning_rate': 0.0002724, 'epoch': 0.07}                                                                                                                   
{'loss': 3.1959, 'learning_rate': 0.0002784, 'epoch': 0.07}                                                                                                                   
{'loss': 3.2377, 'learning_rate': 0.0002844, 'epoch': 0.07}                                                                                                                   
{'loss': 3.1732, 'learning_rate': 0.00029039999999999996, 'epoch': 0.08}                                                                                                      
{'loss': 3.2288, 'learning_rate': 0.0002964, 'epoch': 0.08}                                                                                                                   
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 500/64670 [29:10<4:00:59,  4.44it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 528.5952, 'eval_samples_per_second': 18.12, 'eval_steps_per_second': 2.266, 'epoch': 0.08}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-500                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-500/config.json                                                                       | 500/64670 [37:59<4:00:59,  4.44it/s]
Model weights saved in ./turkish_clean/checkpoint-500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-500/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.4166, 'learning_rate': 0.00029998129967274425, 'epoch': 0.08}
{'loss': 3.2337, 'learning_rate': 0.00029993454885460496, 'epoch': 0.08}                                                                                                      
{'loss': 3.2018, 'learning_rate': 0.0002998877980364656, 'epoch': 0.08}                                                                                                       
{'loss': 3.2635, 'learning_rate': 0.00029984104721832627, 'epoch': 0.08}                                                                                                      
{'loss': 3.2362, 'learning_rate': 0.000299794296400187, 'epoch': 0.09}                                                                                                        
{'loss': 3.2277, 'learning_rate': 0.00029974754558204764, 'epoch': 0.09}                                                                                                      
{'loss': 3.1731, 'learning_rate': 0.00029970079476390835, 'epoch': 0.09}                                                                                                      
{'loss': 3.1497, 'learning_rate': 0.000299654043945769, 'epoch': 0.09}                                                                                                        
{'loss': 3.0919, 'learning_rate': 0.0002996072931276297, 'epoch': 0.09}                                                                                                       
{'loss': 3.0638, 'learning_rate': 0.0002995605423094904, 'epoch': 0.09}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 600/64670 [38:32<4:03:46,  4.38it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 530.2621, 'eval_samples_per_second': 18.063, 'eval_steps_per_second': 2.259, 'epoch': 0.09}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-600                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-600/config.json                                                                       | 600/64670 [47:22<4:03:46,  4.38it/s]
Model weights saved in ./turkish_clean/checkpoint-600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.164, 'learning_rate': 0.0002995137914913511, 'epoch': 0.09}
{'loss': 3.1283, 'learning_rate': 0.00029946704067321174, 'epoch': 0.1}                                                                                                       
{'loss': 3.0546, 'learning_rate': 0.00029942028985507246, 'epoch': 0.1}                                                                                                       
{'loss': 3.2089, 'learning_rate': 0.0002993735390369331, 'epoch': 0.1}                                                                                                        
{'loss': 3.0052, 'learning_rate': 0.00029932678821879377, 'epoch': 0.1}                                                                                                       
{'loss': 3.0657, 'learning_rate': 0.0002992800374006545, 'epoch': 0.1}                                                                                                        
{'loss': 3.0749, 'learning_rate': 0.00029923328658251514, 'epoch': 0.1}                                                                                                       
{'loss': 3.2273, 'learning_rate': 0.00029918653576437585, 'epoch': 0.11}                                                                                                      
{'loss': 3.0062, 'learning_rate': 0.00029913978494623656, 'epoch': 0.11}                                                                                                      
{'loss': 3.0307, 'learning_rate': 0.0002990930341280972, 'epoch': 0.11}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 700/64670 [47:56<3:58:43,  4.47it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 522.4318, 'eval_samples_per_second': 18.333, 'eval_steps_per_second': 2.293, 'epoch': 0.11}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-700                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-700/config.json                                                                       | 700/64670 [56:38<3:58:43,  4.47it/s]
Model weights saved in ./turkish_clean/checkpoint-700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.2509, 'learning_rate': 0.00029904628330995793, 'epoch': 0.11}
{'loss': 3.0663, 'learning_rate': 0.0002989995324918186, 'epoch': 0.11}                                                                                                       
{'loss': 2.9964, 'learning_rate': 0.00029895278167367924, 'epoch': 0.11}                                                                                                      
{'loss': 3.0058, 'learning_rate': 0.00029890603085553995, 'epoch': 0.11}                                                                                                      
{'loss': 2.9958, 'learning_rate': 0.0002988592800374006, 'epoch': 0.12}                                                                                                       
{'loss': 3.0627, 'learning_rate': 0.0002988125292192613, 'epoch': 0.12}                                                                                                       
{'loss': 2.9993, 'learning_rate': 0.000298765778401122, 'epoch': 0.12}                                                                                                        
{'loss': 2.9851, 'learning_rate': 0.0002987190275829827, 'epoch': 0.12}                                                                                                       
{'loss': 2.9412, 'learning_rate': 0.00029867227676484335, 'epoch': 0.12}                                                                                                      
{'loss': 3.0017, 'learning_rate': 0.00029862552594670406, 'epoch': 0.12}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 800/64670 [57:11<3:58:44,  4.46it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9999585027803137, 'eval_cer': 0.9821419975932612, 'eval_runtime': 523.2954, 'eval_samples_per_second': 18.303, 'eval_steps_per_second': 2.289, 'epoch': 0.12}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-800                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-800/config.json                                                                     | 800/64670 [1:05:55<3:58:44,  4.46it/s]
Model weights saved in ./turkish_clean/checkpoint-800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.2427, 'learning_rate': 0.0002985834502103787, 'epoch': 0.13}
{'loss': 2.9476, 'learning_rate': 0.00029853669939223934, 'epoch': 0.13}                                                                                                      
{'loss': 2.9472, 'learning_rate': 0.00029848994857410005, 'epoch': 0.13}                                                                                                      
{'loss': 2.8875, 'learning_rate': 0.0002984431977559607, 'epoch': 0.13}                                                                                                       
{'loss': 2.9554, 'learning_rate': 0.00029839644693782137, 'epoch': 0.13}                                                                                                      
{'loss': 3.0177, 'learning_rate': 0.0002983496961196821, 'epoch': 0.13}                                                                                                       
{'loss': 2.9251, 'learning_rate': 0.00029830294530154274, 'epoch': 0.13}                                                                                                      
{'loss': 2.9111, 'learning_rate': 0.00029825619448340345, 'epoch': 0.14}                                                                                                      
{'loss': 2.8904, 'learning_rate': 0.0002982094436652641, 'epoch': 0.14}                                                                                                       
{'loss': 2.8742, 'learning_rate': 0.0002981626928471248, 'epoch': 0.14}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                        | 900/64670 [1:06:28<3:55:47,  4.51it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0092331313801974, 'eval_cer': 0.9552274368231047, 'eval_runtime': 523.751, 'eval_samples_per_second': 18.287, 'eval_steps_per_second': 2.287, 'epoch': 0.14}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-900                                                                                                                     
Configuration saved in ./turkish_clean/checkpoint-900/config.json                                                                     | 900/64670 [1:15:11<3:55:47,  4.51it/s]
Model weights saved in ./turkish_clean/checkpoint-900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.9511, 'learning_rate': 0.00029811594202898547, 'epoch': 0.14}
{'loss': 2.8651, 'learning_rate': 0.0002980691912108462, 'epoch': 0.14}                                                                                                       
{'loss': 2.9031, 'learning_rate': 0.00029802244039270684, 'epoch': 0.14}                                                                                                      
{'loss': 2.9714, 'learning_rate': 0.00029797568957456755, 'epoch': 0.15}                                                                                                      
{'loss': 2.7997, 'learning_rate': 0.0002979289387564282, 'epoch': 0.15}                                                                                                       
{'loss': 2.8967, 'learning_rate': 0.00029788218793828887, 'epoch': 0.15}                                                                                                      
{'loss': 2.8402, 'learning_rate': 0.0002978354371201496, 'epoch': 0.15}                                                                                                       
{'loss': 2.7434, 'learning_rate': 0.00029778868630201023, 'epoch': 0.15}                                                                                                      
{'loss': 2.7655, 'learning_rate': 0.00029774193548387094, 'epoch': 0.15}                                                                                                      
{'loss': 2.8737, 'learning_rate': 0.00029769518466573166, 'epoch': 0.15}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1000/64670 [1:15:45<4:05:53,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0005187152460786, 'eval_cer': 0.9814079422382671, 'eval_runtime': 528.5421, 'eval_samples_per_second': 18.122, 'eval_steps_per_second': 2.267, 'epoch': 0.15}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-1000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1000/config.json                                                                   | 1000/64670 [1:24:34<4:05:53,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-1000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 3.5646, 'learning_rate': 0.0002976484338475923, 'epoch': 0.16}
{'loss': 2.7407, 'learning_rate': 0.00029760168302945297, 'epoch': 0.16}                                                                                                      
{'loss': 2.6175, 'learning_rate': 0.0002975549322113137, 'epoch': 0.16}                                                                                                       
{'loss': 2.7199, 'learning_rate': 0.00029750818139317434, 'epoch': 0.16}                                                                                                      
{'loss': 2.6782, 'learning_rate': 0.00029746143057503505, 'epoch': 0.16}                                                                                                      
{'loss': 2.8353, 'learning_rate': 0.0002974146797568957, 'epoch': 0.16}                                                                                                       
{'loss': 2.6183, 'learning_rate': 0.0002973679289387564, 'epoch': 0.17}                                                                                                       
{'loss': 2.5086, 'learning_rate': 0.0002973211781206171, 'epoch': 0.17}                                                                                                       
{'loss': 2.3951, 'learning_rate': 0.0002972744273024778, 'epoch': 0.17}                                                                                                       
{'loss': 2.371, 'learning_rate': 0.00029722767648433844, 'epoch': 0.17}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1100/64670 [1:25:07<4:12:27,  4.20it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0123454228566686, 'eval_cer': 0.8878580024067388, 'eval_runtime': 531.8024, 'eval_samples_per_second': 18.01, 'eval_steps_per_second': 2.253, 'epoch': 0.17}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-1100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1100/config.json                                                                   | 1100/64670 [1:33:59<4:12:27,  4.20it/s]
Model weights saved in ./turkish_clean/checkpoint-1100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.3251, 'learning_rate': 0.00029718092566619915, 'epoch': 0.17}
{'loss': 2.192, 'learning_rate': 0.0002971341748480598, 'epoch': 0.17}                                                                                                        
{'loss': 2.4197, 'learning_rate': 0.00029708742402992047, 'epoch': 0.17}                                                                                                      
{'loss': 2.2221, 'learning_rate': 0.0002970406732117812, 'epoch': 0.18}                                                                                                       
{'loss': 2.2063, 'learning_rate': 0.00029699392239364183, 'epoch': 0.18}                                                                                                      
{'loss': 2.0095, 'learning_rate': 0.00029694717157550255, 'epoch': 0.18}                                                                                                      
{'loss': 1.8394, 'learning_rate': 0.00029690042075736326, 'epoch': 0.18}                                                                                                      
{'loss': 1.8939, 'learning_rate': 0.0002968536699392239, 'epoch': 0.18}                                                                                                       
{'loss': 2.0162, 'learning_rate': 0.0002968069191210846, 'epoch': 0.18}                                                                                                       
{'loss': 2.016, 'learning_rate': 0.0002967601683029453, 'epoch': 0.19}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1200/64670 [1:34:33<4:09:48,  4.23it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0222010125321603, 'eval_cer': 0.6851672683513839, 'eval_runtime': 532.4572, 'eval_samples_per_second': 17.988, 'eval_steps_per_second': 2.25, 'epoch': 0.19}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-1200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1200/config.json                                                                   | 1200/64670 [1:43:26<4:09:48,  4.23it/s]
Model weights saved in ./turkish_clean/checkpoint-1200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.701, 'learning_rate': 0.00029671341748480594, 'epoch': 0.19}
{'loss': 1.5804, 'learning_rate': 0.00029666666666666665, 'epoch': 0.19}                                                                                                      
{'loss': 1.5195, 'learning_rate': 0.0002966199158485273, 'epoch': 0.19}                                                                                                       
{'loss': 1.7267, 'learning_rate': 0.000296573165030388, 'epoch': 0.19}                                                                                                        
{'loss': 1.8309, 'learning_rate': 0.0002965264142122487, 'epoch': 0.19}                                                                                                       
{'loss': 1.7038, 'learning_rate': 0.0002964843384759233, 'epoch': 0.19}                                                                                                       
{'loss': 1.5349, 'learning_rate': 0.00029643758765778396, 'epoch': 0.2}                                                                                                       
{'loss': 1.5019, 'learning_rate': 0.00029639083683964467, 'epoch': 0.2}                                                                                                       
{'loss': 1.5909, 'learning_rate': 0.00029634408602150533, 'epoch': 0.2}                                                                                                       
{'loss': 1.5804, 'learning_rate': 0.00029629733520336604, 'epoch': 0.2}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1300/64670 [1:44:00<4:00:01,  4.40it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.029960992613495, 'eval_cer': 0.6055282791817088, 'eval_runtime': 530.4095, 'eval_samples_per_second': 18.058, 'eval_steps_per_second': 2.259, 'epoch': 0.2}                                                                                                                                                                
Saving model checkpoint to ./turkish_clean/checkpoint-1300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1300/config.json                                                                   | 1300/64670 [1:52:50<4:00:01,  4.40it/s]
Model weights saved in ./turkish_clean/checkpoint-1300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.5146, 'learning_rate': 0.00029625058438522675, 'epoch': 0.2}
{'loss': 1.476, 'learning_rate': 0.0002962038335670874, 'epoch': 0.2}                                                                                                         
{'loss': 1.1956, 'learning_rate': 0.00029615708274894806, 'epoch': 0.21}                                                                                                      
{'loss': 1.4525, 'learning_rate': 0.0002961103319308088, 'epoch': 0.21}                                                                                                       
{'loss': 1.351, 'learning_rate': 0.00029606358111266943, 'epoch': 0.21}                                                                                                       
{'loss': 1.3709, 'learning_rate': 0.0002960168302945301, 'epoch': 0.21}                                                                                                       
{'loss': 1.3302, 'learning_rate': 0.0002959700794763908, 'epoch': 0.21}                                                                                                       
{'loss': 1.3882, 'learning_rate': 0.0002959233286582515, 'epoch': 0.21}                                                                                                       
{'loss': 1.3967, 'learning_rate': 0.00029587657784011217, 'epoch': 0.21}                                                                                                      
{'loss': 1.599, 'learning_rate': 0.0002958298270219729, 'epoch': 0.22}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1400/64670 [1:53:24<3:54:29,  4.50it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.036642044982986, 'eval_cer': 0.5971335740072202, 'eval_runtime': 531.1394, 'eval_samples_per_second': 18.033, 'eval_steps_per_second': 2.256, 'epoch': 0.22}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-1400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1400/config.json                                                                   | 1400/64670 [2:02:16<3:54:29,  4.50it/s]
Model weights saved in ./turkish_clean/checkpoint-1400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.3678, 'learning_rate': 0.00029578307620383354, 'epoch': 0.22}
{'loss': 1.0697, 'learning_rate': 0.00029573632538569425, 'epoch': 0.22}                                                                                                      
{'loss': 1.2749, 'learning_rate': 0.0002956895745675549, 'epoch': 0.22}                                                                                                       
{'loss': 1.3617, 'learning_rate': 0.00029564282374941556, 'epoch': 0.22}                                                                                                      
{'loss': 1.4891, 'learning_rate': 0.0002955960729312763, 'epoch': 0.22}                                                                                                       
{'loss': 1.3957, 'learning_rate': 0.00029554932211313693, 'epoch': 0.23}                                                                                                      
{'loss': 1.1541, 'learning_rate': 0.00029550257129499764, 'epoch': 0.23}                                                                                                      
{'loss': 1.1264, 'learning_rate': 0.00029545582047685835, 'epoch': 0.23}                                                                                                      
{'loss': 1.1527, 'learning_rate': 0.000295409069658719, 'epoch': 0.23}                                                                                                        
{'loss': 1.6814, 'learning_rate': 0.00029536231884057967, 'epoch': 0.23}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1500/64670 [2:02:49<3:54:32,  4.49it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0120549423188647, 'eval_cer': 0.5850036101083033, 'eval_runtime': 531.655, 'eval_samples_per_second': 18.015, 'eval_steps_per_second': 2.253, 'epoch': 0.23}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-1500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1500/config.json                                                                   | 1500/64670 [2:11:41<3:54:32,  4.49it/s]
Model weights saved in ./turkish_clean/checkpoint-1500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.096, 'learning_rate': 0.0002953155680224404, 'epoch': 0.23}
{'loss': 1.0675, 'learning_rate': 0.00029526881720430103, 'epoch': 0.24}                                                                                                      
{'loss': 1.2605, 'learning_rate': 0.00029522206638616175, 'epoch': 0.24}                                                                                                      
{'loss': 1.1927, 'learning_rate': 0.0002951753155680224, 'epoch': 0.24}                                                                                                       
{'loss': 1.394, 'learning_rate': 0.0002951285647498831, 'epoch': 0.24}                                                                                                        
{'loss': 1.834, 'learning_rate': 0.00029508181393174377, 'epoch': 0.24}                                                                                                       
{'loss': 1.1355, 'learning_rate': 0.0002950350631136045, 'epoch': 0.24}                                                                                                       
{'loss': 1.1654, 'learning_rate': 0.00029498831229546514, 'epoch': 0.24}                                                                                                      
{'loss': 1.2849, 'learning_rate': 0.00029494156147732585, 'epoch': 0.25}                                                                                                      
{'loss': 1.1679, 'learning_rate': 0.0002948948106591865, 'epoch': 0.25}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1600/64670 [2:12:14<3:58:39,  4.40it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0043779566769027, 'eval_cer': 0.5667966305655836, 'eval_runtime': 530.8555, 'eval_samples_per_second': 18.043, 'eval_steps_per_second': 2.257, 'epoch': 0.25}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-1600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1600/config.json                                                                   | 1600/64670 [2:21:05<3:58:39,  4.40it/s]
Model weights saved in ./turkish_clean/checkpoint-1600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.9775, 'learning_rate': 0.00029484805984104716, 'epoch': 0.25}
{'loss': 1.402, 'learning_rate': 0.0002948013090229079, 'epoch': 0.25}                                                                                                        
{'loss': 1.0986, 'learning_rate': 0.00029475455820476853, 'epoch': 0.25}                                                                                                      
{'loss': 1.1573, 'learning_rate': 0.00029470780738662924, 'epoch': 0.25}                                                                                                      
{'loss': 1.3535, 'learning_rate': 0.00029466105656848995, 'epoch': 0.26}                                                                                                      
{'loss': 1.1022, 'learning_rate': 0.0002946143057503506, 'epoch': 0.26}                                                                                                       
{'loss': 0.9245, 'learning_rate': 0.0002945675549322113, 'epoch': 0.26}                                                                                                       
{'loss': 1.0408, 'learning_rate': 0.000294520804114072, 'epoch': 0.26}                                                                                                        
{'loss': 0.9981, 'learning_rate': 0.00029447405329593264, 'epoch': 0.26}                                                                                                      
{'loss': 1.2954, 'learning_rate': 0.00029442730247779335, 'epoch': 0.26}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1700/64670 [2:21:38<3:55:49,  4.45it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9981326251141174, 'eval_cer': 0.5486064981949459, 'eval_runtime': 531.0234, 'eval_samples_per_second': 18.037, 'eval_steps_per_second': 2.256, 'epoch': 0.26}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-1700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1700/config.json                                                                   | 1700/64670 [2:30:29<3:55:49,  4.45it/s]
Model weights saved in ./turkish_clean/checkpoint-1700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.1521, 'learning_rate': 0.000294380551659654, 'epoch': 0.26}
{'loss': 1.0584, 'learning_rate': 0.0002943338008415147, 'epoch': 0.27}                                                                                                       
{'loss': 1.0761, 'learning_rate': 0.00029428705002337537, 'epoch': 0.27}                                                                                                      
{'loss': 0.9937, 'learning_rate': 0.0002942402992052361, 'epoch': 0.27}                                                                                                       
{'loss': 1.2669, 'learning_rate': 0.00029419354838709674, 'epoch': 0.27}                                                                                                      
{'loss': 1.0961, 'learning_rate': 0.00029414679756895745, 'epoch': 0.27}                                                                                                      
{'loss': 0.999, 'learning_rate': 0.0002941000467508181, 'epoch': 0.27}                                                                                                        
{'loss': 0.9987, 'learning_rate': 0.0002940532959326788, 'epoch': 0.28}                                                                                                       
{'loss': 1.0934, 'learning_rate': 0.0002940065451145395, 'epoch': 0.28}                                                                                                       
{'loss': 1.2075, 'learning_rate': 0.00029395979429640013, 'epoch': 0.28}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1800/64670 [2:31:03<4:02:16,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9851024981326251, 'eval_cer': 0.5305944645006017, 'eval_runtime': 532.3353, 'eval_samples_per_second': 17.992, 'eval_steps_per_second': 2.25, 'epoch': 0.28}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-1800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1800/config.json                                                                   | 1800/64670 [2:39:55<4:02:16,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-1800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.887, 'learning_rate': 0.00029391304347826084, 'epoch': 0.28}
{'loss': 0.8724, 'learning_rate': 0.0002938662926601215, 'epoch': 0.28}                                                                                                       
{'loss': 1.0509, 'learning_rate': 0.0002938195418419822, 'epoch': 0.28}                                                                                                       
{'loss': 1.1471, 'learning_rate': 0.0002937727910238429, 'epoch': 0.28}                                                                                                       
{'loss': 1.2354, 'learning_rate': 0.0002937260402057036, 'epoch': 0.29}                                                                                                       
{'loss': 1.2104, 'learning_rate': 0.00029367928938756424, 'epoch': 0.29}                                                                                                      
{'loss': 0.8591, 'learning_rate': 0.00029363253856942495, 'epoch': 0.29}                                                                                                      
{'loss': 0.9749, 'learning_rate': 0.0002935857877512856, 'epoch': 0.29}                                                                                                       
{'loss': 0.8597, 'learning_rate': 0.0002935390369331463, 'epoch': 0.29}                                                                                                       
{'loss': 1.1946, 'learning_rate': 0.000293492286115007, 'epoch': 0.29}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1900/64670 [2:40:29<3:58:43,  4.38it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9868038841397626, 'eval_cer': 0.5289747292418773, 'eval_runtime': 526.3089, 'eval_samples_per_second': 18.198, 'eval_steps_per_second': 2.276, 'epoch': 0.29}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-1900                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-1900/config.json                                                                   | 1900/64670 [2:49:15<3:58:43,  4.38it/s]
Model weights saved in ./turkish_clean/checkpoint-1900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-1900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.0448, 'learning_rate': 0.0002934455352968677, 'epoch': 0.3}
{'loss': 0.9299, 'learning_rate': 0.00029339878447872834, 'epoch': 0.3}                                                                                                       
{'loss': 0.9251, 'learning_rate': 0.00029335203366058905, 'epoch': 0.3}                                                                                                       
{'loss': 1.096, 'learning_rate': 0.0002933052828424497, 'epoch': 0.3}                                                                                                         
{'loss': 1.0876, 'learning_rate': 0.0002932585320243104, 'epoch': 0.3}                                                                                                        
{'loss': 1.6721, 'learning_rate': 0.0002932117812061711, 'epoch': 0.3}                                                                                                        
{'loss': 0.9345, 'learning_rate': 0.00029316503038803173, 'epoch': 0.3}                                                                                                       
{'loss': 0.8944, 'learning_rate': 0.00029311827956989245, 'epoch': 0.31}                                                                                                      
{'loss': 1.0148, 'learning_rate': 0.0002930715287517531, 'epoch': 0.31}                                                                                                       
{'loss': 1.0667, 'learning_rate': 0.0002930247779336138, 'epoch': 0.31}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2000/64670 [2:49:49<4:00:50,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0030292970370984, 'eval_cer': 0.5253285198555957, 'eval_runtime': 527.6745, 'eval_samples_per_second': 18.151, 'eval_steps_per_second': 2.27, 'epoch': 0.31}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-2000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2000/config.json                                                                   | 2000/64670 [2:58:36<4:00:50,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-2000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.8695, 'learning_rate': 0.0002929780271154745, 'epoch': 0.31}
{'loss': 0.8608, 'learning_rate': 0.0002929312762973352, 'epoch': 0.31}                                                                                                       
{'loss': 0.9062, 'learning_rate': 0.0002928845254791959, 'epoch': 0.31}                                                                                                       
{'loss': 0.8316, 'learning_rate': 0.00029283777466105655, 'epoch': 0.32}                                                                                                      
{'loss': 1.11, 'learning_rate': 0.0002927910238429172, 'epoch': 0.32}                                                                                                         
{'loss': 0.9146, 'learning_rate': 0.0002927442730247779, 'epoch': 0.32}                                                                                                       
{'loss': 0.9691, 'learning_rate': 0.0002926975222066386, 'epoch': 0.32}                                                                                                       
{'loss': 0.932, 'learning_rate': 0.0002926507713884993, 'epoch': 0.32}                                                                                                        
{'loss': 0.9954, 'learning_rate': 0.00029260402057035994, 'epoch': 0.32}                                                                                                      
{'loss': 1.0818, 'learning_rate': 0.00029255726975222065, 'epoch': 0.32}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2100/64670 [2:59:10<4:01:24,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9687733421860736, 'eval_cer': 0.5200361010830324, 'eval_runtime': 534.2712, 'eval_samples_per_second': 17.927, 'eval_steps_per_second': 2.242, 'epoch': 0.32}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-2100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2100/config.json                                                                   | 2100/64670 [3:08:04<4:01:24,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-2100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.6651, 'learning_rate': 0.0002925105189340813, 'epoch': 0.33}
{'loss': 0.9823, 'learning_rate': 0.000292463768115942, 'epoch': 0.33}                                                                                                        
{'loss': 0.936, 'learning_rate': 0.0002924170172978027, 'epoch': 0.33}                                                                                                        
{'loss': 0.9323, 'learning_rate': 0.0002923702664796634, 'epoch': 0.33}                                                                                                       
{'loss': 1.0314, 'learning_rate': 0.00029232351566152405, 'epoch': 0.33}                                                                                                      
{'loss': 0.8148, 'learning_rate': 0.0002922767648433847, 'epoch': 0.33}                                                                                                       
{'loss': 0.8928, 'learning_rate': 0.0002922300140252454, 'epoch': 0.34}                                                                                                       
{'loss': 0.781, 'learning_rate': 0.0002921832632071061, 'epoch': 0.34}                                                                                                        
{'loss': 0.9596, 'learning_rate': 0.0002921365123889668, 'epoch': 0.34}                                                                                                       
{'loss': 0.8755, 'learning_rate': 0.0002920897615708275, 'epoch': 0.34}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2200/64670 [3:08:38<4:00:56,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9799983401112126, 'eval_cer': 0.510404332129964, 'eval_runtime': 531.5591, 'eval_samples_per_second': 18.019, 'eval_steps_per_second': 2.254, 'epoch': 0.34}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-2200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2200/config.json                                                                   | 2200/64670 [3:17:30<4:00:56,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-2200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.0178, 'learning_rate': 0.00029204301075268815, 'epoch': 0.34}
{'loss': 0.8622, 'learning_rate': 0.0002919962599345488, 'epoch': 0.34}                                                                                                       
{'loss': 0.8688, 'learning_rate': 0.0002919495091164095, 'epoch': 0.34}                                                                                                       
{'loss': 0.8619, 'learning_rate': 0.0002919027582982702, 'epoch': 0.35}                                                                                                       
{'loss': 1.1947, 'learning_rate': 0.0002918560074801309, 'epoch': 0.35}                                                                                                       
{'loss': 0.8329, 'learning_rate': 0.00029180925666199154, 'epoch': 0.35}                                                                                                      
{'loss': 0.9957, 'learning_rate': 0.00029176250584385225, 'epoch': 0.35}                                                                                                      
{'loss': 0.7382, 'learning_rate': 0.0002917157550257129, 'epoch': 0.35}                                                                                                       
{'loss': 0.7699, 'learning_rate': 0.0002916690042075736, 'epoch': 0.35}                                                                                                       
{'loss': 1.0747, 'learning_rate': 0.0002916222533894343, 'epoch': 0.36}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2300/64670 [3:18:03<4:01:37,  4.30it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9707859573408582, 'eval_cer': 0.5184259927797834, 'eval_runtime': 531.194, 'eval_samples_per_second': 18.031, 'eval_steps_per_second': 2.255, 'epoch': 0.36}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-2300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2300/config.json                                                                   | 2300/64670 [3:26:54<4:01:37,  4.30it/s]
Model weights saved in ./turkish_clean/checkpoint-2300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.3708, 'learning_rate': 0.000291575502571295, 'epoch': 0.36}
{'loss': 0.8422, 'learning_rate': 0.00029152875175315565, 'epoch': 0.36}                                                                                                      
{'loss': 0.9851, 'learning_rate': 0.0002914820009350163, 'epoch': 0.36}                                                                                                       
{'loss': 0.9016, 'learning_rate': 0.000291435250116877, 'epoch': 0.36}                                                                                                        
{'loss': 1.2069, 'learning_rate': 0.00029138849929873767, 'epoch': 0.36}                                                                                                      
{'loss': 1.0861, 'learning_rate': 0.0002913417484805984, 'epoch': 0.36}                                                                                                       
{'loss': 1.2317, 'learning_rate': 0.0002912949976624591, 'epoch': 0.37}                                                                                                       
{'loss': 0.7553, 'learning_rate': 0.00029124824684431975, 'epoch': 0.37}                                                                                                      
{'loss': 0.846, 'learning_rate': 0.00029120149602618046, 'epoch': 0.37}                                                                                                       
{'loss': 1.1197, 'learning_rate': 0.0002911547452080411, 'epoch': 0.37}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2400/64670 [3:27:28<3:58:58,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9560336957423853, 'eval_cer': 0.5130252707581228, 'eval_runtime': 525.4994, 'eval_samples_per_second': 18.226, 'eval_steps_per_second': 2.28, 'epoch': 0.37}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-2400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2400/config.json                                                                   | 2400/64670 [3:36:14<3:58:58,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-2400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-1900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7828, 'learning_rate': 0.0002911079943899018, 'epoch': 0.37}
{'loss': 0.6214, 'learning_rate': 0.0002910612435717625, 'epoch': 0.37}                                                                                                       
{'loss': 0.8028, 'learning_rate': 0.00029101449275362314, 'epoch': 0.38}                                                                                                      
{'loss': 0.8374, 'learning_rate': 0.00029096774193548386, 'epoch': 0.38}                                                                                                      
{'loss': 0.9942, 'learning_rate': 0.0002909209911173445, 'epoch': 0.38}                                                                                                       
{'loss': 0.7483, 'learning_rate': 0.0002908742402992052, 'epoch': 0.38}                                                                                                       
{'loss': 0.7268, 'learning_rate': 0.0002908274894810659, 'epoch': 0.38}                                                                                                       
{'loss': 0.8355, 'learning_rate': 0.0002907807386629266, 'epoch': 0.38}                                                                                                       
{'loss': 0.9129, 'learning_rate': 0.00029073398784478725, 'epoch': 0.39}                                                                                                      
{'loss': 0.8617, 'learning_rate': 0.00029068723702664796, 'epoch': 0.39}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2500/64670 [3:36:48<3:59:48,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9429413229313636, 'eval_cer': 0.5048712394705175, 'eval_runtime': 524.2198, 'eval_samples_per_second': 18.271, 'eval_steps_per_second': 2.285, 'epoch': 0.39}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-2500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2500/config.json                                                                   | 2500/64670 [3:45:32<3:59:48,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-2500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.0172, 'learning_rate': 0.0002906404862085086, 'epoch': 0.39}
{'loss': 0.7305, 'learning_rate': 0.0002905937353903693, 'epoch': 0.39}                                                                                                       
{'loss': 0.7467, 'learning_rate': 0.00029054698457223, 'epoch': 0.39}                                                                                                         
{'loss': 0.7942, 'learning_rate': 0.0002905002337540907, 'epoch': 0.39}                                                                                                       
{'loss': 1.0017, 'learning_rate': 0.00029045348293595135, 'epoch': 0.39}                                                                                                      
{'loss': 0.6707, 'learning_rate': 0.00029040673211781206, 'epoch': 0.4}                                                                                                       
{'loss': 0.6441, 'learning_rate': 0.0002903599812996727, 'epoch': 0.4}                                                                                                        
{'loss': 0.8615, 'learning_rate': 0.0002903132304815334, 'epoch': 0.4}                                                                                                        
{'loss': 0.8632, 'learning_rate': 0.0002902664796633941, 'epoch': 0.4}                                                                                                        
{'loss': 0.9743, 'learning_rate': 0.00029021972884525475, 'epoch': 0.4}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2600/64670 [3:46:07<3:57:42,  4.35it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9603286579799154, 'eval_cer': 0.5052178098676293, 'eval_runtime': 531.6966, 'eval_samples_per_second': 18.014, 'eval_steps_per_second': 2.253, 'epoch': 0.4}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-2600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2600/config.json                                                                   | 2600/64670 [3:54:58<3:57:42,  4.35it/s]
Model weights saved in ./turkish_clean/checkpoint-2600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.0466, 'learning_rate': 0.00029017297802711546, 'epoch': 0.4}
{'loss': 0.7167, 'learning_rate': 0.0002901262272089761, 'epoch': 0.41}                                                                                                       
{'loss': 0.8429, 'learning_rate': 0.0002900794763908368, 'epoch': 0.41}                                                                                                       
{'loss': 0.7478, 'learning_rate': 0.00029003272557269754, 'epoch': 0.41}                                                                                                      
{'loss': 1.0022, 'learning_rate': 0.0002899859747545582, 'epoch': 0.41}                                                                                                       
{'loss': 0.7726, 'learning_rate': 0.00028993922393641885, 'epoch': 0.41}                                                                                                      
{'loss': 0.859, 'learning_rate': 0.00028989247311827956, 'epoch': 0.41}                                                                                                       
{'loss': 0.7921, 'learning_rate': 0.0002898457223001402, 'epoch': 0.41}                                                                                                       
{'loss': 0.7113, 'learning_rate': 0.0002897989714820009, 'epoch': 0.42}                                                                                                       
{'loss': 0.8267, 'learning_rate': 0.0002897522206638616, 'epoch': 0.42}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2700/64670 [3:55:32<3:56:32,  4.37it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.955846958253797, 'eval_cer': 0.49338628158844766, 'eval_runtime': 530.7361, 'eval_samples_per_second': 18.047, 'eval_steps_per_second': 2.257, 'epoch': 0.42}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-2700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2700/config.json                                                                   | 2700/64670 [4:04:22<3:56:32,  4.37it/s]
Model weights saved in ./turkish_clean/checkpoint-2700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7512, 'learning_rate': 0.0002897054698457223, 'epoch': 0.42}
{'loss': 0.7039, 'learning_rate': 0.00028965871902758295, 'epoch': 0.42}                                                                                                      
{'loss': 0.7277, 'learning_rate': 0.00028961196820944367, 'epoch': 0.42}                                                                                                      
{'loss': 0.7598, 'learning_rate': 0.0002895652173913043, 'epoch': 0.42}                                                                                                       
{'loss': 0.9748, 'learning_rate': 0.00028951846657316503, 'epoch': 0.43}                                                                                                      
{'loss': 0.6671, 'learning_rate': 0.0002894717157550257, 'epoch': 0.43}                                                                                                       
{'loss': 0.6896, 'learning_rate': 0.00028942496493688635, 'epoch': 0.43}                                                                                                      
{'loss': 0.8503, 'learning_rate': 0.00028937821411874706, 'epoch': 0.43}                                                                                                      
{'loss': 0.8221, 'learning_rate': 0.0002893314633006077, 'epoch': 0.43}                                                                                                       
{'loss': 1.0104, 'learning_rate': 0.0002892847124824684, 'epoch': 0.43}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2800/64670 [4:04:56<3:57:43,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9511785210390904, 'eval_cer': 0.5065078219013237, 'eval_runtime': 529.1205, 'eval_samples_per_second': 18.102, 'eval_steps_per_second': 2.264, 'epoch': 0.43}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-2800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2800/config.json                                                                   | 2800/64670 [4:13:45<3:57:43,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-2800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.7031, 'learning_rate': 0.0002892379616643291, 'epoch': 0.43}
{'loss': 0.6657, 'learning_rate': 0.0002891912108461898, 'epoch': 0.44}                                                                                                       
{'loss': 0.6864, 'learning_rate': 0.00028914446002805045, 'epoch': 0.44}                                                                                                      
{'loss': 0.9593, 'learning_rate': 0.00028909770920991116, 'epoch': 0.44}                                                                                                      
{'loss': 0.9567, 'learning_rate': 0.0002890509583917718, 'epoch': 0.44}                                                                                                       
{'loss': 0.6117, 'learning_rate': 0.00028900420757363253, 'epoch': 0.44}                                                                                                      
{'loss': 0.7741, 'learning_rate': 0.0002889574567554932, 'epoch': 0.44}                                                                                                       
{'loss': 0.6974, 'learning_rate': 0.00028891070593735384, 'epoch': 0.45}                                                                                                      
{'loss': 0.8181, 'learning_rate': 0.00028886395511921456, 'epoch': 0.45}                                                                                                      
{'loss': 1.0094, 'learning_rate': 0.00028881720430107527, 'epoch': 0.45}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2900/64670 [4:14:19<3:53:18,  4.41it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9726533322267408, 'eval_cer': 0.4930348977135981, 'eval_runtime': 531.1311, 'eval_samples_per_second': 18.033, 'eval_steps_per_second': 2.256, 'epoch': 0.45}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-2900                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-2900/config.json                                                                   | 2900/64670 [4:23:10<3:53:18,  4.41it/s]
Model weights saved in ./turkish_clean/checkpoint-2900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-2900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.5785, 'learning_rate': 0.0002887704534829359, 'epoch': 0.45}
{'loss': 0.6964, 'learning_rate': 0.00028872370266479663, 'epoch': 0.45}                                                                                                      
{'loss': 0.5711, 'learning_rate': 0.0002886769518466573, 'epoch': 0.45}                                                                                                       
{'loss': 0.8383, 'learning_rate': 0.00028863020102851795, 'epoch': 0.45}                                                                                                      
{'loss': 0.9136, 'learning_rate': 0.00028858345021037866, 'epoch': 0.46}                                                                                                      
{'loss': 0.9162, 'learning_rate': 0.0002885366993922393, 'epoch': 0.46}                                                                                                       
{'loss': 0.7561, 'learning_rate': 0.00028848994857410003, 'epoch': 0.46}                                                                                                      
{'loss': 0.6867, 'learning_rate': 0.0002884431977559607, 'epoch': 0.46}                                                                                                       
{'loss': 0.5665, 'learning_rate': 0.0002883964469378214, 'epoch': 0.46}                                                                                                       
{'loss': 1.1105, 'learning_rate': 0.0002883496961196821, 'epoch': 0.46}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3000/64670 [4:23:44<3:56:35,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9345588845547348, 'eval_cer': 0.49414440433212997, 'eval_runtime': 527.0947, 'eval_samples_per_second': 18.171, 'eval_steps_per_second': 2.273, 'epoch': 0.46}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-3000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3000/config.json                                                                   | 3000/64670 [4:32:31<3:56:35,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-3000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7801, 'learning_rate': 0.00028830294530154276, 'epoch': 0.47}
{'loss': 0.5906, 'learning_rate': 0.0002882561944834034, 'epoch': 0.47}                                                                                                       
{'loss': 0.6586, 'learning_rate': 0.00028820944366526413, 'epoch': 0.47}                                                                                                      
{'loss': 0.7933, 'learning_rate': 0.0002881626928471248, 'epoch': 0.47}                                                                                                       
{'loss': 0.8887, 'learning_rate': 0.00028811594202898545, 'epoch': 0.47}                                                                                                      
{'loss': 0.7303, 'learning_rate': 0.00028806919121084616, 'epoch': 0.47}                                                                                                      
{'loss': 0.6156, 'learning_rate': 0.00028802244039270687, 'epoch': 0.47}                                                                                                      
{'loss': 0.9766, 'learning_rate': 0.0002879756895745675, 'epoch': 0.48}                                                                                                       
{'loss': 0.6852, 'learning_rate': 0.00028792893875642824, 'epoch': 0.48}                                                                                                      
{'loss': 0.707, 'learning_rate': 0.0002878821879382889, 'epoch': 0.48}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3100/64670 [4:33:06<4:04:05,  4.20it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9484812017594821, 'eval_cer': 0.49280385078219013, 'eval_runtime': 530.3075, 'eval_samples_per_second': 18.061, 'eval_steps_per_second': 2.259, 'epoch': 0.48}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-3100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3100/config.json                                                                   | 3100/64670 [4:41:56<4:04:05,  4.20it/s]
Model weights saved in ./turkish_clean/checkpoint-3100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.0785, 'learning_rate': 0.0002878354371201496, 'epoch': 0.48}
{'loss': 0.7426, 'learning_rate': 0.00028778868630201026, 'epoch': 0.48}                                                                                                      
{'loss': 0.7828, 'learning_rate': 0.0002877419354838709, 'epoch': 0.48}                                                                                                       
{'loss': 0.7214, 'learning_rate': 0.00028769518466573163, 'epoch': 0.49}                                                                                                      
{'loss': 1.0021, 'learning_rate': 0.0002876484338475923, 'epoch': 0.49}                                                                                                       
{'loss': 1.0214, 'learning_rate': 0.000287601683029453, 'epoch': 0.49}                                                                                                        
{'loss': 1.0805, 'learning_rate': 0.0002875549322113137, 'epoch': 0.49}                                                                                                       
{'loss': 0.6993, 'learning_rate': 0.00028750818139317437, 'epoch': 0.49}                                                                                                      
{'loss': 0.6818, 'learning_rate': 0.000287461430575035, 'epoch': 0.49}                                                                                                        
{'loss': 1.0818, 'learning_rate': 0.00028741467975689573, 'epoch': 0.49}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3200/64670 [4:42:30<3:56:54,  4.32it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9410531994356378, 'eval_cer': 0.4871263537906137, 'eval_runtime': 528.3704, 'eval_samples_per_second': 18.127, 'eval_steps_per_second': 2.267, 'epoch': 0.49}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-3200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3200/config.json                                                                   | 3200/64670 [4:51:18<3:56:54,  4.32it/s]
Model weights saved in ./turkish_clean/checkpoint-3200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7057, 'learning_rate': 0.0002873679289387564, 'epoch': 0.5}
{'loss': 0.7418, 'learning_rate': 0.0002873211781206171, 'epoch': 0.5}                                                                                                        
{'loss': 0.8095, 'learning_rate': 0.00028727442730247776, 'epoch': 0.5}                                                                                                       
{'loss': 0.715, 'learning_rate': 0.00028722767648433847, 'epoch': 0.5}                                                                                                        
{'loss': 0.967, 'learning_rate': 0.0002871809256661991, 'epoch': 0.5}                                                                                                         
{'loss': 0.6869, 'learning_rate': 0.00028713417484805984, 'epoch': 0.5}                                                                                                       
{'loss': 0.9509, 'learning_rate': 0.0002870874240299205, 'epoch': 0.51}                                                                                                       
{'loss': 0.6777, 'learning_rate': 0.0002870406732117812, 'epoch': 0.51}                                                                                                       
{'loss': 0.8118, 'learning_rate': 0.00028699392239364186, 'epoch': 0.51}                                                                                                      
{'loss': 1.0262, 'learning_rate': 0.0002869471715755025, 'epoch': 0.51}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3300/64670 [4:51:52<3:53:33,  4.38it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9009046393891609, 'eval_cer': 0.47598074608904933, 'eval_runtime': 532.7133, 'eval_samples_per_second': 17.98, 'eval_steps_per_second': 2.249, 'epoch': 0.51}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-3300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3300/config.json                                                                   | 3300/64670 [5:00:45<3:53:33,  4.38it/s]
Model weights saved in ./turkish_clean/checkpoint-3300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6373, 'learning_rate': 0.00028690042075736323, 'epoch': 0.51}
{'loss': 0.6556, 'learning_rate': 0.0002868536699392239, 'epoch': 0.51}                                                                                                       
{'loss': 0.6646, 'learning_rate': 0.0002868069191210846, 'epoch': 0.51}                                                                                                       
{'loss': 0.7494, 'learning_rate': 0.00028676016830294526, 'epoch': 0.52}                                                                                                      
{'loss': 0.9412, 'learning_rate': 0.00028671341748480597, 'epoch': 0.52}                                                                                                      
{'loss': 1.8353, 'learning_rate': 0.0002866666666666667, 'epoch': 0.52}                                                                                                       
{'loss': 0.5972, 'learning_rate': 0.00028661991584852733, 'epoch': 0.52}                                                                                                      
{'loss': 0.5929, 'learning_rate': 0.000286573165030388, 'epoch': 0.52}                                                                                                        
{'loss': 0.7566, 'learning_rate': 0.0002865264142122487, 'epoch': 0.52}                                                                                                       
{'loss': 0.9845, 'learning_rate': 0.00028647966339410936, 'epoch': 0.53}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3400/64670 [5:01:18<3:55:09,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9433977923479127, 'eval_cer': 0.4897930204572804, 'eval_runtime': 525.5366, 'eval_samples_per_second': 18.225, 'eval_steps_per_second': 2.28, 'epoch': 0.53}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-3400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3400/config.json                                                                   | 3400/64670 [5:10:04<3:55:09,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-3400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-2900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6938, 'learning_rate': 0.00028643291257597, 'epoch': 0.53}
{'loss': 0.6644, 'learning_rate': 0.00028638616175783073, 'epoch': 0.53}                                                                                                      
{'loss': 0.7561, 'learning_rate': 0.00028633941093969144, 'epoch': 0.53}                                                                                                      
{'loss': 0.747, 'learning_rate': 0.0002862926601215521, 'epoch': 0.53}                                                                                                        
{'loss': 0.9984, 'learning_rate': 0.0002862459093034128, 'epoch': 0.53}                                                                                                       
{'loss': 0.6118, 'learning_rate': 0.00028619915848527346, 'epoch': 0.54}                                                                                                      
{'loss': 0.6298, 'learning_rate': 0.0002861524076671342, 'epoch': 0.54}                                                                                                       
{'loss': 0.6204, 'learning_rate': 0.00028610565684899483, 'epoch': 0.54}                                                                                                      
{'loss': 0.7134, 'learning_rate': 0.0002860589060308555, 'epoch': 0.54}                                                                                                       
{'loss': 0.7296, 'learning_rate': 0.0002860121552127162, 'epoch': 0.54}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3500/64670 [5:10:38<4:00:40,  4.24it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9413229313635987, 'eval_cer': 0.4784404332129964, 'eval_runtime': 530.2305, 'eval_samples_per_second': 18.064, 'eval_steps_per_second': 2.259, 'epoch': 0.54}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-3500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3500/config.json                                                                   | 3500/64670 [5:19:28<4:00:40,  4.24it/s]
Model weights saved in ./turkish_clean/checkpoint-3500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6089, 'learning_rate': 0.00028596540439457686, 'epoch': 0.54}
{'loss': 0.891, 'learning_rate': 0.00028591865357643757, 'epoch': 0.54}                                                                                                       
{'loss': 0.7043, 'learning_rate': 0.0002858719027582983, 'epoch': 0.55}                                                                                                       
{'loss': 0.6117, 'learning_rate': 0.00028582515194015894, 'epoch': 0.55}                                                                                                      
{'loss': 0.9049, 'learning_rate': 0.0002857784011220196, 'epoch': 0.55}                                                                                                       
{'loss': 1.1539, 'learning_rate': 0.0002857316503038803, 'epoch': 0.55}                                                                                                       
{'loss': 0.7549, 'learning_rate': 0.00028568489948574096, 'epoch': 0.55}                                                                                                      
{'loss': 0.6778, 'learning_rate': 0.00028563814866760167, 'epoch': 0.55}                                                                                                      
{'loss': 0.7157, 'learning_rate': 0.00028559139784946233, 'epoch': 0.56}                                                                                                      
{'loss': 0.9418, 'learning_rate': 0.00028554464703132304, 'epoch': 0.56}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3600/64670 [5:20:02<3:58:42,  4.26it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9294962237530086, 'eval_cer': 0.4932731648616125, 'eval_runtime': 531.7952, 'eval_samples_per_second': 18.011, 'eval_steps_per_second': 2.253, 'epoch': 0.56}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-3600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3600/config.json                                                                   | 3600/64670 [5:28:54<3:58:42,  4.26it/s]
Model weights saved in ./turkish_clean/checkpoint-3600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.4472, 'learning_rate': 0.0002854978962131837, 'epoch': 0.56}
{'loss': 0.6627, 'learning_rate': 0.0002854511453950444, 'epoch': 0.56}                                                                                                       
{'loss': 0.5035, 'learning_rate': 0.00028540439457690507, 'epoch': 0.56}                                                                                                      
{'loss': 0.7074, 'learning_rate': 0.0002853576437587658, 'epoch': 0.56}                                                                                                       
{'loss': 0.8802, 'learning_rate': 0.00028531089294062643, 'epoch': 0.56}                                                                                                      
{'loss': 0.654, 'learning_rate': 0.0002852641421224871, 'epoch': 0.57}                                                                                                        
{'loss': 0.6478, 'learning_rate': 0.0002852173913043478, 'epoch': 0.57}                                                                                                       
{'loss': 0.6882, 'learning_rate': 0.00028517064048620846, 'epoch': 0.57}                                                                                                      
{'loss': 0.6845, 'learning_rate': 0.00028512388966806917, 'epoch': 0.57}                                                                                                      
{'loss': 0.8156, 'learning_rate': 0.0002850771388499299, 'epoch': 0.57}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3700/64670 [5:29:27<4:02:38,  4.19it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9279193294049298, 'eval_cer': 0.5064283995186523, 'eval_runtime': 533.0029, 'eval_samples_per_second': 17.97, 'eval_steps_per_second': 2.248, 'epoch': 0.57}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-3700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3700/config.json                                                                   | 3700/64670 [5:38:20<4:02:38,  4.19it/s]
Model weights saved in ./turkish_clean/checkpoint-3700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5963, 'learning_rate': 0.00028503038803179054, 'epoch': 0.57}
{'loss': 0.6571, 'learning_rate': 0.00028498363721365125, 'epoch': 0.58}                                                                                                      
{'loss': 0.6858, 'learning_rate': 0.0002849368863955119, 'epoch': 0.58}                                                                                                       
{'loss': 0.6292, 'learning_rate': 0.00028489013557737256, 'epoch': 0.58}                                                                                                      
{'loss': 0.7458, 'learning_rate': 0.0002848433847592333, 'epoch': 0.58}                                                                                                       
{'loss': 0.6466, 'learning_rate': 0.00028479663394109393, 'epoch': 0.58}                                                                                                      
{'loss': 0.5708, 'learning_rate': 0.0002847498831229546, 'epoch': 0.58}                                                                                                       
{'loss': 0.8111, 'learning_rate': 0.0002847031323048153, 'epoch': 0.58}                                                                                                       
{'loss': 0.6416, 'learning_rate': 0.000284656381486676, 'epoch': 0.59}                                                                                                        
{'loss': 0.9094, 'learning_rate': 0.00028460963066853667, 'epoch': 0.59}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3800/64670 [5:38:54<3:53:44,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9049298696987302, 'eval_cer': 0.48657039711191336, 'eval_runtime': 525.4286, 'eval_samples_per_second': 18.229, 'eval_steps_per_second': 2.28, 'epoch': 0.59}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-3800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3800/config.json                                                                   | 3800/64670 [5:47:40<3:53:44,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-3800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.8935, 'learning_rate': 0.0002845628798503974, 'epoch': 0.59}
{'loss': 0.742, 'learning_rate': 0.00028451612903225803, 'epoch': 0.59}                                                                                                       
{'loss': 0.626, 'learning_rate': 0.00028446937821411875, 'epoch': 0.59}                                                                                                       
{'loss': 0.646, 'learning_rate': 0.0002844226273959794, 'epoch': 0.59}                                                                                                        
{'loss': 0.8679, 'learning_rate': 0.00028437587657784006, 'epoch': 0.6}                                                                                                       
{'loss': 0.6855, 'learning_rate': 0.00028432912575970077, 'epoch': 0.6}                                                                                                       
{'loss': 0.6035, 'learning_rate': 0.00028428237494156143, 'epoch': 0.6}                                                                                                       
{'loss': 0.6507, 'learning_rate': 0.00028423562412342214, 'epoch': 0.6}                                                                                                       
{'loss': 0.7733, 'learning_rate': 0.00028418887330528285, 'epoch': 0.6}                                                                                                       
{'loss': 0.9105, 'learning_rate': 0.0002841421224871435, 'epoch': 0.6}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3900/64670 [5:48:14<3:53:39,  4.33it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 1.0167233795335713, 'eval_cer': 0.4877208182912154, 'eval_runtime': 530.4292, 'eval_samples_per_second': 18.057, 'eval_steps_per_second': 2.259, 'epoch': 0.6}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-3900                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-3900/config.json                                                                   | 3900/64670 [5:57:04<3:53:39,  4.33it/s]
Model weights saved in ./turkish_clean/checkpoint-3900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-3900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.765, 'learning_rate': 0.00028409537166900416, 'epoch': 0.6}
{'loss': 0.7531, 'learning_rate': 0.0002840486208508649, 'epoch': 0.61}                                                                                                       
{'loss': 0.673, 'learning_rate': 0.00028400187003272553, 'epoch': 0.61}                                                                                                       
{'loss': 0.676, 'learning_rate': 0.00028395511921458624, 'epoch': 0.61}                                                                                                       
{'loss': 0.9175, 'learning_rate': 0.0002839083683964469, 'epoch': 0.61}                                                                                                       
{'loss': 0.6255, 'learning_rate': 0.0002838616175783076, 'epoch': 0.61}                                                                                                       
{'loss': 0.7387, 'learning_rate': 0.00028381486676016827, 'epoch': 0.61}                                                                                                      
{'loss': 0.6334, 'learning_rate': 0.000283768115942029, 'epoch': 0.62}                                                                                                        
{'loss': 0.6241, 'learning_rate': 0.00028372136512388964, 'epoch': 0.62}                                                                                                      
{'loss': 0.8316, 'learning_rate': 0.00028367461430575035, 'epoch': 0.62}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4000/64670 [5:57:38<4:00:01,  4.21it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.920864802058262, 'eval_cer': 0.4778170878459687, 'eval_runtime': 530.1862, 'eval_samples_per_second': 18.065, 'eval_steps_per_second': 2.26, 'epoch': 0.62}                                                                                                                                                                
Saving model checkpoint to ./turkish_clean/checkpoint-4000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4000/config.json                                                                   | 4000/64670 [6:06:28<4:00:01,  4.21it/s]
Model weights saved in ./turkish_clean/checkpoint-4000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6564, 'learning_rate': 0.000283627863487611, 'epoch': 0.62}
{'loss': 0.6186, 'learning_rate': 0.00028358111266947166, 'epoch': 0.62}                                                                                                      
{'loss': 0.6046, 'learning_rate': 0.00028353436185133237, 'epoch': 0.62}                                                                                                      
{'loss': 0.8036, 'learning_rate': 0.00028348761103319303, 'epoch': 0.62}                                                                                                      
{'loss': 0.9706, 'learning_rate': 0.00028344086021505374, 'epoch': 0.63}                                                                                                      
{'loss': 0.8463, 'learning_rate': 0.00028339410939691445, 'epoch': 0.63}                                                                                                      
{'loss': 0.5741, 'learning_rate': 0.0002833473585787751, 'epoch': 0.63}                                                                                                       
{'loss': 0.7508, 'learning_rate': 0.0002833006077606358, 'epoch': 0.63}                                                                                                       
{'loss': 0.4742, 'learning_rate': 0.0002832538569424965, 'epoch': 0.63}                                                                                                       
{'loss': 0.9002, 'learning_rate': 0.00028320710612435713, 'epoch': 0.63}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4100/64670 [6:07:02<3:52:23,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9077931778570836, 'eval_cer': 0.4851119133574007, 'eval_runtime': 532.702, 'eval_samples_per_second': 17.98, 'eval_steps_per_second': 2.249, 'epoch': 0.63}                                                                                                                                                                
Saving model checkpoint to ./turkish_clean/checkpoint-4100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4100/config.json                                                                   | 4100/64670 [6:15:55<3:52:23,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-4100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6145, 'learning_rate': 0.00028316035530621784, 'epoch': 0.64}
{'loss': 0.5904, 'learning_rate': 0.0002831136044880785, 'epoch': 0.64}                                                                                                       
{'loss': 0.6115, 'learning_rate': 0.0002830668536699392, 'epoch': 0.64}                                                                                                       
{'loss': 0.575, 'learning_rate': 0.00028302010285179987, 'epoch': 0.64}                                                                                                       
{'loss': 0.8192, 'learning_rate': 0.0002829733520336606, 'epoch': 0.64}                                                                                                       
{'loss': 1.4772, 'learning_rate': 0.00028292660121552124, 'epoch': 0.64}                                                                                                      
{'loss': 0.6411, 'learning_rate': 0.00028287985039738195, 'epoch': 0.64}                                                                                                      
{'loss': 0.5988, 'learning_rate': 0.0002828330995792426, 'epoch': 0.65}                                                                                                       
{'loss': 0.7007, 'learning_rate': 0.0002827863487611033, 'epoch': 0.65}                                                                                                       
{'loss': 0.8492, 'learning_rate': 0.000282739597942964, 'epoch': 0.65}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4200/64670 [6:16:29<3:55:24,  4.28it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.889098680388414, 'eval_cer': 0.47408904933814683, 'eval_runtime': 530.6276, 'eval_samples_per_second': 18.05, 'eval_steps_per_second': 2.258, 'epoch': 0.65}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-4200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4200/config.json                                                                   | 4200/64670 [6:25:19<3:55:24,  4.28it/s]
Model weights saved in ./turkish_clean/checkpoint-4200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.8087, 'learning_rate': 0.00028269284712482463, 'epoch': 0.65}
{'loss': 0.5617, 'learning_rate': 0.00028264609630668534, 'epoch': 0.65}                                                                                                      
{'loss': 0.6756, 'learning_rate': 0.00028259934548854605, 'epoch': 0.65}                                                                                                      
{'loss': 0.7046, 'learning_rate': 0.0002825525946704067, 'epoch': 0.66}                                                                                                       
{'loss': 0.6561, 'learning_rate': 0.0002825058438522674, 'epoch': 0.66}                                                                                                       
{'loss': 0.6393, 'learning_rate': 0.0002824590930341281, 'epoch': 0.66}                                                                                                       
{'loss': 0.714, 'learning_rate': 0.00028241234221598873, 'epoch': 0.66}                                                                                                       
{'loss': 0.5846, 'learning_rate': 0.00028236559139784945, 'epoch': 0.66}                                                                                                      
{'loss': 0.8327, 'learning_rate': 0.0002823188405797101, 'epoch': 0.66}                                                                                                       
{'loss': 0.8282, 'learning_rate': 0.0002822720897615708, 'epoch': 0.66}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4300/64670 [6:25:53<3:58:49,  4.21it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9016930865632002, 'eval_cer': 0.483971119133574, 'eval_runtime': 531.0379, 'eval_samples_per_second': 18.036, 'eval_steps_per_second': 2.256, 'epoch': 0.66}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-4300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4300/config.json                                                                   | 4300/64670 [6:34:44<3:58:49,  4.21it/s]
Model weights saved in ./turkish_clean/checkpoint-4300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.8173, 'learning_rate': 0.00028222533894343147, 'epoch': 0.67}
{'loss': 0.592, 'learning_rate': 0.0002821785881252922, 'epoch': 0.67}                                                                                                        
{'loss': 0.5927, 'learning_rate': 0.00028213183730715284, 'epoch': 0.67}                                                                                                      
{'loss': 0.5636, 'learning_rate': 0.00028208508648901355, 'epoch': 0.67}                                                                                                      
{'loss': 0.8989, 'learning_rate': 0.0002820383356708742, 'epoch': 0.67}                                                                                                       
{'loss': 0.6478, 'learning_rate': 0.0002819915848527349, 'epoch': 0.67}                                                                                                       
{'loss': 0.5937, 'learning_rate': 0.0002819448340345956, 'epoch': 0.68}                                                                                                       
{'loss': 0.5629, 'learning_rate': 0.00028189808321645623, 'epoch': 0.68}                                                                                                      
{'loss': 0.7438, 'learning_rate': 0.00028185133239831694, 'epoch': 0.68}                                                                                                      
{'loss': 1.0281, 'learning_rate': 0.0002818045815801776, 'epoch': 0.68}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4400/64670 [6:35:17<3:48:07,  4.40it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9112166984812018, 'eval_cer': 0.47430565583634177, 'eval_runtime': 532.9389, 'eval_samples_per_second': 17.972, 'eval_steps_per_second': 2.248, 'epoch': 0.68}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-4400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4400/config.json                                                                   | 4400/64670 [6:44:10<3:48:07,  4.40it/s]
Model weights saved in ./turkish_clean/checkpoint-4400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-3900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.0793, 'learning_rate': 0.0002817578307620383, 'epoch': 0.68}
{'loss': 0.5979, 'learning_rate': 0.000281711079943899, 'epoch': 0.68}                                                                                                        
{'loss': 0.5065, 'learning_rate': 0.0002816643291257597, 'epoch': 0.69}                                                                                                       
{'loss': 0.553, 'learning_rate': 0.0002816175783076204, 'epoch': 0.69}                                                                                                        
{'loss': 0.8832, 'learning_rate': 0.00028157082748948105, 'epoch': 0.69}                                                                                                      
{'loss': 0.6816, 'learning_rate': 0.0002815240766713417, 'epoch': 0.69}                                                                                                       
{'loss': 0.573, 'learning_rate': 0.0002814773258532024, 'epoch': 0.69}                                                                                                        
{'loss': 0.637, 'learning_rate': 0.00028143057503506307, 'epoch': 0.69}                                                                                                       
{'loss': 0.6293, 'learning_rate': 0.0002813838242169238, 'epoch': 0.69}                                                                                                       
{'loss': 0.84, 'learning_rate': 0.00028133707339878444, 'epoch': 0.7}                                                                                                         
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4500/64670 [6:44:44<3:41:44,  4.52it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9050336127479459, 'eval_cer': 0.4696413959085439, 'eval_runtime': 532.715, 'eval_samples_per_second': 17.98, 'eval_steps_per_second': 2.249, 'epoch': 0.7}                                                                                                                                                                 
Saving model checkpoint to ./turkish_clean/checkpoint-4500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4500/config.json                                                                   | 4500/64670 [6:53:37<3:41:44,  4.52it/s]
Model weights saved in ./turkish_clean/checkpoint-4500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6395, 'learning_rate': 0.00028129032258064515, 'epoch': 0.7}
{'loss': 1.0581, 'learning_rate': 0.0002812435717625058, 'epoch': 0.7}                                                                                                        
{'loss': 0.4894, 'learning_rate': 0.0002811968209443665, 'epoch': 0.7}                                                                                                        
{'loss': 0.6354, 'learning_rate': 0.0002811500701262272, 'epoch': 0.7}                                                                                                        
{'loss': 0.7226, 'learning_rate': 0.0002811033193080879, 'epoch': 0.7}                                                                                                        
{'loss': 0.578, 'learning_rate': 0.00028105656848994854, 'epoch': 0.71}                                                                                                       
{'loss': 0.5508, 'learning_rate': 0.0002810098176718092, 'epoch': 0.71}                                                                                                       
{'loss': 0.4868, 'learning_rate': 0.0002809630668536699, 'epoch': 0.71}                                                                                                       
{'loss': 0.5187, 'learning_rate': 0.0002809163160355306, 'epoch': 0.71}                                                                                                       
{'loss': 0.8488, 'learning_rate': 0.0002808695652173913, 'epoch': 0.71}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4600/64670 [6:54:11<4:01:31,  4.15it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.884969707029629, 'eval_cer': 0.4727220216606498, 'eval_runtime': 531.7045, 'eval_samples_per_second': 18.014, 'eval_steps_per_second': 2.253, 'epoch': 0.71}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-4600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4600/config.json                                                                   | 4600/64670 [7:03:02<4:01:31,  4.15it/s]
Model weights saved in ./turkish_clean/checkpoint-4600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.4362, 'learning_rate': 0.000280822814399252, 'epoch': 0.71}
{'loss': 0.5153, 'learning_rate': 0.00028077606358111265, 'epoch': 0.71}                                                                                                      
{'loss': 0.56, 'learning_rate': 0.0002807293127629733, 'epoch': 0.72}                                                                                                         
{'loss': 0.5514, 'learning_rate': 0.000280682561944834, 'epoch': 0.72}                                                                                                        
{'loss': 0.9305, 'learning_rate': 0.00028063581112669467, 'epoch': 0.72}                                                                                                      
{'loss': 0.6894, 'learning_rate': 0.0002805890603085554, 'epoch': 0.72}                                                                                                       
{'loss': 1.1492, 'learning_rate': 0.00028054230949041604, 'epoch': 0.72}                                                                                                      
{'loss': 0.729, 'learning_rate': 0.00028049555867227675, 'epoch': 0.72}                                                                                                       
{'loss': 0.8732, 'learning_rate': 0.00028044880785413746, 'epoch': 0.73}                                                                                                      
{'loss': 0.9293, 'learning_rate': 0.0002804020570359981, 'epoch': 0.73}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4700/64670 [7:03:36<3:54:55,  4.25it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8962984480039837, 'eval_cer': 0.48888327316486163, 'eval_runtime': 529.0885, 'eval_samples_per_second': 18.103, 'eval_steps_per_second': 2.264, 'epoch': 0.73}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-4700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4700/config.json                                                                   | 4700/64670 [7:12:25<3:54:55,  4.25it/s]
Model weights saved in ./turkish_clean/checkpoint-4700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6277, 'learning_rate': 0.0002803553062178588, 'epoch': 0.73}
{'loss': 0.5791, 'learning_rate': 0.0002803085553997195, 'epoch': 0.73}                                                                                                       
{'loss': 0.6503, 'learning_rate': 0.00028026180458158014, 'epoch': 0.73}                                                                                                      
{'loss': 0.8348, 'learning_rate': 0.0002802150537634408, 'epoch': 0.73}                                                                                                       
{'loss': 0.875, 'learning_rate': 0.0002801683029453015, 'epoch': 0.73}                                                                                                        
{'loss': 1.0678, 'learning_rate': 0.00028012155212716217, 'epoch': 0.74}                                                                                                      
{'loss': 0.5718, 'learning_rate': 0.0002800748013090229, 'epoch': 0.74}                                                                                                       
{'loss': 0.6079, 'learning_rate': 0.0002800280504908836, 'epoch': 0.74}                                                                                                       
{'loss': 0.6366, 'learning_rate': 0.00027998129967274425, 'epoch': 0.74}                                                                                                      
{'loss': 0.7278, 'learning_rate': 0.00027993454885460496, 'epoch': 0.74}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4800/64670 [7:12:59<3:50:36,  4.33it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8869408249647274, 'eval_cer': 0.4835980746089049, 'eval_runtime': 534.2143, 'eval_samples_per_second': 17.929, 'eval_steps_per_second': 2.243, 'epoch': 0.74}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-4800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4800/config.json                                                                   | 4800/64670 [7:21:53<3:50:36,  4.33it/s]
Model weights saved in ./turkish_clean/checkpoint-4800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5507, 'learning_rate': 0.0002798877980364656, 'epoch': 0.74}
{'loss': 0.5217, 'learning_rate': 0.0002798410472183263, 'epoch': 0.75}                                                                                                       
{'loss': 0.7218, 'learning_rate': 0.000279794296400187, 'epoch': 0.75}                                                                                                        
{'loss': 0.7183, 'learning_rate': 0.00027974754558204764, 'epoch': 0.75}                                                                                                      
{'loss': 0.6396, 'learning_rate': 0.00027970079476390835, 'epoch': 0.75}                                                                                                      
{'loss': 0.9318, 'learning_rate': 0.000279654043945769, 'epoch': 0.75}                                                                                                        
{'loss': 0.7644, 'learning_rate': 0.0002796072931276297, 'epoch': 0.75}                                                                                                       
{'loss': 0.5678, 'learning_rate': 0.0002795605423094904, 'epoch': 0.75}                                                                                                       
{'loss': 0.7894, 'learning_rate': 0.0002795137914913511, 'epoch': 0.76}                                                                                                       
{'loss': 0.7889, 'learning_rate': 0.00027946704067321175, 'epoch': 0.76}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 4900/64670 [7:22:28<3:49:19,  4.34it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8903850941986887, 'eval_cer': 0.48009386281588445, 'eval_runtime': 531.3633, 'eval_samples_per_second': 18.025, 'eval_steps_per_second': 2.255, 'epoch': 0.76}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-4900                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-4900/config.json                                                                   | 4900/64670 [7:31:19<3:49:19,  4.34it/s]
Model weights saved in ./turkish_clean/checkpoint-4900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-4900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.4823, 'learning_rate': 0.00027942028985507246, 'epoch': 0.76}
{'loss': 0.5375, 'learning_rate': 0.0002793735390369331, 'epoch': 0.76}                                                                                                       
{'loss': 0.4734, 'learning_rate': 0.00027932678821879377, 'epoch': 0.76}                                                                                                      
{'loss': 0.591, 'learning_rate': 0.0002792800374006545, 'epoch': 0.76}                                                                                                        
{'loss': 0.812, 'learning_rate': 0.0002792332865825152, 'epoch': 0.77}                                                                                                        
{'loss': 0.5664, 'learning_rate': 0.00027918653576437585, 'epoch': 0.77}                                                                                                      
{'loss': 0.5825, 'learning_rate': 0.00027913978494623656, 'epoch': 0.77}                                                                                                      
{'loss': 0.5907, 'learning_rate': 0.0002790930341280972, 'epoch': 0.77}                                                                                                       
{'loss': 0.5626, 'learning_rate': 0.0002790462833099579, 'epoch': 0.77}                                                                                                       
{'loss': 0.7453, 'learning_rate': 0.0002789995324918186, 'epoch': 0.77}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5000/64670 [7:31:53<3:54:59,  4.23it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.892646692671591, 'eval_cer': 0.46980264741275574, 'eval_runtime': 531.9551, 'eval_samples_per_second': 18.005, 'eval_steps_per_second': 2.252, 'epoch': 0.77}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5000/config.json                                                                   | 5000/64670 [7:40:45<3:54:59,  4.23it/s]
Model weights saved in ./turkish_clean/checkpoint-5000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6462, 'learning_rate': 0.00027895278167367924, 'epoch': 0.77}
{'loss': 0.5036, 'learning_rate': 0.00027890603085553995, 'epoch': 0.78}                                                                                                      
{'loss': 0.553, 'learning_rate': 0.0002788592800374006, 'epoch': 0.78}                                                                                                        
{'loss': 0.6673, 'learning_rate': 0.0002788125292192613, 'epoch': 0.78}                                                                                                       
{'loss': 0.8883, 'learning_rate': 0.00027876577840112203, 'epoch': 0.78}                                                                                                      
{'loss': 0.6003, 'learning_rate': 0.0002787190275829827, 'epoch': 0.78}                                                                                                       
{'loss': 0.5688, 'learning_rate': 0.00027867227676484335, 'epoch': 0.78}                                                                                                      
{'loss': 0.5361, 'learning_rate': 0.00027862552594670406, 'epoch': 0.79}                                                                                                      
{'loss': 0.5189, 'learning_rate': 0.0002785787751285647, 'epoch': 0.79}                                                                                                       
{'loss': 0.9743, 'learning_rate': 0.00027853202431042537, 'epoch': 0.79}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5100/64670 [7:41:19<3:47:22,  4.37it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8887667026309237, 'eval_cer': 0.47242839951865223, 'eval_runtime': 525.4111, 'eval_samples_per_second': 18.23, 'eval_steps_per_second': 2.28, 'epoch': 0.79}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-5100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5100/config.json                                                                   | 5100/64670 [7:50:05<3:47:22,  4.37it/s]
Model weights saved in ./turkish_clean/checkpoint-5100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.9861, 'learning_rate': 0.0002784852734922861, 'epoch': 0.79}
{'loss': 0.6012, 'learning_rate': 0.0002784385226741468, 'epoch': 0.79}                                                                                                       
{'loss': 0.6144, 'learning_rate': 0.00027839177185600745, 'epoch': 0.79}                                                                                                      
{'loss': 0.6906, 'learning_rate': 0.00027834502103786816, 'epoch': 0.79}                                                                                                      
{'loss': 0.8641, 'learning_rate': 0.0002782982702197288, 'epoch': 0.8}                                                                                                        
{'loss': 0.9366, 'learning_rate': 0.00027825151940158953, 'epoch': 0.8}                                                                                                       
{'loss': 0.4669, 'learning_rate': 0.0002782047685834502, 'epoch': 0.8}                                                                                                        
{'loss': 0.4828, 'learning_rate': 0.00027815801776531084, 'epoch': 0.8}                                                                                                       
{'loss': 0.5111, 'learning_rate': 0.00027811126694717156, 'epoch': 0.8}                                                                                                       
{'loss': 0.8716, 'learning_rate': 0.0002780645161290322, 'epoch': 0.8}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5200/64670 [7:50:39<3:52:12,  4.27it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8915262677400614, 'eval_cer': 0.46400240673886883, 'eval_runtime': 525.3052, 'eval_samples_per_second': 18.233, 'eval_steps_per_second': 2.281, 'epoch': 0.8}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5200/config.json                                                                   | 5200/64670 [7:59:24<3:52:12,  4.27it/s]
Model weights saved in ./turkish_clean/checkpoint-5200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5583, 'learning_rate': 0.0002780177653108929, 'epoch': 0.81}
{'loss': 0.5283, 'learning_rate': 0.0002779710144927536, 'epoch': 0.81}                                                                                                       
{'loss': 0.5578, 'learning_rate': 0.0002779242636746143, 'epoch': 0.81}                                                                                                       
{'loss': 0.5773, 'learning_rate': 0.00027787751285647495, 'epoch': 0.81}                                                                                                      
{'loss': 0.7411, 'learning_rate': 0.00027783076203833566, 'epoch': 0.81}                                                                                                      
{'loss': 0.5393, 'learning_rate': 0.0002777840112201963, 'epoch': 0.81}                                                                                                       
{'loss': 0.526, 'learning_rate': 0.00027773726040205703, 'epoch': 0.81}                                                                                                       
{'loss': 0.5281, 'learning_rate': 0.0002776905095839177, 'epoch': 0.82}                                                                                                       
{'loss': 0.6963, 'learning_rate': 0.00027764375876577834, 'epoch': 0.82}                                                                                                      
{'loss': 0.9229, 'learning_rate': 0.00027759700794763905, 'epoch': 0.82}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5300/64670 [7:59:58<3:41:53,  4.46it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8714831106315877, 'eval_cer': 0.466632972322503, 'eval_runtime': 530.6005, 'eval_samples_per_second': 18.051, 'eval_steps_per_second': 2.258, 'epoch': 0.82}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-5300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5300/config.json                                                                   | 5300/64670 [8:08:48<3:41:53,  4.46it/s]
Model weights saved in ./turkish_clean/checkpoint-5300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5466, 'learning_rate': 0.00027755025712949976, 'epoch': 0.82}
{'loss': 0.6264, 'learning_rate': 0.0002775035063113604, 'epoch': 0.82}                                                                                                       
{'loss': 0.5013, 'learning_rate': 0.00027745675549322113, 'epoch': 0.82}                                                                                                      
{'loss': 0.6649, 'learning_rate': 0.0002774100046750818, 'epoch': 0.83}                                                                                                       
{'loss': 0.6558, 'learning_rate': 0.00027736325385694245, 'epoch': 0.83}                                                                                                      
{'loss': 0.743, 'learning_rate': 0.00027731650303880316, 'epoch': 0.83}                                                                                                       
{'loss': 0.5553, 'learning_rate': 0.0002772697522206638, 'epoch': 0.83}                                                                                                       
{'loss': 0.7102, 'learning_rate': 0.0002772230014025245, 'epoch': 0.83}                                                                                                       
{'loss': 0.5688, 'learning_rate': 0.0002771762505843852, 'epoch': 0.83}                                                                                                       
{'loss': 0.7344, 'learning_rate': 0.0002771294997662459, 'epoch': 0.84}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5400/64670 [8:09:23<3:55:34,  4.19it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8907170719561789, 'eval_cer': 0.4635042117930205, 'eval_runtime': 529.8242, 'eval_samples_per_second': 18.078, 'eval_steps_per_second': 2.261, 'epoch': 0.84}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5400/config.json                                                                   | 5400/64670 [8:18:13<3:55:34,  4.19it/s]
Model weights saved in ./turkish_clean/checkpoint-5400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-4900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5608, 'learning_rate': 0.0002770827489481066, 'epoch': 0.84}
{'loss': 0.52, 'learning_rate': 0.00027703599812996726, 'epoch': 0.84}                                                                                                        
{'loss': 0.6216, 'learning_rate': 0.0002769892473118279, 'epoch': 0.84}                                                                                                       
{'loss': 0.609, 'learning_rate': 0.00027694249649368863, 'epoch': 0.84}                                                                                                       
{'loss': 0.5984, 'learning_rate': 0.0002768957456755493, 'epoch': 0.84}                                                                                                       
{'loss': 0.8573, 'learning_rate': 0.00027684899485740994, 'epoch': 0.84}                                                                                                      
{'loss': 0.557, 'learning_rate': 0.00027680224403927065, 'epoch': 0.85}                                                                                                       
{'loss': 0.5832, 'learning_rate': 0.00027675549322113137, 'epoch': 0.85}                                                                                                      
{'loss': 0.641, 'learning_rate': 0.000276708742402992, 'epoch': 0.85}                                                                                                         
{'loss': 0.8973, 'learning_rate': 0.00027666199158485273, 'epoch': 0.85}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5500/64670 [8:18:47<3:46:23,  4.36it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8590546933355465, 'eval_cer': 0.4628616125150421, 'eval_runtime': 531.2071, 'eval_samples_per_second': 18.031, 'eval_steps_per_second': 2.255, 'epoch': 0.85}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5500/config.json                                                                   | 5500/64670 [8:27:38<3:46:23,  4.36it/s]
Model weights saved in ./turkish_clean/checkpoint-5500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5727, 'learning_rate': 0.0002766152407667134, 'epoch': 0.85}
{'loss': 0.5283, 'learning_rate': 0.0002765684899485741, 'epoch': 0.85}                                                                                                       
{'loss': 0.5593, 'learning_rate': 0.00027652173913043476, 'epoch': 0.86}                                                                                                      
{'loss': 0.7025, 'learning_rate': 0.0002764749883122954, 'epoch': 0.86}                                                                                                       
{'loss': 0.9596, 'learning_rate': 0.0002764282374941561, 'epoch': 0.86}                                                                                                       
{'loss': 0.6907, 'learning_rate': 0.0002763814866760168, 'epoch': 0.86}                                                                                                       
{'loss': 0.4744, 'learning_rate': 0.0002763347358578775, 'epoch': 0.86}                                                                                                       
{'loss': 0.4716, 'learning_rate': 0.0002762879850397382, 'epoch': 0.86}                                                                                                       
{'loss': 0.5819, 'learning_rate': 0.00027624123422159886, 'epoch': 0.86}                                                                                                      
{'loss': 0.8194, 'learning_rate': 0.0002761944834034595, 'epoch': 0.87}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5600/64670 [8:28:12<3:48:48,  4.30it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8745331562785293, 'eval_cer': 0.46289530685920577, 'eval_runtime': 525.686, 'eval_samples_per_second': 18.22, 'eval_steps_per_second': 2.279, 'epoch': 0.87}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-5600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5600/config.json                                                                   | 5600/64670 [8:36:57<3:48:48,  4.30it/s]
Model weights saved in ./turkish_clean/checkpoint-5600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5144, 'learning_rate': 0.00027614773258532023, 'epoch': 0.87}
{'loss': 0.5082, 'learning_rate': 0.0002761009817671809, 'epoch': 0.87}                                                                                                       
{'loss': 0.4828, 'learning_rate': 0.0002760542309490416, 'epoch': 0.87}                                                                                                       
{'loss': 0.6278, 'learning_rate': 0.00027600748013090226, 'epoch': 0.87}                                                                                                      
{'loss': 0.8348, 'learning_rate': 0.00027596072931276297, 'epoch': 0.87}                                                                                                      
{'loss': 0.543, 'learning_rate': 0.0002759139784946236, 'epoch': 0.88}                                                                                                        
{'loss': 0.5725, 'learning_rate': 0.00027586722767648433, 'epoch': 0.88}                                                                                                      
{'loss': 0.5078, 'learning_rate': 0.000275820476858345, 'epoch': 0.88}                                                                                                        
{'loss': 0.6159, 'learning_rate': 0.0002757737260402057, 'epoch': 0.88}                                                                                                       
{'loss': 0.7547, 'learning_rate': 0.00027572697522206636, 'epoch': 0.88}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5700/64670 [8:37:31<4:00:36,  4.08it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.867395634492489, 'eval_cer': 0.46184837545126356, 'eval_runtime': 531.6194, 'eval_samples_per_second': 18.017, 'eval_steps_per_second': 2.253, 'epoch': 0.88}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5700/config.json                                                                   | 5700/64670 [8:46:23<4:00:36,  4.08it/s]
Model weights saved in ./turkish_clean/checkpoint-5700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7125, 'learning_rate': 0.000275680224403927, 'epoch': 0.88}
{'loss': 0.6488, 'learning_rate': 0.00027563347358578773, 'epoch': 0.88}                                                                                                      
{'loss': 0.6261, 'learning_rate': 0.0002755867227676484, 'epoch': 0.89}                                                                                                       
{'loss': 0.7446, 'learning_rate': 0.0002755399719495091, 'epoch': 0.89}                                                                                                       
{'loss': 0.818, 'learning_rate': 0.00027549322113136975, 'epoch': 0.89}                                                                                                       
{'loss': 0.5517, 'learning_rate': 0.00027544647031323046, 'epoch': 0.89}                                                                                                      
{'loss': 0.5011, 'learning_rate': 0.0002753997194950912, 'epoch': 0.89}                                                                                                       
{'loss': 0.421, 'learning_rate': 0.00027535296867695183, 'epoch': 0.89}                                                                                                       
{'loss': 0.5615, 'learning_rate': 0.0002753062178588125, 'epoch': 0.9}                                                                                                        
{'loss': 0.9624, 'learning_rate': 0.0002752594670406732, 'epoch': 0.9}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5800/64670 [8:46:56<3:49:29,  4.28it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8603826043655075, 'eval_cer': 0.4618965102286402, 'eval_runtime': 532.5331, 'eval_samples_per_second': 17.986, 'eval_steps_per_second': 2.25, 'epoch': 0.9}                                                                                                                                                                
Saving model checkpoint to ./turkish_clean/checkpoint-5800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5800/config.json                                                                   | 5800/64670 [8:55:49<3:49:29,  4.28it/s]
Model weights saved in ./turkish_clean/checkpoint-5800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5704, 'learning_rate': 0.00027521271622253386, 'epoch': 0.9}
{'loss': 0.5806, 'learning_rate': 0.0002751659654043945, 'epoch': 0.9}                                                                                                        
{'loss': 0.9929, 'learning_rate': 0.0002751192145862552, 'epoch': 0.9}                                                                                                        
{'loss': 0.6231, 'learning_rate': 0.00027507246376811594, 'epoch': 0.9}                                                                                                       
{'loss': 1.0258, 'learning_rate': 0.0002750257129499766, 'epoch': 0.9}                                                                                                        
{'loss': 0.9828, 'learning_rate': 0.0002749789621318373, 'epoch': 0.91}                                                                                                       
{'loss': 0.6462, 'learning_rate': 0.00027493221131369796, 'epoch': 0.91}                                                                                                      
{'loss': 0.6407, 'learning_rate': 0.00027488546049555867, 'epoch': 0.91}                                                                                                      
{'loss': 0.6374, 'learning_rate': 0.00027483870967741933, 'epoch': 0.91}                                                                                                      
{'loss': 0.8083, 'learning_rate': 0.00027479195885928, 'epoch': 0.91}                                                                                                         
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 5900/64670 [8:56:23<3:41:46,  4.42it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8767740061415885, 'eval_cer': 0.4613694344163658, 'eval_runtime': 531.1258, 'eval_samples_per_second': 18.033, 'eval_steps_per_second': 2.256, 'epoch': 0.91}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-5900                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-5900/config.json                                                                   | 5900/64670 [9:05:15<3:41:46,  4.42it/s]
Model weights saved in ./turkish_clean/checkpoint-5900/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-5900/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5400] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.2005, 'learning_rate': 0.0002747452080411407, 'epoch': 0.91}
{'loss': 0.5635, 'learning_rate': 0.00027469845722300135, 'epoch': 0.92}                                                                                                      
{'loss': 0.4873, 'learning_rate': 0.00027465170640486207, 'epoch': 0.92}                                                                                                      
{'loss': 0.5852, 'learning_rate': 0.0002746049555867228, 'epoch': 0.92}                                                                                                       
{'loss': 0.7723, 'learning_rate': 0.00027455820476858343, 'epoch': 0.92}                                                                                                      
{'loss': 0.4742, 'learning_rate': 0.0002745114539504441, 'epoch': 0.92}                                                                                                       
{'loss': 0.6118, 'learning_rate': 0.0002744647031323048, 'epoch': 0.92}                                                                                                       
{'loss': 0.5572, 'learning_rate': 0.00027441795231416546, 'epoch': 0.92}                                                                                                      
{'loss': 0.5781, 'learning_rate': 0.00027437120149602617, 'epoch': 0.93}                                                                                                      
{'loss': 0.5867, 'learning_rate': 0.0002743244506778868, 'epoch': 0.93}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6000/64670 [9:05:49<3:52:28,  4.21it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8496555730766039, 'eval_cer': 0.4595836341756919, 'eval_runtime': 531.7048, 'eval_samples_per_second': 18.014, 'eval_steps_per_second': 2.253, 'epoch': 0.93}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-6000                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6000/config.json                                                                   | 6000/64670 [9:14:41<3:52:28,  4.21it/s]
Model weights saved in ./turkish_clean/checkpoint-6000/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6000/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5500] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.707, 'learning_rate': 0.00027427769985974754, 'epoch': 0.93}
{'loss': 0.4607, 'learning_rate': 0.0002742309490416082, 'epoch': 0.93}                                                                                                       
{'loss': 0.5483, 'learning_rate': 0.0002741841982234689, 'epoch': 0.93}                                                                                                       
{'loss': 0.5723, 'learning_rate': 0.00027413744740532956, 'epoch': 0.93}                                                                                                      
{'loss': 0.8329, 'learning_rate': 0.0002740906965871903, 'epoch': 0.94}                                                                                                       
{'loss': 0.7015, 'learning_rate': 0.00027404394576905093, 'epoch': 0.94}                                                                                                      
{'loss': 0.7652, 'learning_rate': 0.0002739971949509116, 'epoch': 0.94}                                                                                                       
{'loss': 0.4337, 'learning_rate': 0.0002739504441327723, 'epoch': 0.94}                                                                                                       
{'loss': 0.6546, 'learning_rate': 0.00027390369331463296, 'epoch': 0.94}                                                                                                      
{'loss': 0.6912, 'learning_rate': 0.00027385694249649367, 'epoch': 0.94}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6100/64670 [9:15:16<3:51:44,  4.21it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.9148269565939082, 'eval_cer': 0.46642117930204574, 'eval_runtime': 532.7278, 'eval_samples_per_second': 17.979, 'eval_steps_per_second': 2.249, 'epoch': 0.94}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-6100                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6100/config.json                                                                   | 6100/64670 [9:24:08<3:51:44,  4.21it/s]
Model weights saved in ./turkish_clean/checkpoint-6100/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6100/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5600] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.4738, 'learning_rate': 0.0002738101916783544, 'epoch': 0.94}
{'loss': 0.5037, 'learning_rate': 0.00027376344086021503, 'epoch': 0.95}                                                                                                      
{'loss': 0.6558, 'learning_rate': 0.00027371669004207575, 'epoch': 0.95}                                                                                                      
{'loss': 0.5775, 'learning_rate': 0.0002736699392239364, 'epoch': 0.95}                                                                                                       
{'loss': 0.7333, 'learning_rate': 0.00027362318840579706, 'epoch': 0.95}                                                                                                      
{'loss': 0.562, 'learning_rate': 0.00027357643758765777, 'epoch': 0.95}                                                                                                       
{'loss': 0.435, 'learning_rate': 0.00027352968676951843, 'epoch': 0.95}                                                                                                       
{'loss': 0.5264, 'learning_rate': 0.00027348293595137914, 'epoch': 0.96}                                                                                                      
{'loss': 0.7286, 'learning_rate': 0.0002734361851332398, 'epoch': 0.96}                                                                                                       
{'loss': 0.8158, 'learning_rate': 0.0002733894343151005, 'epoch': 0.96}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6200/64670 [9:24:43<3:39:33,  4.44it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.879948543447589, 'eval_cer': 0.4658387484957882, 'eval_runtime': 526.5986, 'eval_samples_per_second': 18.188, 'eval_steps_per_second': 2.275, 'epoch': 0.96}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-6200                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6200/config.json                                                                   | 6200/64670 [9:33:29<3:39:33,  4.44it/s]
Model weights saved in ./turkish_clean/checkpoint-6200/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6200/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5700] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.7156, 'learning_rate': 0.00027334268349696116, 'epoch': 0.96}
{'loss': 0.5428, 'learning_rate': 0.0002732959326788219, 'epoch': 0.96}                                                                                                       
{'loss': 0.4072, 'learning_rate': 0.00027324918186068253, 'epoch': 0.96}                                                                                                      
{'loss': 0.5964, 'learning_rate': 0.00027320243104254324, 'epoch': 0.96}                                                                                                      
{'loss': 0.6623, 'learning_rate': 0.0002731556802244039, 'epoch': 0.97}                                                                                                       
{'loss': 0.5108, 'learning_rate': 0.00027310892940626456, 'epoch': 0.97}                                                                                                      
{'loss': 0.6023, 'learning_rate': 0.00027306217858812527, 'epoch': 0.97}                                                                                                      
{'loss': 0.4533, 'learning_rate': 0.0002730154277699859, 'epoch': 0.97}                                                                                                       
{'loss': 0.5267, 'learning_rate': 0.00027296867695184664, 'epoch': 0.97}                                                                                                      
{'loss': 0.8048, 'learning_rate': 0.00027292192613370735, 'epoch': 0.97}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6300/64670 [9:34:04<3:48:57,  4.25it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8834758071209229, 'eval_cer': 0.461898916967509, 'eval_runtime': 530.9725, 'eval_samples_per_second': 18.039, 'eval_steps_per_second': 2.256, 'epoch': 0.97}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-6300                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6300/config.json                                                                   | 6300/64670 [9:42:54<3:48:57,  4.25it/s]
Model weights saved in ./turkish_clean/checkpoint-6300/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6300/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5800] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6155, 'learning_rate': 0.000272875175315568, 'epoch': 0.98}
{'loss': 0.6298, 'learning_rate': 0.00027282842449742866, 'epoch': 0.98}                                                                                                      
{'loss': 0.5843, 'learning_rate': 0.00027278167367928937, 'epoch': 0.98}                                                                                                      
{'loss': 0.5929, 'learning_rate': 0.00027273492286115003, 'epoch': 0.98}                                                                                                      
{'loss': 0.6815, 'learning_rate': 0.00027268817204301074, 'epoch': 0.98}                                                                                                      
{'loss': 0.488, 'learning_rate': 0.0002726414212248714, 'epoch': 0.98}                                                                                                        
{'loss': 0.5072, 'learning_rate': 0.0002725946704067321, 'epoch': 0.99}                                                                                                       
{'loss': 0.6291, 'learning_rate': 0.00027254791958859276, 'epoch': 0.99}                                                                                                      
{'loss': 0.6424, 'learning_rate': 0.0002725011687704535, 'epoch': 0.99}                                                                                                       
{'loss': 0.824, 'learning_rate': 0.00027245441795231413, 'epoch': 0.99}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6400/64670 [9:43:29<3:46:06,  4.30it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8965474313221015, 'eval_cer': 0.46360048134777376, 'eval_runtime': 525.7844, 'eval_samples_per_second': 18.217, 'eval_steps_per_second': 2.279, 'epoch': 0.99}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-6400                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6400/config.json                                                                   | 6400/64670 [9:52:15<3:46:06,  4.30it/s]
Model weights saved in ./turkish_clean/checkpoint-6400/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6400/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-5900] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5255, 'learning_rate': 0.00027240766713417484, 'epoch': 0.99}
{'loss': 0.5568, 'learning_rate': 0.0002723609163160355, 'epoch': 0.99}                                                                                                       
{'loss': 0.521, 'learning_rate': 0.00027231416549789616, 'epoch': 0.99}                                                                                                       
{'loss': 0.4926, 'learning_rate': 0.00027226741467975687, 'epoch': 1.0}                                                                                                       
{'loss': 0.6688, 'learning_rate': 0.0002722206638616175, 'epoch': 1.0}                                                                                                        
{'loss': 0.5679, 'learning_rate': 0.00027217391304347824, 'epoch': 1.0}                                                                                                       
{'loss': 0.6872, 'learning_rate': 0.00027212716222533895, 'epoch': 1.0}                                                                                                       
{'loss': 0.433, 'learning_rate': 0.0002720804114071996, 'epoch': 1.0}                                                                                                         
{'loss': 0.5931, 'learning_rate': 0.0002720336605890603, 'epoch': 1.0}                                                                                                        
{'loss': 0.4946, 'learning_rate': 0.000271986909770921, 'epoch': 1.01}                                                                                                        
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 6500/64670 [9:52:50<4:25:05,  3.66it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8610673084903312, 'eval_cer': 0.4583489771359808, 'eval_runtime': 526.002, 'eval_samples_per_second': 18.209, 'eval_steps_per_second': 2.278, 'epoch': 1.01}                                                                                                                                                               
Saving model checkpoint to ./turkish_clean/checkpoint-6500                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6500/config.json                                                                  | 6500/64670 [10:01:36<4:25:05,  3.66it/s]
Model weights saved in ./turkish_clean/checkpoint-6500/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6500/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-6000] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6095, 'learning_rate': 0.00027194015895278163, 'epoch': 1.01}
{'loss': 0.9067, 'learning_rate': 0.00027189340813464234, 'epoch': 1.01}                                                                                                      
{'loss': 0.4759, 'learning_rate': 0.000271846657316503, 'epoch': 1.01}                                                                                                        
{'loss': 0.4476, 'learning_rate': 0.0002717999064983637, 'epoch': 1.01}                                                                                                       
{'loss': 0.5053, 'learning_rate': 0.00027175315568022437, 'epoch': 1.01}                                                                                                      
{'loss': 0.4904, 'learning_rate': 0.0002717064048620851, 'epoch': 1.01}                                                                                                       
{'loss': 0.712, 'learning_rate': 0.00027165965404394573, 'epoch': 1.02}                                                                                                       
{'loss': 0.5506, 'learning_rate': 0.00027161290322580645, 'epoch': 1.02}                                                                                                      
{'loss': 0.5619, 'learning_rate': 0.0002715661524076671, 'epoch': 1.02}                                                                                                       
{'loss': 0.8499, 'learning_rate': 0.0002715194015895278, 'epoch': 1.02}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                      | 6600/64670 [10:02:11<4:38:57,  3.47it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8477052037513486, 'eval_cer': 0.4577641395908544, 'eval_runtime': 531.6284, 'eval_samples_per_second': 18.016, 'eval_steps_per_second': 2.253, 'epoch': 1.02}                                                                                                                                                              
Saving model checkpoint to ./turkish_clean/checkpoint-6600                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6600/config.json                                                                  | 6600/64670 [10:11:02<4:38:57,  3.47it/s]
Model weights saved in ./turkish_clean/checkpoint-6600/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6600/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-6100] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.523, 'learning_rate': 0.00027147265077138847, 'epoch': 1.02}
{'loss': 0.6674, 'learning_rate': 0.00027142589995324913, 'epoch': 1.02}                                                                                                      
{'loss': 0.4637, 'learning_rate': 0.00027137914913510984, 'epoch': 1.03}                                                                                                      
{'loss': 0.471, 'learning_rate': 0.00027133239831697055, 'epoch': 1.03}                                                                                                       
{'loss': 0.4245, 'learning_rate': 0.0002712856474988312, 'epoch': 1.03}                                                                                                       
{'loss': 0.575, 'learning_rate': 0.0002712388966806919, 'epoch': 1.03}                                                                                                        
{'loss': 0.5852, 'learning_rate': 0.0002711921458625526, 'epoch': 1.03}                                                                                                       
{'loss': 0.5165, 'learning_rate': 0.00027114539504441323, 'epoch': 1.03}                                                                                                      
{'loss': 0.4466, 'learning_rate': 0.00027109864422627394, 'epoch': 1.03}                                                                                                      
{'loss': 0.5365, 'learning_rate': 0.0002710518934081346, 'epoch': 1.04}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                      | 6700/64670 [10:11:37<4:26:01,  3.63it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.861482280687194, 'eval_cer': 0.46256317689530685, 'eval_runtime': 525.3991, 'eval_samples_per_second': 18.23, 'eval_steps_per_second': 2.28, 'epoch': 1.04}                                                                                                                                                                
Saving model checkpoint to ./turkish_clean/checkpoint-6700                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6700/config.json                                                                  | 6700/64670 [10:20:23<4:26:01,  3.63it/s]
Model weights saved in ./turkish_clean/checkpoint-6700/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6700/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-6200] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5506, 'learning_rate': 0.0002710051425899953, 'epoch': 1.04}
{'loss': 0.7819, 'learning_rate': 0.00027095839177185597, 'epoch': 1.04}                                                                                                      
{'loss': 0.5242, 'learning_rate': 0.0002709116409537167, 'epoch': 1.04}                                                                                                       
{'loss': 0.516, 'learning_rate': 0.00027086489013557734, 'epoch': 1.04}                                                                                                       
{'loss': 0.614, 'learning_rate': 0.00027081813931743805, 'epoch': 1.04}                                                                                                       
{'loss': 0.4228, 'learning_rate': 0.0002707713884992987, 'epoch': 1.05}                                                                                                       
{'loss': 0.669, 'learning_rate': 0.0002707246376811594, 'epoch': 1.05}                                                                                                        
{'loss': 0.4801, 'learning_rate': 0.00027067788686302007, 'epoch': 1.05}                                                                                                      
{'loss': 0.5265, 'learning_rate': 0.00027063113604488073, 'epoch': 1.05}                                                                                                      
{'loss': 0.5196, 'learning_rate': 0.00027058438522674144, 'epoch': 1.05}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                      | 6800/64670 [10:20:57<4:29:53,  3.57it/s]
  Num examples = 9578
  Batch size = 8
{'eval_loss': inf, 'eval_wer': 0.8444891692256619, 'eval_cer': 0.46036341756919374, 'eval_runtime': 532.6227, 'eval_samples_per_second': 17.983, 'eval_steps_per_second': 2.249, 'epoch': 1.05}                                                                                                                                                             
Saving model checkpoint to ./turkish_clean/checkpoint-6800                                                                                                                    
Configuration saved in ./turkish_clean/checkpoint-6800/config.json                                                                  | 6800/64670 [10:29:50<4:29:53,  3.57it/s]
Model weights saved in ./turkish_clean/checkpoint-6800/pytorch_model.bin
Feature extractor saved in ./turkish_clean/checkpoint-6800/preprocessor_config.json
Deleting older checkpoint [turkish_clean/checkpoint-6300] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                      | 6900/64670 [10:30:25<4:40:13,  3.44it/s]
  Num examples = 9578
  Batch size = 8
 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                   | 455/1198 [01:49<02:23,  5.16it/s]^CTraceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2964, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2343, in __getitem__
    return self._getitem(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2328, in _getitem
    formatted_output = format_table(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 517, in format_table
    formatted_output = formatter(pa_table_to_format, query_type=query_type)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 282, in __call__
    return self.format_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 311, in format_row
    row = self.python_arrow_extractor().extract_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 141, in extract_row
    return _unnest(pa_table.to_pydict())
KeyboardInterrupt
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                | 6900/64670 [10:32:15<88:13:32,  5.50s/it]
                                                                                                                                                                              
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.codevectors', 'project_q.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'project_hid.weight', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:17<00:00, 38.60it/s]
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 6900/64670 [00:49<4:26:19,  3.62it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:16<01:10,  2.68it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                      | 6900/64670 [05:06<42:48, 22.49it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.codevectors', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:17<00:00, 37.17it/s]
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 6900/64670 [00:50<4:30:08,  3.56it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:21<01:10,  2.67it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                      | 6900/64670 [05:12<43:37, 22.07it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'quantizer.weight_proj.bias', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:17<00:00, 37.27it/s]
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 6900/64670 [00:50<4:30:18,  3.56it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:20<01:10,  2.66it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                      | 6900/64670 [05:11<43:30, 22.13it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.bias', 'project_q.weight', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:17<00:00, 37.57it/s]
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 6900/64670 [00:49<4:30:24,  3.56it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:19<01:10,  2.68it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                      | 6900/64670 [05:09<43:14, 22.26it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'project_hid.weight', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'quantizer.codevectors', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:16<00:00, 39.54it/s]
{'loss': 0.6044, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}
{'loss': 0.6832, 'learning_rate': 0.0002704908835904628, 'epoch': 1.05}                                                                                                       
{'loss': 0.7486, 'learning_rate': 0.0002704441327723235, 'epoch': 1.06}                                                                                                       
{'loss': 0.4164, 'learning_rate': 0.0002703973819541842, 'epoch': 1.06}                                                                                                       
{'loss': 0.4938, 'learning_rate': 0.0002703506311360449, 'epoch': 1.06}                                                                                                       
{'loss': 0.5601, 'learning_rate': 0.00027030388031790554, 'epoch': 1.06}                                                                                                      
{'loss': 0.5763, 'learning_rate': 0.0002702571294997662, 'epoch': 1.06}                                                                                                       
{'loss': 0.5604, 'learning_rate': 0.0002702103786816269, 'epoch': 1.06}                                                                                                       
{'loss': 0.4931, 'learning_rate': 0.00027016362786348757, 'epoch': 1.07}                                                                                                      
{'loss': 0.6176, 'learning_rate': 0.0002701168770453483, 'epoch': 1.07}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 6900/64670 [00:48<4:25:29,  3.63it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:14<01:09,  2.72it/s]Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                      | 6900/64670 [05:03<42:24, 22.71it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main.py                                                                                                                         
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
----------------- Loading Datasets complete. ----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.codevectors', 'project_hid.weight', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./turkish_clean/checkpoint-6800.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 25867
  Num Epochs = 10
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 2
  Total optimization steps = 64670
  Number of trainable parameters = 311286969
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 6800
  Will skip the first 1 epochs then the first 666 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                     | 0/666 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:18<00:00, 36.88it/s]
{'loss': 0.6476, 'learning_rate': 0.0002705610098176718, 'epoch': 1.05}
{'loss': 0.5613, 'learning_rate': 0.0002705376344086021, 'epoch': 1.05}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                          | 6810/64670 [00:21<02:42, 355.94it/s]
  Num examples = 9578
  Batch size = 8
 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 1010/1198 [04:20<01:10,  2.68it/s]Traceback (most recent call last):                                                                                                      | 6810/64670 [00:29<02:42, 355.94it/s]
  File "/home/or/Desktop/wav2vec2/main.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2987, in evaluation_loop
    labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 115, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer_pt_utils.py", line 80, in torch_pad_and_concatenate
    result = tensor1.new_full(new_shape, padding_index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 15.75 GiB total capacity; 8.46 GiB already allocated; 1.84 GiB free; 12.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                      | 6810/64670 [04:42<40:03, 24.08it/s]
(base) or@anidjar:~/Desktop/wav2vec2$ cd loading_data_trying/
(base) or@anidjar:~/Desktop/wav2vec2/loading_data_trying$ python3 saving_datasets_to_arrow.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Downloading and preparing dataset common_voice_11_0/pt to /home/or/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/pt/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f...
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 525M/525M [01:19<00:00, 6.59MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274M/274M [00:42<00:00, 6.44MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 286M/286M [00:43<00:00, 6.56MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436M/436M [01:11<00:00, 6.11MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162M/162M [00:26<00:00, 6.02MB/s]
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [04:38<00:00, 55.77s/it]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:04<00:00,  1.00it/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.04M/4.04M [00:01<00:00, 3.09MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.91M/1.91M [00:00<00:00, 1.97MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.86M/1.86M [00:00<00:00, 2.28MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.63M/3.63M [00:01<00:00, 2.84MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.09M/1.09M [00:00<00:00, 1.49MB/s]
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:23<00:00,  4.75s/it]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 5564.21it/s]
Reading metadata...: 18211it [00:00, 273270.81it/s]es/s]
Reading metadata...: 8688it [00:00, 296058.90it/s]examples/s]     
Reading metadata...: 8693it [00:00, 302481.21it/s]es/s]               
Reading metadata...: 16751it [00:00, 298725.68it/s]es/s]        
Reading metadata...: 4870it [00:00, 296884.69it/s] examples/s]    
Dataset common_voice_11_0 downloaded and prepared to /home/or/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/pt/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f. Subsequent calls will reuse this data.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 1244.97it/s]
Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/loading_data_trying/saving_datasets_to_arrow.py", line 14, in <module>
    train.save_to_disk('/hoome/or/Desktop/portu')
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/dataset_dict.py", line 1073, in save_to_disk
    os.makedirs(dest_dataset_dict_path, exist_ok=True)
  File "/home/or/anaconda3/lib/python3.9/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/home/or/anaconda3/lib/python3.9/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/home/or/anaconda3/lib/python3.9/os.py", line 215, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/home/or/anaconda3/lib/python3.9/os.py", line 225, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/hoome'
(base) or@anidjar:~/Desktop/wav2vec2/loading_data_trying$ python3 saving_datasets_to_arrow.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Found cached dataset common_voice_11_0 (/home/or/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/pt/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 1183.56it/s]
----------------- Loading Datasets complete. -----------------


----------------- Removing special characters... -----------------
Parameter 'function'=<function remove_special_characters at 0x7fd3916f23a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|                                                                                                                                               | 0/18211 [00:00<?, ?ex/s]
Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/loading_data_trying/saving_datasets_to_arrow.py", line 37, in <module>
    train = train.map(remove_special_characters)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/dataset_dict.py", line 776, in map
    {
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/dataset_dict.py", line 777, in <dictcomp>
    k: dataset.map(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2572, in map
    return self._map_single(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 584, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 551, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/fingerprint.py", line 480, in wrapper
    out = func(self, *args, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2953, in _map_single
    example = apply_function_on_filtered_inputs(example, i, offset=offset)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2852, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2532, in decorated
    result = f(decorated_item, *args, **kwargs)
  File "/home/or/Desktop/wav2vec2/loading_data_trying/saving_datasets_to_arrow.py", line 31, in remove_special_characters
    batch["sentence"] = re.sub(chars_to_ignore_regex, '',
NameError: name 'chars_to_ignore_regex' is not defined
(base) or@anidjar:~/Desktop/wav2vec2/loading_data_trying$ cd ..
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_portu.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Using custom data configuration default-e495d5cc8d0ebc57
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15709.00it/s]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2830.16it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1009.70it/s]
Using custom data configuration default-2717b34285018d3a
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 15709.00it/s]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3134.76it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1255.40it/s]
Casting the dataset:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 1/2 [00:00<00:00, 13.53ba/s]
Casting the dataset:   0%|                                                                                                                              | 0/1 [00:00<?, ?ba/s]
^CTraceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main_portu.py", line 65, in <module>
    train['audio']
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2343, in __getitem__
    return self._getitem(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2328, in _getitem
    formatted_output = format_table(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 512, in format_table
    return formatter(pa_table, query_type)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 284, in __call__
    return self.format_column(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 319, in format_column
    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 225, in decode_column
    return self.features.decode_column(column, column_name) if self.features else column
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1811, in decode_column
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1811, in <listcomp>
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1262, in decode_nested_example
    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 145, in decode_example
    array, sampling_rate = self._decode_mp3(file if file else path)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 296, in _decode_mp3
    array, sampling_rate = self._decode_mp3_torchaudio(path_or_file)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 325, in _decode_mp3_torchaudio
    array, sampling_rate = torchaudio.load(path_or_file, format="mp3")
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py", line 227, in load
    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/io/_compat.py", line 103, in load_audio
    return _load_audio(s, frame_offset, num_frames, convert, channels_first)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/io/_compat.py", line 84, in _load_audio
    s.process_all_packets()
KeyboardInterrupt

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_portu.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Using custom data configuration default-e495d5cc8d0ebc57
Found cached dataset csv (/home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1112.25it/s]
Using custom data configuration default-2717b34285018d3a
Found cached dataset csv (/home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1391.15it/s]
Loading cached processed dataset at /home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-801fbde40c46a107.arrow
Loading cached processed dataset at /home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4d6671007b656f8b.arrow
^CTraceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main_portu.py", line 66, in <module>
    validation['audio']
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2343, in __getitem__
    return self._getitem(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2328, in _getitem
    formatted_output = format_table(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 512, in format_table
    return formatter(pa_table, query_type)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 284, in __call__
    return self.format_column(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 319, in format_column
    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 225, in decode_column
    return self.features.decode_column(column, column_name) if self.features else column
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1811, in decode_column
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1811, in <listcomp>
    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/features.py", line 1262, in decode_nested_example
    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 145, in decode_example
    array, sampling_rate = self._decode_mp3(file if file else path)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 296, in _decode_mp3
    array, sampling_rate = self._decode_mp3_torchaudio(path_or_file)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/features/audio.py", line 325, in _decode_mp3_torchaudio
    array, sampling_rate = torchaudio.load(path_or_file, format="mp3")
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py", line 227, in load
    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/io/_compat.py", line 103, in load_audio
    return _load_audio(s, frame_offset, num_frames, convert, channels_first)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torchaudio/io/_compat.py", line 84, in _load_audio
    s.process_all_packets()
KeyboardInterrupt

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_portu.py
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Using custom data configuration default-e495d5cc8d0ebc57
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14563.56it/s]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 2146.52it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-e495d5cc8d0ebc57/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1153.55it/s]
Using custom data configuration default-2717b34285018d3a
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 17403.75it/s]
Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 3663.15it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-2717b34285018d3a/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1323.96it/s]
Casting the dataset:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                           | 1/2 [00:00<00:00, 14.14ba/s]
Casting the dataset:   0%|                                                                                                                              | 0/1 [00:00<?, ?ba/s]
----------------- Loading Datasets complete. -----------------


----------------- Removing special characters... -----------------
Parameter 'function'=<function remove_special_characters at 0x7f9b67f8dc10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17960/17960 [00:00<00:00, 36965.14ex/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8688/8688 [00:00<00:00, 39870.23ex/s]
----------------- Removing special characters complete. -----------------


----------------- Extracting all characters... -----------------
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
----------------- Extracting all characters complete. -----------------


----------------- Preparing vocab... -----------------
Vocab_dict: {'j': 0, 'm': 1, '¬ª': 2, '√©': 3, '5': 4, '6': 5, 'b': 6, 'y': 7, 'l': 8, 'a': 9, '√≠': 10, 'v': 11, '√∫': 12, 'k': 13, 'r': 14, 'o': 15, '√¥': 16, ' ': 17, '√†': 18, '\t': 19, '√µ': 20, '¬¥': 21, '√£': 22, '√°': 23, '0': 24, 'z': 25, 'n': 26, '1': 27, '8': 28, '7': 29, 'x': 30, '√™': 31, 'e': 32, 'f': 33, '2': 34, '≈æ': 35, '9': 36, '4': 37, 'q': 38, 'd': 39, '&': 40, '√¢': 41, 's': 42, '√ß': 43, '√º': 44, '¬´': 45, 't': 46, 'c': 47, 'p': 48, 'h': 49, 'g': 50, '_': 51, '√±': 52, 'u': 53, '3': 54, '\n': 55, '√≥': 56, 'w': 57, "'": 58, '≈°': 59, 'i': 60}
Vocab_len: 63
----------------- Preparing vocab complete. -----------------


----------------- Saving vocab to jason... -----------------
----------------- Saving vocab to jason complete. -----------------


sample path: common_voice_pt_24977028.mp3

Sanity Check sampling rate is 48khz: {'path': '/home/or/Desktop/portu_dataset/clips/common_voice_pt_24977028.mp3', 'array': array([-6.2598435e-17, -4.1998743e-14, -2.0979723e-13, ...,
        1.3286041e-07, -8.8752904e-07, -7.6265934e-07], dtype=float32), 'sampling_rate': 48000}

----------------- Resampling to 16khz... -----------------
Making sure the sampling rate changed to 16khz {'path': '/home/or/Desktop/portu_dataset/clips/common_voice_pt_24977028.mp3', 'array': array([ 2.8104035e-14, -4.9100405e-13, -5.3096373e-13, ...,
       -6.6721195e-06, -1.2034812e-06,  7.3115433e-07], dtype=float32), 'sampling_rate': 16000}
----------------- Resampling complete. -----------------


----------------- Preparing datasets... -----------------
#0:   0%|                                                                                                                                            | 0/4490 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/4490 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4490/4490 [07:32<00:00,  9.93ex/s]
#2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4490/4490 [07:34<00:00,  9.89ex/s]
#1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4490/4490 [07:34<00:00,  9.88ex/s]
#0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4490/4490 [07:34<00:00,  9.88ex/s]
#0:   0%|                                                                                                                                            | 0/2172 [00:00<?, ?ex/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/2172 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2172/2172 [03:32<00:00, 10.23ex/s]
#2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2172/2172 [03:35<00:00, 10.09ex/s]
#0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2172/2172 [03:36<00:00, 10.06ex/s]
#1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2172/2172 [03:36<00:00, 10.05ex/s]
#1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2171/2172 [03:36<00:00, 13.73ex/s]

----------------- Preparing datasets complete. -----------------


----------------- saving datasets... -----------------
----------------- saving datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_portu.py:246: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.bias', 'quantizer.codevectors', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 17960
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 11220
  Number of trainable parameters = 311293119
  0%|                                                                                                                                               | 0/11220 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 20.4949, 'learning_rate': 4.2e-06, 'epoch': 0.01}                                                                                                                    
{'loss': 19.8572, 'learning_rate': 1.02e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 22.1167, 'learning_rate': 1.6199999999999997e-05, 'epoch': 0.03}                                                                                                     
{'loss': 25.8237, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.04}                                                                                                     
{'loss': 35.4451, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.04}                                                                                                     
{'loss': 17.4166, 'learning_rate': 3.36e-05, 'epoch': 0.05}                                                                                                                   
{'loss': 17.4244, 'learning_rate': 3.96e-05, 'epoch': 0.06}                                                                                                                   
{'loss': 17.681, 'learning_rate': 4.56e-05, 'epoch': 0.07}                                                                                                                    
{'loss': 15.133, 'learning_rate': 5.1e-05, 'epoch': 0.08}                                                                                                                     
{'loss': 12.9214, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.09}                                                                                                     
  1%|‚ñà‚ñè                                                                                                                                 | 100/11220 [01:23<1:28:03,  2.10it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 5.961987018585205, 'eval_wer': 1.0, 'eval_cer': 0.9971668215430531, 'eval_runtime': 291.237, 'eval_samples_per_second': 29.831, 'eval_steps_per_second': 3.729, 'epoch': 0.09}                                                                                                                                                                
{'loss': 5.6651, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.1}                                                                                                        
{'loss': 4.2185, 'learning_rate': 6.9e-05, 'epoch': 0.11}                                                                                                                     
{'loss': 3.9059, 'learning_rate': 7.5e-05, 'epoch': 0.12}                                                                                                                     
{'loss': 3.7218, 'learning_rate': 8.1e-05, 'epoch': 0.12}                                                                                                                     
{'loss': 3.7188, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.13}                                                                                                       
{'loss': 3.3126, 'learning_rate': 9.3e-05, 'epoch': 0.14}                                                                                                                     
{'loss': 3.1719, 'learning_rate': 9.9e-05, 'epoch': 0.15}                                                                                                                     
{'loss': 3.1502, 'learning_rate': 0.00010499999999999999, 'epoch': 0.16}                                                                                                      
{'loss': 3.1601, 'learning_rate': 0.00011099999999999999, 'epoch': 0.17}                                                                                                      
{'loss': 3.2997, 'learning_rate': 0.000117, 'epoch': 0.18}                                                                                                                    
  2%|‚ñà‚ñà‚ñé                                                                                                                                | 200/11220 [07:36<1:33:39,  1.96it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 3.175806760787964, 'eval_wer': 1.0, 'eval_cer': 0.9971668215430531, 'eval_runtime': 293.2128, 'eval_samples_per_second': 29.63, 'eval_steps_per_second': 3.704, 'epoch': 0.18}                                                                                                                                                                
{'loss': 3.1012, 'learning_rate': 0.00012299999999999998, 'epoch': 0.19}                                                                                                      
{'loss': 3.0246, 'learning_rate': 0.000129, 'epoch': 0.2}                                                                                                                     
{'loss': 3.1025, 'learning_rate': 0.000135, 'epoch': 0.2}                                                                                                                     
{'loss': 3.1614, 'learning_rate': 0.00014099999999999998, 'epoch': 0.21}                                                                                                      
{'loss': 3.2478, 'learning_rate': 0.000147, 'epoch': 0.22}                                                                                                                    
{'loss': 3.1174, 'learning_rate': 0.00015299999999999998, 'epoch': 0.23}                                                                                                      
{'loss': 3.0222, 'learning_rate': 0.000159, 'epoch': 0.24}                                                                                                                    
{'loss': 3.0601, 'learning_rate': 0.000165, 'epoch': 0.25}                                                                                                                    
{'loss': 3.1005, 'learning_rate': 0.00017099999999999998, 'epoch': 0.26}                                                                                                      
{'loss': 3.2405, 'learning_rate': 0.00017699999999999997, 'epoch': 0.27}                                                                                                      
  3%|‚ñà‚ñà‚ñà‚ñå                                                                                                                               | 300/11220 [13:50<1:29:32,  2.03it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 3.165616035461426, 'eval_wer': 1.0, 'eval_cer': 0.9971668215430531, 'eval_runtime': 293.6808, 'eval_samples_per_second': 29.583, 'eval_steps_per_second': 3.698, 'epoch': 0.27}                                                                                                                                                               
{'loss': 3.0981, 'learning_rate': 0.00018299999999999998, 'epoch': 0.28}                                                                                                      
{'loss': 2.9982, 'learning_rate': 0.00018899999999999999, 'epoch': 0.29}                                                                                                      
{'loss': 2.9993, 'learning_rate': 0.000195, 'epoch': 0.29}                                                                                                                    
{'loss': 3.1138, 'learning_rate': 0.000201, 'epoch': 0.3}                                                                                                                     
{'loss': 3.1878, 'learning_rate': 0.00020699999999999996, 'epoch': 0.31}                                                                                                      
{'loss': 3.2164, 'learning_rate': 0.00021299999999999997, 'epoch': 0.32}                                                                                                      
{'loss': 2.9935, 'learning_rate': 0.00021899999999999998, 'epoch': 0.33}                                                                                                      
{'loss': 3.0234, 'learning_rate': 0.000225, 'epoch': 0.34}                                                                                                                    
{'loss': 3.1118, 'learning_rate': 0.00023099999999999998, 'epoch': 0.35}                                                                                                      
{'loss': 3.2124, 'learning_rate': 0.000237, 'epoch': 0.36}                                                                                                                    
  4%|‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                              | 400/11220 [20:06<1:26:47,  2.08it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 3.025644302368164, 'eval_wer': 1.0, 'eval_cer': 0.9971668215430531, 'eval_runtime': 292.7486, 'eval_samples_per_second': 29.677, 'eval_steps_per_second': 3.71, 'epoch': 0.36}                                                                                                                                                                
{'loss': 2.9596, 'learning_rate': 0.000243, 'epoch': 0.37}                                                                                                                    
{'loss': 2.9721, 'learning_rate': 0.000249, 'epoch': 0.37}                                                                                                                    
{'loss': 3.0095, 'learning_rate': 0.00025499999999999996, 'epoch': 0.38}                                                                                                      
{'loss': 3.0792, 'learning_rate': 0.000261, 'epoch': 0.39}                                                                                                                    
{'loss': 3.1809, 'learning_rate': 0.000267, 'epoch': 0.4}                                                                                                                     
{'loss': 2.9469, 'learning_rate': 0.00027299999999999997, 'epoch': 0.41}                                                                                                      
{'loss': 2.9587, 'learning_rate': 0.000279, 'epoch': 0.42}                                                                                                                    
{'loss': 2.9814, 'learning_rate': 0.000285, 'epoch': 0.43}                                                                                                                    
{'loss': 3.0238, 'learning_rate': 0.00029099999999999997, 'epoch': 0.44}                                                                                                      
{'loss': 3.0662, 'learning_rate': 0.00029699999999999996, 'epoch': 0.45}                                                                                                      
  4%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                             | 500/11220 [26:20<1:25:41,  2.08it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 2.9886505603790283, 'eval_wer': 1.0, 'eval_cer': 0.9971668215430531, 'eval_runtime': 294.1601, 'eval_samples_per_second': 29.535, 'eval_steps_per_second': 3.692, 'epoch': 0.45}                                                                                                                                                              
{'loss': 2.9199, 'learning_rate': 0.00029986007462686566, 'epoch': 0.45}                                                                                                      
{'loss': 2.8941, 'learning_rate': 0.000299580223880597, 'epoch': 0.46}                                                                                                        
{'loss': 2.8851, 'learning_rate': 0.0002993003731343283, 'epoch': 0.47}                                                                                                       
{'loss': 2.8416, 'learning_rate': 0.00029902052238805967, 'epoch': 0.48}                                                                                                      
{'loss': 2.7529, 'learning_rate': 0.00029874067164179104, 'epoch': 0.49}                                                                                                      
{'loss': 2.5892, 'learning_rate': 0.00029846082089552236, 'epoch': 0.5}                                                                                                       
  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                            | 561/11220 [32:09<3:08:31,  1.06s/it]Saving model checkpoint to ./portu_clean/checkpoint-561
Configuration saved in ./portu_clean/checkpoint-561/config.json
Model weights saved in ./portu_clean/checkpoint-561/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-561/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.3258, 'learning_rate': 0.00029818097014925373, 'epoch': 0.51}                                                                                                      
{'loss': 2.0762, 'learning_rate': 0.00029790111940298505, 'epoch': 0.52}                                                                                                      
{'loss': 1.7978, 'learning_rate': 0.00029762126865671637, 'epoch': 0.53}                                                                                                      
{'loss': 1.7666, 'learning_rate': 0.00029734141791044774, 'epoch': 0.53}                                                                                                      
  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                            | 600/11220 [32:39<1:26:12,  2.05it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 1.5574402809143066, 'eval_wer': 1.0360718013655263, 'eval_cer': 0.4331826946069722, 'eval_runtime': 295.8095, 'eval_samples_per_second': 29.37, 'eval_steps_per_second': 3.671, 'epoch': 0.53}                                                                                                                                                
{'loss': 1.8438, 'learning_rate': 0.0002970615671641791, 'epoch': 0.54}                                                                                                       
{'loss': 1.5703, 'learning_rate': 0.00029678171641791043, 'epoch': 0.55}                                                                                                      
{'loss': 1.4638, 'learning_rate': 0.00029650186567164175, 'epoch': 0.56}                                                                                                      
{'loss': 1.4126, 'learning_rate': 0.0002962220149253731, 'epoch': 0.57}                                                                                                       
{'loss': 1.439, 'learning_rate': 0.00029594216417910444, 'epoch': 0.58}                                                                                                       
{'loss': 1.3095, 'learning_rate': 0.00029566231343283576, 'epoch': 0.59}                                                                                                      
{'loss': 1.1765, 'learning_rate': 0.00029538246268656713, 'epoch': 0.6}                                                                                                       
{'loss': 1.2225, 'learning_rate': 0.0002951026119402985, 'epoch': 0.61}                                                                                                       
{'loss': 1.216, 'learning_rate': 0.0002948227611940298, 'epoch': 0.61}                                                                                                        
{'loss': 1.2056, 'learning_rate': 0.0002945429104477612, 'epoch': 0.62}                                                                                                       
  6%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                                          | 700/11220 [38:57<1:27:55,  1.99it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.9797704815864563, 'eval_wer': 0.8493129588117524, 'eval_cer': 0.3023903429780459, 'eval_runtime': 293.7499, 'eval_samples_per_second': 29.576, 'eval_steps_per_second': 3.697, 'epoch': 0.62}                                                                                                                                               
{'loss': 1.2657, 'learning_rate': 0.0002942630597014925, 'epoch': 0.63}                                                                                                       
{'loss': 1.1144, 'learning_rate': 0.0002940111940298507, 'epoch': 0.64}                                                                                                       
{'loss': 1.0509, 'learning_rate': 0.00029373134328358205, 'epoch': 0.65}                                                                                                      
{'loss': 1.0825, 'learning_rate': 0.0002934514925373134, 'epoch': 0.66}                                                                                                       
{'loss': 1.0787, 'learning_rate': 0.00029317164179104474, 'epoch': 0.67}                                                                                                      
{'loss': 1.0084, 'learning_rate': 0.00029289179104477605, 'epoch': 0.68}                                                                                                      
{'loss': 0.9166, 'learning_rate': 0.0002926119402985074, 'epoch': 0.69}                                                                                                       
{'loss': 0.8942, 'learning_rate': 0.0002923320895522388, 'epoch': 0.69}                                                                                                       
{'loss': 0.9502, 'learning_rate': 0.0002920522388059701, 'epoch': 0.7}                                                                                                        
{'loss': 0.9548, 'learning_rate': 0.0002917723880597015, 'epoch': 0.71}                                                                                                       
  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                         | 800/11220 [45:13<1:25:46,  2.02it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.8020749688148499, 'eval_wer': 0.7307277674155958, 'eval_cer': 0.2494589416141247, 'eval_runtime': 295.4321, 'eval_samples_per_second': 29.408, 'eval_steps_per_second': 3.676, 'epoch': 0.71}                                                                                                                                               
{'loss': 0.9437, 'learning_rate': 0.0002914925373134328, 'epoch': 0.72}                                                                                                       
{'loss': 0.87, 'learning_rate': 0.0002912126865671641, 'epoch': 0.73}                                                                                                         
{'loss': 0.8961, 'learning_rate': 0.0002909328358208955, 'epoch': 0.74}                                                                                                       
{'loss': 1.0559, 'learning_rate': 0.00029065298507462687, 'epoch': 0.75}                                                                                                      
{'loss': 0.9576, 'learning_rate': 0.0002903731343283582, 'epoch': 0.76}                                                                                                       
{'loss': 0.8559, 'learning_rate': 0.0002900932835820895, 'epoch': 0.77}                                                                                                       
{'loss': 0.8259, 'learning_rate': 0.0002898134328358209, 'epoch': 0.78}                                                                                                       
{'loss': 0.917, 'learning_rate': 0.0002895615671641791, 'epoch': 0.78}                                                                                                        
{'loss': 0.8952, 'learning_rate': 0.0002892817164179104, 'epoch': 0.79}                                                                                                       
{'loss': 0.9482, 'learning_rate': 0.0002890018656716418, 'epoch': 0.8}                                                                                                        
  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                        | 900/11220 [51:30<1:22:11,  2.09it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.6949304938316345, 'eval_wer': 0.7150704153048478, 'eval_cer': 0.2469133186629577, 'eval_runtime': 293.8165, 'eval_samples_per_second': 29.569, 'eval_steps_per_second': 3.696, 'epoch': 0.8}                                                                                                                                                
{'loss': 1.0416, 'learning_rate': 0.0002887220149253731, 'epoch': 0.81}                                                                                                       
{'loss': 0.8626, 'learning_rate': 0.0002884421641791044, 'epoch': 0.82}                                                                                                       
{'loss': 0.8817, 'learning_rate': 0.0002881623134328358, 'epoch': 0.83}                                                                                                       
{'loss': 0.8065, 'learning_rate': 0.00028788246268656717, 'epoch': 0.84}                                                                                                      
{'loss': 0.8142, 'learning_rate': 0.0002876026119402985, 'epoch': 0.85}                                                                                                       
{'loss': 0.8101, 'learning_rate': 0.0002873227611940298, 'epoch': 0.86}                                                                                                       
{'loss': 0.7636, 'learning_rate': 0.0002870429104477612, 'epoch': 0.86}                                                                                                       
{'loss': 0.8132, 'learning_rate': 0.0002867630597014925, 'epoch': 0.87}                                                                                                       
{'loss': 0.7709, 'learning_rate': 0.00028648320895522387, 'epoch': 0.88}                                                                                                      
{'loss': 0.7762, 'learning_rate': 0.00028620335820895524, 'epoch': 0.89}                                                                                                      
  9%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                      | 1000/11220 [57:46<1:24:56,  2.01it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.6040982007980347, 'eval_wer': 0.6384435051934496, 'eval_cer': 0.2094887263107234, 'eval_runtime': 293.542, 'eval_samples_per_second': 29.597, 'eval_steps_per_second': 3.7, 'epoch': 0.89}                                                                                                                                                  
{'loss': 0.7794, 'learning_rate': 0.00028592350746268656, 'epoch': 0.9}                                                                                                       
{'loss': 0.8022, 'learning_rate': 0.0002856436567164179, 'epoch': 0.91}                                                                                                       
{'loss': 0.7668, 'learning_rate': 0.00028536380597014925, 'epoch': 0.92}                                                                                                      
{'loss': 0.8442, 'learning_rate': 0.00028508395522388057, 'epoch': 0.93}                                                                                                      
{'loss': 0.7223, 'learning_rate': 0.0002848041044776119, 'epoch': 0.94}                                                                                                       
{'loss': 0.7186, 'learning_rate': 0.00028452425373134326, 'epoch': 0.94}                                                                                                      
{'loss': 0.7521, 'learning_rate': 0.00028424440298507463, 'epoch': 0.95}                                                                                                      
{'loss': 0.7846, 'learning_rate': 0.00028396455223880595, 'epoch': 0.96}                                                                                                      
{'loss': 0.8989, 'learning_rate': 0.00028368470149253727, 'epoch': 0.97}                                                                                                      
{'loss': 0.7939, 'learning_rate': 0.00028340485074626864, 'epoch': 0.98}                                                                                                      
 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                   | 1100/11220 [1:04:01<1:21:43,  2.06it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.569991946220398, 'eval_wer': 0.6283646195177878, 'eval_cer': 0.20487573061800218, 'eval_runtime': 294.7348, 'eval_samples_per_second': 29.477, 'eval_steps_per_second': 3.685, 'epoch': 0.98}                                                                                                                                               
{'loss': 0.7338, 'learning_rate': 0.00028312499999999996, 'epoch': 0.99}                                                                                                      
{'loss': 0.8073, 'learning_rate': 0.00028284514925373133, 'epoch': 1.0}                                                                                                       
 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                   | 1122/11220 [1:09:15<1:44:21,  1.61it/s]Saving model checkpoint to ./portu_clean/checkpoint-1122
Configuration saved in ./portu_clean/checkpoint-1122/config.json
Model weights saved in ./portu_clean/checkpoint-1122/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-1122/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.8182, 'learning_rate': 0.00028256529850746265, 'epoch': 1.01}                                                                                                      
{'loss': 0.6077, 'learning_rate': 0.000282285447761194, 'epoch': 1.02}                                                                                                        
{'loss': 0.6433, 'learning_rate': 0.00028200559701492534, 'epoch': 1.02}                                                                                                      
{'loss': 0.6408, 'learning_rate': 0.0002817257462686567, 'epoch': 1.03}                                                                                                       
{'loss': 0.7776, 'learning_rate': 0.00028144589552238803, 'epoch': 1.04}                                                                                                      
{'loss': 0.6156, 'learning_rate': 0.00028116604477611935, 'epoch': 1.05}                                                                                                      
{'loss': 0.6821, 'learning_rate': 0.0002808861940298507, 'epoch': 1.06}                                                                                                       
{'loss': 0.5998, 'learning_rate': 0.0002806063432835821, 'epoch': 1.07}                                                                                                       
 11%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                  | 1200/11220 [1:10:28<2:05:22,  1.33it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.48622533679008484, 'eval_wer': 0.5485377915433187, 'eval_cer': 0.17699495415759858, 'eval_runtime': 295.7934, 'eval_samples_per_second': 29.372, 'eval_steps_per_second': 3.671, 'epoch': 1.07}                                                                                                                                             
{'loss': 0.6347, 'learning_rate': 0.0002803264925373134, 'epoch': 1.08}                                                                                                       
{'loss': 0.6377, 'learning_rate': 0.0002800466417910448, 'epoch': 1.09}                                                                                                       
{'loss': 0.5725, 'learning_rate': 0.0002797667910447761, 'epoch': 1.1}                                                                                                        
{'loss': 0.5702, 'learning_rate': 0.0002794869402985074, 'epoch': 1.11}                                                                                                       
{'loss': 0.6096, 'learning_rate': 0.0002792070895522388, 'epoch': 1.11}                                                                                                       
{'loss': 0.5802, 'learning_rate': 0.0002789272388059701, 'epoch': 1.12}                                                                                                       
{'loss': 0.6695, 'learning_rate': 0.0002786473880597015, 'epoch': 1.13}                                                                                                       
{'loss': 0.7164, 'learning_rate': 0.0002783675373134328, 'epoch': 1.14}                                                                                                       
{'loss': 0.783, 'learning_rate': 0.0002780876865671642, 'epoch': 1.15}                                                                                                        
{'loss': 0.6008, 'learning_rate': 0.0002778078358208955, 'epoch': 1.16}                                                                                                       
 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                 | 1300/11220 [1:16:45<2:01:42,  1.36it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.478555291891098, 'eval_wer': 0.5433357860332997, 'eval_cer': 0.17584775903453995, 'eval_runtime': 294.3175, 'eval_samples_per_second': 29.519, 'eval_steps_per_second': 3.69, 'epoch': 1.16}                                                                                                                                                
{'loss': 0.6822, 'learning_rate': 0.0002775279850746268, 'epoch': 1.17}                                                                                                       
{'loss': 0.5876, 'learning_rate': 0.0002772481343283582, 'epoch': 1.18}                                                                                                       
{'loss': 0.6543, 'learning_rate': 0.0002769682835820895, 'epoch': 1.19}                                                                                                       
{'loss': 0.5536, 'learning_rate': 0.0002766884328358209, 'epoch': 1.19}                                                                                                       
{'loss': 0.5914, 'learning_rate': 0.00027640858208955225, 'epoch': 1.2}                                                                                                       
{'loss': 0.6894, 'learning_rate': 0.00027612873134328356, 'epoch': 1.21}                                                                                                      
{'loss': 0.6093, 'learning_rate': 0.0002758488805970149, 'epoch': 1.22}                                                                                                       
{'loss': 0.6884, 'learning_rate': 0.00027556902985074625, 'epoch': 1.23}                                                                                                      
{'loss': 0.6731, 'learning_rate': 0.0002752891791044776, 'epoch': 1.24}                                                                                                       
{'loss': 0.6072, 'learning_rate': 0.00027500932835820895, 'epoch': 1.25}                                                                                                      
 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                | 1400/11220 [1:23:02<2:03:22,  1.33it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.4589637815952301, 'eval_wer': 0.5471175071442016, 'eval_cer': 0.17115001014011522, 'eval_runtime': 295.0555, 'eval_samples_per_second': 29.445, 'eval_steps_per_second': 3.681, 'epoch': 1.25}                                                                                                                                              
{'loss': 0.6018, 'learning_rate': 0.00027472947761194026, 'epoch': 1.26}                                                                                                      
{'loss': 0.6061, 'learning_rate': 0.00027444962686567164, 'epoch': 1.27}                                                                                                      
{'loss': 0.6131, 'learning_rate': 0.00027416977611940295, 'epoch': 1.27}                                                                                                      
{'loss': 0.5075, 'learning_rate': 0.0002738899253731343, 'epoch': 1.28}                                                                                                       
{'loss': 0.591, 'learning_rate': 0.00027361007462686565, 'epoch': 1.29}                                                                                                       
{'loss': 0.5997, 'learning_rate': 0.00027333022388059696, 'epoch': 1.3}                                                                                                       
{'loss': 0.6655, 'learning_rate': 0.00027305037313432834, 'epoch': 1.31}                                                                                                      
{'loss': 0.5484, 'learning_rate': 0.0002727705223880597, 'epoch': 1.32}                                                                                                       
{'loss': 0.5241, 'learning_rate': 0.000272490671641791, 'epoch': 1.33}                                                                                                        
{'loss': 0.5865, 'learning_rate': 0.00027221082089552235, 'epoch': 1.34}                                                                                                      
 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                               | 1500/11220 [1:29:18<1:59:51,  1.35it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.4342607259750366, 'eval_wer': 0.5149985454918804, 'eval_cer': 0.16755405286783645, 'eval_runtime': 297.094, 'eval_samples_per_second': 29.243, 'eval_steps_per_second': 3.655, 'epoch': 1.34}                                                                                                                                               
{'loss': 0.5592, 'learning_rate': 0.0002719309701492537, 'epoch': 1.35}                                                                                                       
{'loss': 0.6109, 'learning_rate': 0.00027165111940298504, 'epoch': 1.35}                                                                                                      
{'loss': 0.5538, 'learning_rate': 0.00027137126865671635, 'epoch': 1.36}                                                                                                      
{'loss': 0.4901, 'learning_rate': 0.0002710914179104477, 'epoch': 1.37}                                                                                                       
{'loss': 0.5801, 'learning_rate': 0.0002708115671641791, 'epoch': 1.38}                                                                                                       
{'loss': 0.5691, 'learning_rate': 0.0002705317164179104, 'epoch': 1.39}                                                                                                       
{'loss': 0.6507, 'learning_rate': 0.0002702518656716418, 'epoch': 1.4}                                                                                                        
{'loss': 0.6651, 'learning_rate': 0.0002699720149253731, 'epoch': 1.41}                                                                                                       
{'loss': 0.5324, 'learning_rate': 0.0002696921641791044, 'epoch': 1.42}                                                                                                       
{'loss': 0.5247, 'learning_rate': 0.0002694123134328358, 'epoch': 1.43}                                                                                                       
 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                             | 1600/11220 [1:35:38<2:00:58,  1.33it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.43157875537872314, 'eval_wer': 0.51970430705522, 'eval_cer': 0.16521425912080662, 'eval_runtime': 296.2394, 'eval_samples_per_second': 29.328, 'eval_steps_per_second': 3.666, 'epoch': 1.43}                                                                                                                                               
{'loss': 0.5554, 'learning_rate': 0.00026913246268656717, 'epoch': 1.43}                                                                                                      
{'loss': 0.5545, 'learning_rate': 0.0002688526119402985, 'epoch': 1.44}                                                                                                       
{'loss': 0.5444, 'learning_rate': 0.0002685727611940298, 'epoch': 1.45}                                                                                                       
{'loss': 0.482, 'learning_rate': 0.0002682929104477612, 'epoch': 1.46}                                                                                                        
{'loss': 0.5003, 'learning_rate': 0.0002680130597014925, 'epoch': 1.47}                                                                                                       
{'loss': 0.6197, 'learning_rate': 0.0002677332089552238, 'epoch': 1.48}                                                                                                       
{'loss': 0.7591, 'learning_rate': 0.0002674533582089552, 'epoch': 1.49}                                                                                                       
{'loss': 0.7685, 'learning_rate': 0.00026717350746268656, 'epoch': 1.5}                                                                                                       
 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                            | 1683/11220 [1:41:43<2:50:53,  1.08s/it]Saving model checkpoint to ./portu_clean/checkpoint-1683
Configuration saved in ./portu_clean/checkpoint-1683/config.json
Model weights saved in ./portu_clean/checkpoint-1683/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-1683/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.48, 'learning_rate': 0.0002668936567164179, 'epoch': 1.51}                                                                                                         
{'loss': 0.5615, 'learning_rate': 0.00026661380597014925, 'epoch': 1.51}                                                                                                      
 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                            | 1700/11220 [1:42:01<2:05:27,  1.26it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.38416746258735657, 'eval_wer': 0.4838720717329181, 'eval_cer': 0.15439611830336197, 'eval_runtime': 296.7943, 'eval_samples_per_second': 29.273, 'eval_steps_per_second': 3.659, 'epoch': 1.51}                                                                                                                                             
{'loss': 0.5385, 'learning_rate': 0.00026633395522388057, 'epoch': 1.52}                                                                                                      
{'loss': 0.5116, 'learning_rate': 0.0002660541044776119, 'epoch': 1.53}                                                                                                       
{'loss': 0.49, 'learning_rate': 0.00026577425373134326, 'epoch': 1.54}                                                                                                        
{'loss': 0.541, 'learning_rate': 0.00026549440298507463, 'epoch': 1.55}                                                                                                       
{'loss': 0.6308, 'learning_rate': 0.0002652425373134328, 'epoch': 1.56}                                                                                                       
{'loss': 0.5328, 'learning_rate': 0.00026496268656716417, 'epoch': 1.57}                                                                                                      
{'loss': 0.6426, 'learning_rate': 0.0002646828358208955, 'epoch': 1.58}                                                                                                       
{'loss': 0.532, 'learning_rate': 0.00026440298507462686, 'epoch': 1.59}                                                                                                       
{'loss': 0.534, 'learning_rate': 0.0002641231343283582, 'epoch': 1.6}                                                                                                         
{'loss': 0.6545, 'learning_rate': 0.0002638712686567164, 'epoch': 1.6}                                                                                                        
 16%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                           | 1800/11220 [1:48:20<1:59:35,  1.31it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3948425054550171, 'eval_wer': 0.4936258320642037, 'eval_cer': 0.1619996912561938, 'eval_runtime': 296.8121, 'eval_samples_per_second': 29.271, 'eval_steps_per_second': 3.659, 'epoch': 1.6}                                                                                                                                                
{'loss': 0.5505, 'learning_rate': 0.00026359141791044776, 'epoch': 1.61}                                                                                                      
{'loss': 0.5879, 'learning_rate': 0.0002633115671641791, 'epoch': 1.62}                                                                                                       
{'loss': 0.6485, 'learning_rate': 0.0002630317164179104, 'epoch': 1.63}                                                                                                       
{'loss': 0.7092, 'learning_rate': 0.00026275186567164177, 'epoch': 1.64}                                                                                                      
{'loss': 0.5061, 'learning_rate': 0.0002624720149253731, 'epoch': 1.65}                                                                                                       
{'loss': 0.5049, 'learning_rate': 0.00026219216417910446, 'epoch': 1.66}                                                                                                      
{'loss': 0.5512, 'learning_rate': 0.00026191231343283583, 'epoch': 1.67}                                                                                                      
{'loss': 0.5341, 'learning_rate': 0.00026163246268656715, 'epoch': 1.68}                                                                                                      
{'loss': 0.5124, 'learning_rate': 0.00026135261194029847, 'epoch': 1.68}                                                                                                      
{'loss': 0.4724, 'learning_rate': 0.00026107276119402984, 'epoch': 1.69}                                                                                                      
 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                          | 1900/11220 [1:54:40<1:58:49,  1.31it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3808515667915344, 'eval_wer': 0.48010746248224645, 'eval_cer': 0.15670866994984425, 'eval_runtime': 297.1799, 'eval_samples_per_second': 29.235, 'eval_steps_per_second': 3.654, 'epoch': 1.69}                                                                                                                                             
{'loss': 0.4766, 'learning_rate': 0.00026079291044776116, 'epoch': 1.7}                                                                                                       
{'loss': 0.6268, 'learning_rate': 0.0002605130597014925, 'epoch': 1.71}                                                                                                       
{'loss': 0.7952, 'learning_rate': 0.00026023320895522385, 'epoch': 1.72}                                                                                                      
{'loss': 0.5203, 'learning_rate': 0.0002599533582089552, 'epoch': 1.73}                                                                                                       
{'loss': 0.442, 'learning_rate': 0.00025967350746268654, 'epoch': 1.74}                                                                                                       
{'loss': 0.6266, 'learning_rate': 0.00025939365671641786, 'epoch': 1.75}                                                                                                      
{'loss': 0.5775, 'learning_rate': 0.00025911380597014923, 'epoch': 1.76}                                                                                                      
{'loss': 0.5046, 'learning_rate': 0.00025883395522388055, 'epoch': 1.76}                                                                                                      
{'loss': 0.5646, 'learning_rate': 0.0002585541044776119, 'epoch': 1.77}                                                                                                       
{'loss': 0.4939, 'learning_rate': 0.0002582742537313433, 'epoch': 1.78}                                                                                                       
 18%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                         | 2000/11220 [2:00:59<1:56:35,  1.32it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.38156187534332275, 'eval_wer': 0.47548726022005855, 'eval_cer': 0.15458681300719493, 'eval_runtime': 297.0414, 'eval_samples_per_second': 29.248, 'eval_steps_per_second': 3.656, 'epoch': 1.78}                                                                                                                                            
{'loss': 0.4916, 'learning_rate': 0.0002579944029850746, 'epoch': 1.79}                                                                                                       
{'loss': 0.5471, 'learning_rate': 0.00025771455223880593, 'epoch': 1.8}                                                                                                       
{'loss': 0.5226, 'learning_rate': 0.0002574347014925373, 'epoch': 1.81}                                                                                                       
{'loss': 0.4971, 'learning_rate': 0.0002571548507462686, 'epoch': 1.82}                                                                                                       
{'loss': 0.5319, 'learning_rate': 0.00025687499999999994, 'epoch': 1.83}                                                                                                      
{'loss': 0.5097, 'learning_rate': 0.0002565951492537313, 'epoch': 1.84}                                                                                                       
{'loss': 0.52, 'learning_rate': 0.0002563152985074627, 'epoch': 1.84}                                                                                                         
{'loss': 0.5322, 'learning_rate': 0.000256035447761194, 'epoch': 1.85}                                                                                                        
{'loss': 0.5162, 'learning_rate': 0.0002557555970149254, 'epoch': 1.86}                                                                                                       
{'loss': 0.5038, 'learning_rate': 0.0002554757462686567, 'epoch': 1.87}                                                                                                       
 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                        | 2100/11220 [2:07:19<1:54:09,  1.33it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3708235025405884, 'eval_wer': 0.4627047006280053, 'eval_cer': 0.14885083739190183, 'eval_runtime': 298.4442, 'eval_samples_per_second': 29.111, 'eval_steps_per_second': 3.639, 'epoch': 1.87}                                                                                                                                              
{'loss': 0.5763, 'learning_rate': 0.000255195895522388, 'epoch': 1.88}                                                                                                        
{'loss': 0.6186, 'learning_rate': 0.0002549160447761194, 'epoch': 1.89}                                                                                                       
{'loss': 0.4785, 'learning_rate': 0.00025463619402985076, 'epoch': 1.9}                                                                                                       
{'loss': 0.4725, 'learning_rate': 0.0002543563432835821, 'epoch': 1.91}                                                                                                       
{'loss': 0.4983, 'learning_rate': 0.0002540764925373134, 'epoch': 1.92}                                                                                                       
{'loss': 0.4688, 'learning_rate': 0.00025379664179104477, 'epoch': 1.92}                                                                                                      
{'loss': 0.5342, 'learning_rate': 0.0002535167910447761, 'epoch': 1.93}                                                                                                       
{'loss': 0.4556, 'learning_rate': 0.0002532369402985074, 'epoch': 1.94}                                                                                                       
{'loss': 0.4362, 'learning_rate': 0.0002529570895522388, 'epoch': 1.95}                                                                                                       
{'loss': 0.4897, 'learning_rate': 0.00025267723880597015, 'epoch': 1.96}                                                                                                      
 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                       | 2200/11220 [2:13:40<1:54:19,  1.31it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.35247376561164856, 'eval_wer': 0.461797772035798, 'eval_cer': 0.1492049846990202, 'eval_runtime': 296.3896, 'eval_samples_per_second': 29.313, 'eval_steps_per_second': 3.664, 'epoch': 1.96}                                                                                                                                               
{'loss': 0.5243, 'learning_rate': 0.00025239738805970147, 'epoch': 1.97}                                                                                                      
{'loss': 0.5114, 'learning_rate': 0.00025211753731343284, 'epoch': 1.98}                                                                                                      
{'loss': 0.6122, 'learning_rate': 0.00025183768656716416, 'epoch': 1.99}                                                                                                      
{'loss': 0.5116, 'learning_rate': 0.0002515578358208955, 'epoch': 2.0}                                                                                                        
 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                      | 2244/11220 [2:19:08<1:23:21,  1.79it/s]Saving model checkpoint to ./portu_clean/checkpoint-2244
Configuration saved in ./portu_clean/checkpoint-2244/config.json
Model weights saved in ./portu_clean/checkpoint-2244/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-2244/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.4767, 'learning_rate': 0.00025127798507462685, 'epoch': 2.01}                                                                                                      
{'loss': 0.3881, 'learning_rate': 0.0002509981343283582, 'epoch': 2.01}                                                                                                       
{'loss': 0.4298, 'learning_rate': 0.00025071828358208954, 'epoch': 2.02}                                                                                                      
{'loss': 0.3989, 'learning_rate': 0.00025043843283582086, 'epoch': 2.03}                                                                                                      
{'loss': 0.423, 'learning_rate': 0.00025015858208955223, 'epoch': 2.04}                                                                                                       
{'loss': 0.4782, 'learning_rate': 0.00024987873134328355, 'epoch': 2.05}                                                                                                      
 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                                     | 2300/11220 [2:20:00<2:51:04,  1.15s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3609195351600647, 'eval_wer': 0.45840962371019356, 'eval_cer': 0.1479700094741972, 'eval_runtime': 295.5537, 'eval_samples_per_second': 29.396, 'eval_steps_per_second': 3.674, 'epoch': 2.05}                                                                                                                                              
{'loss': 0.3902, 'learning_rate': 0.00024959888059701487, 'epoch': 2.06}                                                                                                      
{'loss': 0.4176, 'learning_rate': 0.00024931902985074624, 'epoch': 2.07}                                                                                                      
{'loss': 0.4333, 'learning_rate': 0.0002490391791044776, 'epoch': 2.08}                                                                                                       
{'loss': 0.4736, 'learning_rate': 0.00024875932835820893, 'epoch': 2.09}                                                                                                      
{'loss': 0.3724, 'learning_rate': 0.0002484794776119403, 'epoch': 2.09}                                                                                                       
{'loss': 0.3996, 'learning_rate': 0.0002481996268656716, 'epoch': 2.1}                                                                                                        
{'loss': 0.3768, 'learning_rate': 0.00024794776119402984, 'epoch': 2.11}                                                                                                      
{'loss': 0.4097, 'learning_rate': 0.00024766791044776115, 'epoch': 2.12}                                                                                                      
{'loss': 0.4452, 'learning_rate': 0.0002473880597014925, 'epoch': 2.13}                                                                                                       
{'loss': 0.522, 'learning_rate': 0.00024710820895522384, 'epoch': 2.14}                                                                                                       
 21%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                    | 2400/11220 [2:26:17<2:56:14,  1.20s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3731658458709717, 'eval_wer': 0.46600728965245813, 'eval_cer': 0.15304914777628786, 'eval_runtime': 295.3351, 'eval_samples_per_second': 29.417, 'eval_steps_per_second': 3.677, 'epoch': 2.14}                                                                                                                                             
{'loss': 0.3794, 'learning_rate': 0.0002468283582089552, 'epoch': 2.15}                                                                                                       
{'loss': 0.398, 'learning_rate': 0.00024654850746268653, 'epoch': 2.16}                                                                                                       
{'loss': 0.4022, 'learning_rate': 0.0002462686567164179, 'epoch': 2.17}                                                                                                       
{'loss': 0.4777, 'learning_rate': 0.0002459888059701492, 'epoch': 2.17}                                                                                                       
{'loss': 0.466, 'learning_rate': 0.0002457089552238806, 'epoch': 2.18}                                                                                                        
{'loss': 0.3519, 'learning_rate': 0.0002454291044776119, 'epoch': 2.19}                                                                                                       
{'loss': 0.4195, 'learning_rate': 0.00024514925373134323, 'epoch': 2.2}                                                                                                       
{'loss': 0.4345, 'learning_rate': 0.0002448694029850746, 'epoch': 2.21}                                                                                                       
{'loss': 0.3817, 'learning_rate': 0.000244589552238806, 'epoch': 2.22}                                                                                                        
{'loss': 0.4786, 'learning_rate': 0.0002443097014925373, 'epoch': 2.23}                                                                                                       
 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                   | 2500/11220 [2:32:34<2:47:12,  1.15s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.35530176758766174, 'eval_wer': 0.4505895035849347, 'eval_cer': 0.14926249580017617, 'eval_runtime': 295.8477, 'eval_samples_per_second': 29.366, 'eval_steps_per_second': 3.671, 'epoch': 2.23}                                                                                                                                             
{'loss': 0.4106, 'learning_rate': 0.00024402985074626864, 'epoch': 2.24}                                                                                                      
{'loss': 0.4261, 'learning_rate': 0.00024375, 'epoch': 2.25}                                                                                                                  
{'loss': 0.4431, 'learning_rate': 0.0002434701492537313, 'epoch': 2.25}                                                                                                       
{'loss': 0.4028, 'learning_rate': 0.00024319029850746265, 'epoch': 2.26}                                                                                                      
{'loss': 0.4083, 'learning_rate': 0.000242910447761194, 'epoch': 2.27}                                                                                                        
{'loss': 0.3559, 'learning_rate': 0.00024263059701492537, 'epoch': 2.28}                                                                                                      
{'loss': 0.3691, 'learning_rate': 0.00024235074626865672, 'epoch': 2.29}                                                                                                      
{'loss': 0.4019, 'learning_rate': 0.00024207089552238803, 'epoch': 2.3}                                                                                                       
{'loss': 0.384, 'learning_rate': 0.00024179104477611938, 'epoch': 2.31}                                                                                                       
{'loss': 0.6234, 'learning_rate': 0.00024151119402985072, 'epoch': 2.32}                                                                                                      
 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                  | 2600/11220 [2:38:52<2:51:56,  1.20s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.33844324946403503, 'eval_wer': 0.4572973527952224, 'eval_cer': 0.14950767470510426, 'eval_runtime': 297.6868, 'eval_samples_per_second': 29.185, 'eval_steps_per_second': 3.648, 'epoch': 2.32}                                                                                                                                             
{'loss': 0.3878, 'learning_rate': 0.00024123134328358207, 'epoch': 2.33}                                                                                                      
{'loss': 0.3988, 'learning_rate': 0.0002409514925373134, 'epoch': 2.33}                                                                                                       
{'loss': 0.4395, 'learning_rate': 0.00024067164179104476, 'epoch': 2.34}                                                                                                      
{'loss': 0.4489, 'learning_rate': 0.0002403917910447761, 'epoch': 2.35}                                                                                                       
{'loss': 0.5028, 'learning_rate': 0.00024011194029850745, 'epoch': 2.36}                                                                                                      
{'loss': 0.4167, 'learning_rate': 0.00023983208955223877, 'epoch': 2.37}                                                                                                      
{'loss': 0.4037, 'learning_rate': 0.00023955223880597012, 'epoch': 2.38}                                                                                                      
{'loss': 0.4172, 'learning_rate': 0.00023927238805970146, 'epoch': 2.39}                                                                                                      
{'loss': 0.3926, 'learning_rate': 0.00023899253731343283, 'epoch': 2.4}                                                                                                       
{'loss': 0.4444, 'learning_rate': 0.00023871268656716418, 'epoch': 2.41}                                                                                                      
 24%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                 | 2700/11220 [2:45:14<2:49:29,  1.19s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3360731899738312, 'eval_wer': 0.43921011653176817, 'eval_cer': 0.14457685450599478, 'eval_runtime': 298.1974, 'eval_samples_per_second': 29.135, 'eval_steps_per_second': 3.642, 'epoch': 2.41}                                                                                                                                             
{'loss': 0.4619, 'learning_rate': 0.0002384328358208955, 'epoch': 2.42}                                                                                                       
{'loss': 0.4005, 'learning_rate': 0.00023815298507462684, 'epoch': 2.42}                                                                                                      
{'loss': 0.4417, 'learning_rate': 0.0002378731343283582, 'epoch': 2.43}                                                                                                       
{'loss': 0.4528, 'learning_rate': 0.00023759328358208953, 'epoch': 2.44}                                                                                                      
{'loss': 0.4531, 'learning_rate': 0.00023731343283582085, 'epoch': 2.45}                                                                                                      
{'loss': 0.4075, 'learning_rate': 0.00023703358208955222, 'epoch': 2.46}                                                                                                      
{'loss': 0.3785, 'learning_rate': 0.00023675373134328357, 'epoch': 2.47}                                                                                                      
{'loss': 0.4082, 'learning_rate': 0.00023647388059701491, 'epoch': 2.48}                                                                                                      
{'loss': 0.4054, 'learning_rate': 0.00023619402985074626, 'epoch': 2.49}                                                                                                      
{'loss': 0.5009, 'learning_rate': 0.00023591417910447758, 'epoch': 2.5}                                                                                                       
 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                | 2800/11220 [2:51:34<2:49:05,  1.20s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3452422320842743, 'eval_wer': 0.4464313215489656, 'eval_cer': 0.14497640531402575, 'eval_runtime': 297.0281, 'eval_samples_per_second': 29.25, 'eval_steps_per_second': 3.656, 'epoch': 2.5}                                                                                                                                                
 25%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                               | 2805/11220 [2:56:36<52:34:02, 22.49s/it]Saving model checkpoint to ./portu_clean/checkpoint-2805                                                                                                                      
Configuration saved in ./portu_clean/checkpoint-2805/config.json
Model weights saved in ./portu_clean/checkpoint-2805/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-2805/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.3954, 'learning_rate': 0.00023563432835820892, 'epoch': 2.5}                                                                                                       
{'loss': 0.3771, 'learning_rate': 0.00023535447761194027, 'epoch': 2.51}                                                                                                      
{'loss': 0.4408, 'learning_rate': 0.00023507462686567164, 'epoch': 2.52}                                                                                                      
{'loss': 0.4161, 'learning_rate': 0.00023479477611940296, 'epoch': 2.53}                                                                                                      
{'loss': 0.4238, 'learning_rate': 0.0002345149253731343, 'epoch': 2.54}                                                                                                       
{'loss': 0.4221, 'learning_rate': 0.00023423507462686565, 'epoch': 2.55}                                                                                                      
{'loss': 0.4105, 'learning_rate': 0.000233955223880597, 'epoch': 2.56}                                                                                                        
{'loss': 0.3742, 'learning_rate': 0.00023367537313432831, 'epoch': 2.57}                                                                                                      
{'loss': 0.4422, 'learning_rate': 0.0002333955223880597, 'epoch': 2.58}                                                                                                       
{'loss': 0.3531, 'learning_rate': 0.00023311567164179103, 'epoch': 2.58}                                                                                                      
 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                               | 2900/11220 [2:57:57<2:54:24,  1.26s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.34361618757247925, 'eval_wer': 0.43318674173069355, 'eval_cer': 0.14220376485829567, 'eval_runtime': 297.4675, 'eval_samples_per_second': 29.207, 'eval_steps_per_second': 3.651, 'epoch': 2.58}                                                                                                                                            
{'loss': 0.3902, 'learning_rate': 0.00023283582089552238, 'epoch': 2.59}                                                                                                      
{'loss': 0.3835, 'learning_rate': 0.00023255597014925372, 'epoch': 2.6}                                                                                                       
{'loss': 0.4396, 'learning_rate': 0.00023227611940298504, 'epoch': 2.61}                                                                                                      
{'loss': 0.4656, 'learning_rate': 0.00023199626865671639, 'epoch': 2.62}                                                                                                      
{'loss': 0.429, 'learning_rate': 0.00023171641791044773, 'epoch': 2.63}                                                                                                       
{'loss': 0.3916, 'learning_rate': 0.0002314365671641791, 'epoch': 2.64}                                                                                                       
{'loss': 0.3772, 'learning_rate': 0.00023115671641791045, 'epoch': 2.65}                                                                                                      
{'loss': 0.3893, 'learning_rate': 0.00023087686567164177, 'epoch': 2.66}                                                                                                      
{'loss': 0.409, 'learning_rate': 0.0002305970149253731, 'epoch': 2.66}                                                                                                        
{'loss': 0.4802, 'learning_rate': 0.00023031716417910446, 'epoch': 2.67}                                                                                                      
 27%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                             | 3000/11220 [3:04:17<2:42:05,  1.18s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3182152807712555, 'eval_wer': 0.4262906620578723, 'eval_cer': 0.13902249289435212, 'eval_runtime': 297.1602, 'eval_samples_per_second': 29.237, 'eval_steps_per_second': 3.655, 'epoch': 2.67}                                                                                                                                              
{'loss': 0.3372, 'learning_rate': 0.00023003731343283578, 'epoch': 2.68}                                                                                                      
{'loss': 0.3798, 'learning_rate': 0.00022975746268656712, 'epoch': 2.69}                                                                                                      
{'loss': 0.3362, 'learning_rate': 0.0002294776119402985, 'epoch': 2.7}                                                                                                        
{'loss': 0.4509, 'learning_rate': 0.00022919776119402984, 'epoch': 2.71}                                                                                                      
{'loss': 0.4068, 'learning_rate': 0.00022891791044776119, 'epoch': 2.72}                                                                                                      
{'loss': 0.4321, 'learning_rate': 0.0002286380597014925, 'epoch': 2.73}                                                                                                       
{'loss': 0.4358, 'learning_rate': 0.00022835820895522385, 'epoch': 2.74}                                                                                                      
{'loss': 0.4209, 'learning_rate': 0.0002280783582089552, 'epoch': 2.74}                                                                                                       
{'loss': 0.3899, 'learning_rate': 0.00022779850746268657, 'epoch': 2.75}                                                                                                      
{'loss': 0.4502, 'learning_rate': 0.0002275186567164179, 'epoch': 2.76}                                                                                                       
 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                            | 3100/11220 [3:10:38<2:41:02,  1.19s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.31378260254859924, 'eval_wer': 0.4165882373072777, 'eval_cer': 0.13651319274391518, 'eval_runtime': 297.5516, 'eval_samples_per_second': 29.198, 'eval_steps_per_second': 3.65, 'epoch': 2.76}                                                                                                                                              
{'loss': 0.3398, 'learning_rate': 0.00022723880597014923, 'epoch': 2.77}                                                                                                      
{'loss': 0.385, 'learning_rate': 0.00022695895522388058, 'epoch': 2.78}                                                                                                       
{'loss': 0.4487, 'learning_rate': 0.00022667910447761192, 'epoch': 2.79}                                                                                                      
{'loss': 0.4373, 'learning_rate': 0.00022639925373134327, 'epoch': 2.8}                                                                                                       
{'loss': 0.3888, 'learning_rate': 0.00022611940298507459, 'epoch': 2.81}                                                                                                      
{'loss': 0.3925, 'learning_rate': 0.00022583955223880596, 'epoch': 2.82}                                                                                                      
{'loss': 0.3901, 'learning_rate': 0.00022558768656716414, 'epoch': 2.82}                                                                                                      
{'loss': 0.3881, 'learning_rate': 0.0002253078358208955, 'epoch': 2.83}                                                                                                       
{'loss': 0.4169, 'learning_rate': 0.00022502798507462686, 'epoch': 2.84}                                                                                                      
{'loss': 0.4647, 'learning_rate': 0.0002247481343283582, 'epoch': 2.85}                                                                                                       
 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                           | 3200/11220 [3:16:58<2:39:55,  1.20s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3151502013206482, 'eval_wer': 0.4279334006399836, 'eval_cer': 0.1418102678503864, 'eval_runtime': 297.3675, 'eval_samples_per_second': 29.216, 'eval_steps_per_second': 3.652, 'epoch': 2.85}                                                                                                                                               
{'loss': 0.3359, 'learning_rate': 0.00022446828358208953, 'epoch': 2.86}                                                                                                      
{'loss': 0.4078, 'learning_rate': 0.00022418843283582087, 'epoch': 2.87}                                                                                                      
{'loss': 0.4185, 'learning_rate': 0.00022390858208955222, 'epoch': 2.88}                                                                                                      
{'loss': 0.443, 'learning_rate': 0.00022362873134328356, 'epoch': 2.89}                                                                                                       
{'loss': 0.3561, 'learning_rate': 0.00022334888059701488, 'epoch': 2.9}                                                                                                       
{'loss': 0.3815, 'learning_rate': 0.00022306902985074625, 'epoch': 2.91}                                                                                                      
{'loss': 0.3748, 'learning_rate': 0.0002227891791044776, 'epoch': 2.91}                                                                                                       
{'loss': 0.3528, 'learning_rate': 0.00022250932835820894, 'epoch': 2.92}                                                                                                      
{'loss': 0.4665, 'learning_rate': 0.0002222294776119403, 'epoch': 2.93}                                                                                                       
{'loss': 0.4879, 'learning_rate': 0.0002219496268656716, 'epoch': 2.94}                                                                                                       
 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                          | 3300/11220 [3:23:18<2:42:37,  1.23s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3041604161262512, 'eval_wer': 0.4228682900118072, 'eval_cer': 0.13931307530019282, 'eval_runtime': 298.8064, 'eval_samples_per_second': 29.076, 'eval_steps_per_second': 3.634, 'epoch': 2.94}                                                                                                                                              
{'loss': 0.3554, 'learning_rate': 0.00022166977611940295, 'epoch': 2.95}                                                                                                      
{'loss': 0.3857, 'learning_rate': 0.00022138992537313432, 'epoch': 2.96}                                                                                                      
{'loss': 0.3742, 'learning_rate': 0.00022111007462686567, 'epoch': 2.97}                                                                                                      
{'loss': 0.4117, 'learning_rate': 0.000220830223880597, 'epoch': 2.98}                                                                                                        
{'loss': 0.4126, 'learning_rate': 0.00022055037313432833, 'epoch': 2.99}                                                                                                      
{'loss': 0.3957, 'learning_rate': 0.00022027052238805968, 'epoch': 2.99}                                                                                                      
 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                         | 3366/11220 [3:29:11<1:15:11,  1.74it/s]Saving model checkpoint to ./portu_clean/checkpoint-3366
Configuration saved in ./portu_clean/checkpoint-3366/config.json
Model weights saved in ./portu_clean/checkpoint-3366/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-3366/preprocessor_config.json
Deleting older checkpoint [portu_clean/checkpoint-561] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.4188, 'learning_rate': 0.00021999067164179102, 'epoch': 3.0}                                                                                                       
{'loss': 0.3036, 'learning_rate': 0.00021971082089552234, 'epoch': 3.01}                                                                                                      
{'loss': 0.2905, 'learning_rate': 0.00021943097014925372, 'epoch': 3.02}                                                                                                      
{'loss': 0.324, 'learning_rate': 0.00021915111940298506, 'epoch': 3.03}                                                                                                       
 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                         | 3400/11220 [3:29:46<1:29:45,  1.45it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.318766325712204, 'eval_wer': 0.42137955817176886, 'eval_cer': 0.14115948433730563, 'eval_runtime': 297.0733, 'eval_samples_per_second': 29.245, 'eval_steps_per_second': 3.656, 'epoch': 3.03}                                                                                                                                              
{'loss': 0.3659, 'learning_rate': 0.0002188712686567164, 'epoch': 3.04}                                                                                                       
{'loss': 0.3748, 'learning_rate': 0.00021859141791044775, 'epoch': 3.05}                                                                                                      
{'loss': 0.3355, 'learning_rate': 0.00021831156716417907, 'epoch': 3.06}                                                                                                      
{'loss': 0.2929, 'learning_rate': 0.00021803171641791042, 'epoch': 3.07}                                                                                                      
{'loss': 0.2987, 'learning_rate': 0.00021775186567164176, 'epoch': 3.07}                                                                                                      
{'loss': 0.3343, 'learning_rate': 0.00021747201492537313, 'epoch': 3.08}                                                                                                      
{'loss': 0.4091, 'learning_rate': 0.00021719216417910448, 'epoch': 3.09}                                                                                                      
{'loss': 0.2976, 'learning_rate': 0.0002169123134328358, 'epoch': 3.1}                                                                                                        
{'loss': 0.3255, 'learning_rate': 0.00021663246268656714, 'epoch': 3.11}                                                                                                      
{'loss': 0.3086, 'learning_rate': 0.0002163526119402985, 'epoch': 3.12}                                                                                                       
 31%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                        | 3500/11220 [3:36:05<1:25:21,  1.51it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3274012804031372, 'eval_wer': 0.4160919933605982, 'eval_cer': 0.13677047924908664, 'eval_runtime': 298.0763, 'eval_samples_per_second': 29.147, 'eval_steps_per_second': 3.643, 'epoch': 3.12}                                                                                                                                              
{'loss': 0.3488, 'learning_rate': 0.00021607276119402983, 'epoch': 3.13}                                                                                                      
{'loss': 0.4167, 'learning_rate': 0.00021579291044776118, 'epoch': 3.14}                                                                                                      
{'loss': 0.3062, 'learning_rate': 0.00021551305970149252, 'epoch': 3.15}                                                                                                      
{'loss': 0.2907, 'learning_rate': 0.00021523320895522387, 'epoch': 3.16}                                                                                                      
{'loss': 0.3751, 'learning_rate': 0.00021495335820895521, 'epoch': 3.16}                                                                                                      
{'loss': 0.4127, 'learning_rate': 0.00021467350746268653, 'epoch': 3.17}                                                                                                      
{'loss': 0.4954, 'learning_rate': 0.00021439365671641788, 'epoch': 3.18}                                                                                                      
{'loss': 0.3302, 'learning_rate': 0.00021411380597014922, 'epoch': 3.19}                                                                                                      
{'loss': 0.3263, 'learning_rate': 0.0002138339552238806, 'epoch': 3.2}                                                                                                        
{'loss': 0.3234, 'learning_rate': 0.00021355410447761194, 'epoch': 3.21}                                                                                                      
 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                       | 3600/11220 [3:42:25<1:24:06,  1.51it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.30638620257377625, 'eval_wer': 0.40914457810708604, 'eval_cer': 0.1324753080627537, 'eval_runtime': 299.0357, 'eval_samples_per_second': 29.053, 'eval_steps_per_second': 3.632, 'epoch': 3.21}                                                                                                                                             
{'loss': 0.3914, 'learning_rate': 0.00021327425373134326, 'epoch': 3.22}                                                                                                      
{'loss': 0.4103, 'learning_rate': 0.0002129944029850746, 'epoch': 3.23}                                                                                                       
{'loss': 0.3191, 'learning_rate': 0.00021271455223880595, 'epoch': 3.24}                                                                                                      
{'loss': 0.3343, 'learning_rate': 0.0002124347014925373, 'epoch': 3.24}                                                                                                       
{'loss': 0.3338, 'learning_rate': 0.00021215485074626861, 'epoch': 3.25}                                                                                                      
{'loss': 0.3443, 'learning_rate': 0.000211875, 'epoch': 3.26}                                                                                                                 
{'loss': 0.4024, 'learning_rate': 0.00021159514925373133, 'epoch': 3.27}                                                                                                      
{'loss': 0.3177, 'learning_rate': 0.00021131529850746268, 'epoch': 3.28}                                                                                                      
{'loss': 0.2584, 'learning_rate': 0.00021103544776119402, 'epoch': 3.29}                                                                                                      
{'loss': 0.3979, 'learning_rate': 0.00021075559701492534, 'epoch': 3.3}                                                                                                       
 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                     | 3700/11220 [3:48:47<1:25:07,  1.47it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.31217727065086365, 'eval_wer': 0.41354232618627973, 'eval_cer': 0.13698236225334548, 'eval_runtime': 299.2941, 'eval_samples_per_second': 29.028, 'eval_steps_per_second': 3.629, 'epoch': 3.3}                                                                                                                                             
{'loss': 0.3555, 'learning_rate': 0.00021047574626865669, 'epoch': 3.31}                                                                                                      
{'loss': 0.3436, 'learning_rate': 0.00021019589552238803, 'epoch': 3.32}                                                                                                      
{'loss': 0.2757, 'learning_rate': 0.0002099160447761194, 'epoch': 3.32}                                                                                                       
{'loss': 0.3463, 'learning_rate': 0.0002096641791044776, 'epoch': 3.33}                                                                                                       
{'loss': 0.3056, 'learning_rate': 0.00020938432835820896, 'epoch': 3.34}                                                                                                      
{'loss': 0.3868, 'learning_rate': 0.00020910447761194028, 'epoch': 3.35}                                                                                                      
{'loss': 0.3438, 'learning_rate': 0.00020882462686567163, 'epoch': 3.36}                                                                                                      
{'loss': 0.3042, 'learning_rate': 0.00020854477611940297, 'epoch': 3.37}                                                                                                      
{'loss': 0.2994, 'learning_rate': 0.00020826492537313432, 'epoch': 3.38}                                                                                                      
{'loss': 0.3351, 'learning_rate': 0.00020798507462686564, 'epoch': 3.39}                                                                                                      
 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                    | 3800/11220 [3:55:10<1:25:00,  1.45it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.2917274832725525, 'eval_wer': 0.4088707883434008, 'eval_cer': 0.1336588259865424, 'eval_runtime': 299.164, 'eval_samples_per_second': 29.041, 'eval_steps_per_second': 3.63, 'epoch': 3.39}                                                                                                                                                 
{'loss': 0.3367, 'learning_rate': 0.00020770522388059698, 'epoch': 3.4}                                                                                                       
{'loss': 0.3669, 'learning_rate': 0.00020742537313432835, 'epoch': 3.4}                                                                                                       
{'loss': 0.2488, 'learning_rate': 0.0002071455223880597, 'epoch': 3.41}                                                                                                       
{'loss': 0.2967, 'learning_rate': 0.00020686567164179104, 'epoch': 3.42}                                                                                                      
{'loss': 0.3739, 'learning_rate': 0.00020658582089552236, 'epoch': 3.43}                                                                                                      
{'loss': 0.4133, 'learning_rate': 0.0002063059701492537, 'epoch': 3.44}                                                                                                       
{'loss': 0.3258, 'learning_rate': 0.00020602611940298505, 'epoch': 3.45}                                                                                                      
{'loss': 0.3092, 'learning_rate': 0.00020574626865671637, 'epoch': 3.46}                                                                                                      
{'loss': 0.3069, 'learning_rate': 0.00020546641791044774, 'epoch': 3.47}                                                                                                      
{'loss': 0.3117, 'learning_rate': 0.0002051865671641791, 'epoch': 3.48}                                                                                                       
 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                   | 3900/11220 [4:01:32<1:24:22,  1.45it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3098316192626953, 'eval_wer': 0.4049521723506562, 'eval_cer': 0.1322149946575214, 'eval_runtime': 299.2143, 'eval_samples_per_second': 29.036, 'eval_steps_per_second': 3.63, 'epoch': 3.48}                                                                                                                                                
{'loss': 0.3464, 'learning_rate': 0.00020490671641791044, 'epoch': 3.48}                                                                                                      
{'loss': 0.3248, 'learning_rate': 0.00020462686567164178, 'epoch': 3.49}                                                                                                      
 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                   | 3927/11220 [4:06:54<2:12:24,  1.09s/it]Saving model checkpoint to ./portu_clean/checkpoint-3927
Configuration saved in ./portu_clean/checkpoint-3927/config.json
Model weights saved in ./portu_clean/checkpoint-3927/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-3927/preprocessor_config.json
Deleting older checkpoint [portu_clean/checkpoint-1122] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.27, 'learning_rate': 0.0002043470149253731, 'epoch': 3.5}                                                                                                          
{'loss': 0.2588, 'learning_rate': 0.00020406716417910444, 'epoch': 3.51}                                                                                                      
{'loss': 0.3305, 'learning_rate': 0.00020378731343283582, 'epoch': 3.52}                                                                                                      
{'loss': 0.3078, 'learning_rate': 0.00020350746268656716, 'epoch': 3.53}                                                                                                      
{'loss': 0.4111, 'learning_rate': 0.0002032276119402985, 'epoch': 3.54}                                                                                                       
{'loss': 0.2756, 'learning_rate': 0.00020294776119402983, 'epoch': 3.55}                                                                                                      
{'loss': 0.3007, 'learning_rate': 0.00020266791044776117, 'epoch': 3.56}                                                                                                      
{'loss': 0.2752, 'learning_rate': 0.00020238805970149252, 'epoch': 3.56}                                                                                                      
 36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                  | 4000/11220 [4:07:58<1:25:04,  1.41it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.30825352668762207, 'eval_wer': 0.40375434213453343, 'eval_cer': 0.13231790925958997, 'eval_runtime': 298.1936, 'eval_samples_per_second': 29.135, 'eval_steps_per_second': 3.642, 'epoch': 3.56}                                                                                                                                            
{'loss': 0.3084, 'learning_rate': 0.00020210820895522386, 'epoch': 3.57}                                                                                                      
{'loss': 0.3856, 'learning_rate': 0.00020182835820895523, 'epoch': 3.58}                                                                                                      
{'loss': 0.28, 'learning_rate': 0.00020154850746268655, 'epoch': 3.59}                                                                                                        
{'loss': 0.3186, 'learning_rate': 0.0002012686567164179, 'epoch': 3.6}                                                                                                        
{'loss': 0.3565, 'learning_rate': 0.00020098880597014924, 'epoch': 3.61}                                                                                                      
{'loss': 0.3535, 'learning_rate': 0.00020070895522388056, 'epoch': 3.62}                                                                                                      
{'loss': 0.3901, 'learning_rate': 0.0002004291044776119, 'epoch': 3.63}                                                                                                       
{'loss': 0.3089, 'learning_rate': 0.00020014925373134325, 'epoch': 3.64}                                                                                                      
{'loss': 0.3236, 'learning_rate': 0.00019986940298507462, 'epoch': 3.64}                                                                                                      
{'loss': 0.2957, 'learning_rate': 0.00019958955223880597, 'epoch': 3.65}                                                                                                      
 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                 | 4100/11220 [4:14:19<1:21:17,  1.46it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.2984485328197479, 'eval_wer': 0.40168380704666407, 'eval_cer': 0.13160961464535326, 'eval_runtime': 299.4935, 'eval_samples_per_second': 29.009, 'eval_steps_per_second': 3.626, 'epoch': 3.65}                                                                                                                                             
{'loss': 0.3389, 'learning_rate': 0.0001993097014925373, 'epoch': 3.66}                                                                                                       
{'loss': 0.3308, 'learning_rate': 0.00019902985074626863, 'epoch': 3.67}                                                                                                      
{'loss': 0.2891, 'learning_rate': 0.00019874999999999998, 'epoch': 3.68}                                                                                                      
{'loss': 0.286, 'learning_rate': 0.00019847014925373132, 'epoch': 3.69}                                                                                                       
{'loss': 0.3015, 'learning_rate': 0.00019819029850746264, 'epoch': 3.7}                                                                                                       
{'loss': 0.3785, 'learning_rate': 0.00019791044776119402, 'epoch': 3.71}                                                                                                      
{'loss': 0.3155, 'learning_rate': 0.00019763059701492536, 'epoch': 3.72}                                                                                                      
{'loss': 0.4588, 'learning_rate': 0.0001973507462686567, 'epoch': 3.73}                                                                                                       
{'loss': 0.3173, 'learning_rate': 0.00019707089552238805, 'epoch': 3.73}                                                                                                      
{'loss': 0.3259, 'learning_rate': 0.00019679104477611937, 'epoch': 3.74}                                                                                                      
 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                | 4200/11220 [4:20:41<1:19:59,  1.46it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.29547691345214844, 'eval_wer': 0.4004346412498503, 'eval_cer': 0.13145221584218955, 'eval_runtime': 299.6332, 'eval_samples_per_second': 28.995, 'eval_steps_per_second': 3.624, 'epoch': 3.74}                                                                                                                                             
{'loss': 0.339, 'learning_rate': 0.00019651119402985072, 'epoch': 3.75}                                                                                                       
{'loss': 0.3157, 'learning_rate': 0.0001962313432835821, 'epoch': 3.76}                                                                                                       
{'loss': 0.3181, 'learning_rate': 0.00019595149253731343, 'epoch': 3.77}                                                                                                      
{'loss': 0.3143, 'learning_rate': 0.00019567164179104475, 'epoch': 3.78}                                                                                                      
{'loss': 0.3051, 'learning_rate': 0.0001953917910447761, 'epoch': 3.79}                                                                                                       
{'loss': 0.3355, 'learning_rate': 0.00019511194029850744, 'epoch': 3.8}                                                                                                       
{'loss': 0.3261, 'learning_rate': 0.0001948320895522388, 'epoch': 3.81}                                                                                                       
{'loss': 0.2991, 'learning_rate': 0.0001945522388059701, 'epoch': 3.81}                                                                                                       
{'loss': 0.2865, 'learning_rate': 0.00019427238805970148, 'epoch': 3.82}                                                                                                      
{'loss': 0.2899, 'learning_rate': 0.00019399253731343282, 'epoch': 3.83}                                                                                                      
 38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                               | 4300/11220 [4:27:05<1:21:19,  1.42it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.2897927463054657, 'eval_wer': 0.39459949691130924, 'eval_cer': 0.12849190758268733, 'eval_runtime': 297.7707, 'eval_samples_per_second': 29.177, 'eval_steps_per_second': 3.647, 'epoch': 3.83}                                                                                                                                             
{'loss': 0.3108, 'learning_rate': 0.00019371268656716417, 'epoch': 3.84}                                                                                                      
{'loss': 0.3868, 'learning_rate': 0.00019343283582089551, 'epoch': 3.85}                                                                                                      
{'loss': 0.2609, 'learning_rate': 0.00019315298507462683, 'epoch': 3.86}                                                                                                      
{'loss': 0.3039, 'learning_rate': 0.00019287313432835818, 'epoch': 3.87}                                                                                                      
{'loss': 0.2869, 'learning_rate': 0.00019259328358208952, 'epoch': 3.88}                                                                                                      
{'loss': 0.3197, 'learning_rate': 0.0001923134328358209, 'epoch': 3.89}                                                                                                       
{'loss': 0.3759, 'learning_rate': 0.00019203358208955224, 'epoch': 3.89}                                                                                                      
{'loss': 0.2841, 'learning_rate': 0.00019175373134328356, 'epoch': 3.9}                                                                                                       
{'loss': 0.3234, 'learning_rate': 0.0001914738805970149, 'epoch': 3.91}                                                                                                       
{'loss': 0.3013, 'learning_rate': 0.00019119402985074625, 'epoch': 3.92}                                                                                                      
 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                             | 4400/11220 [4:33:26<1:18:47,  1.44it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3074724078178406, 'eval_wer': 0.40671469395437976, 'eval_cer': 0.13166712574650924, 'eval_runtime': 297.6916, 'eval_samples_per_second': 29.185, 'eval_steps_per_second': 3.648, 'epoch': 3.92}                                                                                                                                             
{'loss': 0.3377, 'learning_rate': 0.0001909141791044776, 'epoch': 3.93}                                                                                                       
{'loss': 0.3031, 'learning_rate': 0.00019063432835820894, 'epoch': 3.94}                                                                                                      
{'loss': 0.5216, 'learning_rate': 0.00019035447761194029, 'epoch': 3.95}                                                                                                      
{'loss': 0.3251, 'learning_rate': 0.00019010261194029847, 'epoch': 3.96}                                                                                                      
{'loss': 0.3656, 'learning_rate': 0.00018982276119402985, 'epoch': 3.97}                                                                                                      
{'loss': 0.3016, 'learning_rate': 0.0001895429104477612, 'epoch': 3.97}                                                                                                       
{'loss': 0.3017, 'learning_rate': 0.00018926305970149254, 'epoch': 3.98}                                                                                                      
{'loss': 0.289, 'learning_rate': 0.00018898320895522385, 'epoch': 3.99}                                                                                                       
 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                            | 4488/11220 [4:39:33<1:04:24,  1.74it/s]Saving model checkpoint to ./portu_clean/checkpoint-4488
Configuration saved in ./portu_clean/checkpoint-4488/config.json
Model weights saved in ./portu_clean/checkpoint-4488/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-4488/preprocessor_config.json
Deleting older checkpoint [portu_clean/checkpoint-1683] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.3112, 'learning_rate': 0.0001887033582089552, 'epoch': 4.0}                                                                                                        
{'loss': 0.2344, 'learning_rate': 0.00018842350746268655, 'epoch': 4.01}                                                                                                      
 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                            | 4500/11220 [4:39:51<1:59:29,  1.07s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.29776135087013245, 'eval_wer': 0.3941374766850904, 'eval_cer': 0.12891567359120504, 'eval_runtime': 297.5908, 'eval_samples_per_second': 29.194, 'eval_steps_per_second': 3.649, 'epoch': 4.01}                                                                                                                                             
{'loss': 0.2442, 'learning_rate': 0.0001881436567164179, 'epoch': 4.02}                                                                                                       
{'loss': 0.2699, 'learning_rate': 0.00018786380597014926, 'epoch': 4.03}                                                                                                      
{'loss': 0.3015, 'learning_rate': 0.00018758395522388058, 'epoch': 4.04}                                                                                                      
{'loss': 0.3222, 'learning_rate': 0.00018730410447761193, 'epoch': 4.05}                                                                                                      
{'loss': 0.2306, 'learning_rate': 0.00018702425373134327, 'epoch': 4.06}                                                                                                      
{'loss': 0.236, 'learning_rate': 0.0001867444029850746, 'epoch': 4.06}                                                                                                        
{'loss': 0.2885, 'learning_rate': 0.00018646455223880594, 'epoch': 4.07}                                                                                                      
{'loss': 0.2765, 'learning_rate': 0.0001861847014925373, 'epoch': 4.08}                                                                                                       
{'loss': 0.4149, 'learning_rate': 0.00018590485074626865, 'epoch': 4.09}                                                                                                      
{'loss': 0.2454, 'learning_rate': 0.000185625, 'epoch': 4.1}                                                                                                                  
 41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                           | 4600/11220 [4:46:10<1:54:42,  1.04s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.30612435936927795, 'eval_wer': 0.39287119902804635, 'eval_cer': 0.1293818162005745, 'eval_runtime': 299.8617, 'eval_samples_per_second': 28.973, 'eval_steps_per_second': 3.622, 'epoch': 4.1}                                                                                                                                              
{'loss': 0.3047, 'learning_rate': 0.00018534514925373132, 'epoch': 4.11}                                                                                                      
{'loss': 0.2852, 'learning_rate': 0.00018506529850746266, 'epoch': 4.12}                                                                                                      
{'loss': 0.3007, 'learning_rate': 0.000184785447761194, 'epoch': 4.13}                                                                                                        
{'loss': 0.3175, 'learning_rate': 0.00018450559701492535, 'epoch': 4.14}                                                                                                      
{'loss': 0.2444, 'learning_rate': 0.00018422574626865673, 'epoch': 4.14}                                                                                                      
{'loss': 0.2904, 'learning_rate': 0.00018394589552238804, 'epoch': 4.15}                                                                                                      
{'loss': 0.2765, 'learning_rate': 0.0001836660447761194, 'epoch': 4.16}                                                                                                       
{'loss': 0.327, 'learning_rate': 0.00018338619402985073, 'epoch': 4.17}                                                                                                       
{'loss': 0.3469, 'learning_rate': 0.00018310634328358208, 'epoch': 4.18}                                                                                                      
{'loss': 0.2482, 'learning_rate': 0.0001828264925373134, 'epoch': 4.19}                                                                                                       
 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                          | 4700/11220 [4:52:32<1:57:02,  1.08s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.30011337995529175, 'eval_wer': 0.3978507503550711, 'eval_cer': 0.1328778857708455, 'eval_runtime': 298.3099, 'eval_samples_per_second': 29.124, 'eval_steps_per_second': 3.641, 'epoch': 4.19}                                                                                                                                              
{'loss': 0.2741, 'learning_rate': 0.00018254664179104474, 'epoch': 4.2}                                                                                                       
{'loss': 0.308, 'learning_rate': 0.00018229477611940296, 'epoch': 4.21}                                                                                                       
{'loss': 0.292, 'learning_rate': 0.0001820149253731343, 'epoch': 4.22}                                                                                                        
{'loss': 0.3231, 'learning_rate': 0.00018173507462686565, 'epoch': 4.22}                                                                                                      
{'loss': 0.2937, 'learning_rate': 0.00018145522388059702, 'epoch': 4.23}                                                                                                      
{'loss': 0.3036, 'learning_rate': 0.00018117537313432834, 'epoch': 4.24}                                                                                                      
{'loss': 0.2904, 'learning_rate': 0.00018089552238805968, 'epoch': 4.25}                                                                                                      
{'loss': 0.3243, 'learning_rate': 0.00018061567164179103, 'epoch': 4.26}                                                                                                      
{'loss': 0.3638, 'learning_rate': 0.00018033582089552238, 'epoch': 4.27}                                                                                                      
{'loss': 0.3013, 'learning_rate': 0.0001800559701492537, 'epoch': 4.28}                                                                                                       
 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                         | 4800/11220 [4:58:54<1:49:10,  1.02s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.29368582367897034, 'eval_wer': 0.39278563972689473, 'eval_cer': 0.13121309073738313, 'eval_runtime': 299.5568, 'eval_samples_per_second': 29.003, 'eval_steps_per_second': 3.625, 'epoch': 4.28}                                                                                                                                            
{'loss': 0.2002, 'learning_rate': 0.00017977611940298507, 'epoch': 4.29}                                                                                                      
{'loss': 0.3183, 'learning_rate': 0.0001794962686567164, 'epoch': 4.3}                                                                                                        
{'loss': 0.279, 'learning_rate': 0.00017921641791044776, 'epoch': 4.3}                                                                                                        
{'loss': 0.3118, 'learning_rate': 0.0001789365671641791, 'epoch': 4.31}                                                                                                       
{'loss': 0.3101, 'learning_rate': 0.00017865671641791042, 'epoch': 4.32}                                                                                                      
{'loss': 0.2605, 'learning_rate': 0.00017837686567164177, 'epoch': 4.33}                                                                                                      
{'loss': 0.3248, 'learning_rate': 0.0001780970149253731, 'epoch': 4.34}                                                                                                       
{'loss': 0.3576, 'learning_rate': 0.00017781716417910448, 'epoch': 4.35}                                                                                                      
{'loss': 0.3169, 'learning_rate': 0.0001775373134328358, 'epoch': 4.36}                                                                                                       
{'loss': 0.2294, 'learning_rate': 0.00017725746268656715, 'epoch': 4.37}                                                                                                      
 44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                        | 4900/11220 [5:05:15<1:53:08,  1.07s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.28993114829063416, 'eval_wer': 0.3892605965194476, 'eval_cer': 0.1281014374748389, 'eval_runtime': 300.6293, 'eval_samples_per_second': 28.899, 'eval_steps_per_second': 3.612, 'epoch': 4.37}                                                                                                                                              
{'loss': 0.2601, 'learning_rate': 0.0001769776119402985, 'epoch': 4.38}                                                                                                       
{'loss': 0.2811, 'learning_rate': 0.00017669776119402984, 'epoch': 4.38}                                                                                                      
{'loss': 0.3536, 'learning_rate': 0.00017641791044776116, 'epoch': 4.39}                                                                                                      
{'loss': 0.3076, 'learning_rate': 0.0001761380597014925, 'epoch': 4.4}                                                                                                        
{'loss': 0.3013, 'learning_rate': 0.00017585820895522387, 'epoch': 4.41}                                                                                                      
{'loss': 0.3132, 'learning_rate': 0.00017557835820895522, 'epoch': 4.42}                                                                                                      
{'loss': 0.2843, 'learning_rate': 0.00017529850746268656, 'epoch': 4.43}                                                                                                      
{'loss': 0.3139, 'learning_rate': 0.00017501865671641788, 'epoch': 4.44}                                                                                                      
{'loss': 0.3825, 'learning_rate': 0.00017473880597014923, 'epoch': 4.45}                                                                                                      
{'loss': 0.2459, 'learning_rate': 0.00017445895522388057, 'epoch': 4.46}                                                                                                      
 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                       | 5000/11220 [5:11:38<1:48:50,  1.05s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.2803632318973541, 'eval_wer': 0.3845548349561081, 'eval_cer': 0.12672117104709554, 'eval_runtime': 300.6783, 'eval_samples_per_second': 28.895, 'eval_steps_per_second': 3.612, 'epoch': 4.46}                                                                                                                                              
{'loss': 0.2582, 'learning_rate': 0.00017417910447761195, 'epoch': 4.47}                                                                                                      
{'loss': 0.2971, 'learning_rate': 0.0001738992537313433, 'epoch': 4.47}                                                                                                       
{'loss': 0.3003, 'learning_rate': 0.0001736194029850746, 'epoch': 4.48}                                                                                                       
{'loss': 0.3603, 'learning_rate': 0.00017333955223880596, 'epoch': 4.49}                                                                                                      
 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                      | 5049/11220 [5:17:19<1:52:14,  1.09s/it]Saving model checkpoint to ./portu_clean/checkpoint-5049
Configuration saved in ./portu_clean/checkpoint-5049/config.json
Model weights saved in ./portu_clean/checkpoint-5049/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-5049/preprocessor_config.json
Deleting older checkpoint [portu_clean/checkpoint-2244] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.3095, 'learning_rate': 0.0001730597014925373, 'epoch': 4.5}                                                                                                        
{'loss': 0.2894, 'learning_rate': 0.00017277985074626865, 'epoch': 4.51}                                                                                                      
{'loss': 0.2659, 'learning_rate': 0.00017249999999999996, 'epoch': 4.52}                                                                                                      
{'loss': 0.2891, 'learning_rate': 0.00017222014925373134, 'epoch': 4.53}                                                                                                      
{'loss': 0.2692, 'learning_rate': 0.00017194029850746268, 'epoch': 4.54}                                                                                                      
{'loss': 0.2505, 'learning_rate': 0.00017166044776119403, 'epoch': 4.55}                                                                                                      
 45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                     | 5100/11220 [5:18:05<1:49:41,  1.08s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.28538280725479126, 'eval_wer': 0.38532486866647275, 'eval_cer': 0.12853125728347828, 'eval_runtime': 299.7261, 'eval_samples_per_second': 28.986, 'eval_steps_per_second': 3.623, 'epoch': 4.55}                                                                                                                                            
{'loss': 0.2577, 'learning_rate': 0.00017138059701492535, 'epoch': 4.55}                                                                                                      
{'loss': 0.3446, 'learning_rate': 0.0001711007462686567, 'epoch': 4.56}                                                                                                       
{'loss': 0.3317, 'learning_rate': 0.00017082089552238804, 'epoch': 4.57}                                                                                                      
{'loss': 0.3329, 'learning_rate': 0.00017054104477611938, 'epoch': 4.58}                                                                                                      
{'loss': 0.2472, 'learning_rate': 0.00017026119402985075, 'epoch': 4.59}                                                                                                      
{'loss': 0.3211, 'learning_rate': 0.00017000932835820894, 'epoch': 4.6}                                                                                                       
{'loss': 0.3203, 'learning_rate': 0.00016972947761194026, 'epoch': 4.61}                                                                                                      
{'loss': 0.3636, 'learning_rate': 0.00016944962686567163, 'epoch': 4.62}                                                                                                      
{'loss': 0.3809, 'learning_rate': 0.00016916977611940298, 'epoch': 4.63}                                                                                                      
{'loss': 0.3532, 'learning_rate': 0.00016888992537313432, 'epoch': 4.63}                                                                                                      
 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                    | 5200/11220 [5:24:26<1:40:31,  1.00s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3038024306297302, 'eval_wer': 0.38907236605691403, 'eval_cer': 0.12893383499157007, 'eval_runtime': 298.3719, 'eval_samples_per_second': 29.118, 'eval_steps_per_second': 3.64, 'epoch': 4.63}                                                                                                                                              
{'loss': 0.3555, 'learning_rate': 0.00016861007462686564, 'epoch': 4.64}                                                                                                      
{'loss': 0.3067, 'learning_rate': 0.00016833022388059699, 'epoch': 4.65}                                                                                                      
{'loss': 0.3428, 'learning_rate': 0.00016805037313432833, 'epoch': 4.66}                                                                                                      
{'loss': 0.4099, 'learning_rate': 0.0001677705223880597, 'epoch': 4.67}                                                                                                       
{'loss': 0.3206, 'learning_rate': 0.00016749067164179105, 'epoch': 4.68}                                                                                                      
{'loss': 0.3881, 'learning_rate': 0.00016721082089552237, 'epoch': 4.69}                                                                                                      
{'loss': 0.3219, 'learning_rate': 0.0001669309701492537, 'epoch': 4.7}                                                                                                        
{'loss': 0.3763, 'learning_rate': 0.00016665111940298506, 'epoch': 4.71}                                                                                                      
{'loss': 0.4302, 'learning_rate': 0.0001663712686567164, 'epoch': 4.71}                                                                                                       
{'loss': 0.3428, 'learning_rate': 0.00016609141791044772, 'epoch': 4.72}                                                                                                      
 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                   | 5300/11220 [5:30:47<1:43:08,  1.05s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3551158308982849, 'eval_wer': 0.40183781378873695, 'eval_cer': 0.13660097284567954, 'eval_runtime': 301.9121, 'eval_samples_per_second': 28.777, 'eval_steps_per_second': 3.597, 'epoch': 4.72}                                                                                                                                             
{'loss': 0.3893, 'learning_rate': 0.0001658115671641791, 'epoch': 4.73}                                                                                                       
{'loss': 0.3961, 'learning_rate': 0.00016553171641791044, 'epoch': 4.74}                                                                                                      
{'loss': 0.4768, 'learning_rate': 0.00016525186567164179, 'epoch': 4.75}                                                                                                      
{'loss': 0.474, 'learning_rate': 0.00016497201492537313, 'epoch': 4.76}                                                                                                       
{'loss': 0.4665, 'learning_rate': 0.00016469216417910445, 'epoch': 4.77}                                                                                                      
{'loss': 0.4393, 'learning_rate': 0.0001644123134328358, 'epoch': 4.78}                                                                                                       
{'loss': 0.5284, 'learning_rate': 0.00016413246268656714, 'epoch': 4.79}                                                                                                      
{'loss': 0.4506, 'learning_rate': 0.0001638526119402985, 'epoch': 4.79}                                                                                                       
{'loss': 0.5703, 'learning_rate': 0.00016357276119402986, 'epoch': 4.8}                                                                                                       
{'loss': 0.3897, 'learning_rate': 0.00016329291044776118, 'epoch': 4.81}                                                                                                      
 48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                  | 5400/11220 [5:37:12<1:40:37,  1.04s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3621087968349457, 'eval_wer': 0.424699259056452, 'eval_cer': 0.1388802285914926, 'eval_runtime': 299.104, 'eval_samples_per_second': 29.047, 'eval_steps_per_second': 3.631, 'epoch': 4.81}                                                                                                                                                 
{'loss': 0.4014, 'learning_rate': 0.00016301305970149252, 'epoch': 4.82}                                                                                                      
{'loss': 0.4319, 'learning_rate': 0.00016273320895522387, 'epoch': 4.83}                                                                                                      
{'loss': 0.3967, 'learning_rate': 0.00016245335820895519, 'epoch': 4.84}                                                                                                      
{'loss': 0.5096, 'learning_rate': 0.00016217350746268656, 'epoch': 4.85}                                                                                                      
{'loss': 0.4168, 'learning_rate': 0.0001618936567164179, 'epoch': 4.86}                                                                                                       
{'loss': 0.4729, 'learning_rate': 0.00016161380597014925, 'epoch': 4.87}                                                                                                      
{'loss': 0.5027, 'learning_rate': 0.0001613339552238806, 'epoch': 4.87}                                                                                                       
{'loss': 0.4767, 'learning_rate': 0.0001610541044776119, 'epoch': 4.88}                                                                                                       
{'loss': 0.4188, 'learning_rate': 0.00016077425373134326, 'epoch': 4.89}                                                                                                      
{'loss': 0.3691, 'learning_rate': 0.0001604944029850746, 'epoch': 4.9}                                                                                                        
 49%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                 | 5500/11220 [5:43:34<1:40:58,  1.06s/it]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.3477896749973297, 'eval_wer': 0.4373449237666627, 'eval_cer': 0.1479124983730412, 'eval_runtime': 312.0854, 'eval_samples_per_second': 27.839, 'eval_steps_per_second': 3.48, 'epoch': 4.9}                                                                                                                                                 
{'loss': 0.3651, 'learning_rate': 0.00016021455223880598, 'epoch': 4.91}                                                                                                      
{'loss': 0.4379, 'learning_rate': 0.00015993470149253732, 'epoch': 4.92}                                                                                                      
{'loss': 0.4953, 'learning_rate': 0.00015965485074626864, 'epoch': 4.93}                                                                                                      
{'loss': 0.5268, 'learning_rate': 0.00015937499999999998, 'epoch': 4.94}                                                                                                      
{'loss': 0.4065, 'learning_rate': 0.00015909514925373133, 'epoch': 4.95}                                                                                                      
{'loss': 0.4187, 'learning_rate': 0.00015881529850746268, 'epoch': 4.96}                                                                                                      
{'loss': 0.4533, 'learning_rate': 0.000158535447761194, 'epoch': 4.96}                                                                                                        
{'loss': 0.4628, 'learning_rate': 0.00015825559701492537, 'epoch': 4.97}                                                                                                      
{'loss': 0.4452, 'learning_rate': 0.0001579757462686567, 'epoch': 4.98}                                                                                                       
{'loss': 0.4485, 'learning_rate': 0.00015769589552238806, 'epoch': 4.99}                                                                                                      
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                | 5600/11220 [5:50:08<1:22:45,  1.13it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8
{'eval_loss': 0.39998993277549744, 'eval_wer': 0.4547134619004432, 'eval_cer': 0.14597830923416402, 'eval_runtime': 311.4487, 'eval_samples_per_second': 27.895, 'eval_steps_per_second': 3.487, 'epoch': 4.99}                                                                                                                                             
{'loss': 0.4813, 'learning_rate': 0.00015741604477611937, 'epoch': 5.0}                                                                                                       
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 5610/11220 [5:55:26<6:49:52,  4.38s/it]Saving model checkpoint to ./portu_clean/checkpoint-5610
Configuration saved in ./portu_clean/checkpoint-5610/config.json
Model weights saved in ./portu_clean/checkpoint-5610/pytorch_model.bin
Feature extractor saved in ./portu_clean/checkpoint-5610/preprocessor_config.json
Deleting older checkpoint [portu_clean/checkpoint-2805] due to args.save_total_limit
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.4112, 'learning_rate': 0.00015713619402985072, 'epoch': 5.01}                                                                                                      
{'loss': 0.3793, 'learning_rate': 0.00015685634328358207, 'epoch': 5.02}                                                                                                      
{'loss': 0.4256, 'learning_rate': 0.0001565764925373134, 'epoch': 5.03}                                                                                                       
{'loss': 0.458, 'learning_rate': 0.00015629664179104478, 'epoch': 5.04}                                                                                                       
{'loss': 0.4943, 'learning_rate': 0.0001560167910447761, 'epoch': 5.04}                                                                                                       
{'loss': 0.3343, 'learning_rate': 0.00015573694029850745, 'epoch': 5.05}                                                                                                      
{'loss': 0.3665, 'learning_rate': 0.0001554570895522388, 'epoch': 5.06}                                                                                                       
{'loss': 0.4097, 'learning_rate': 0.00015517723880597014, 'epoch': 5.07}                                                                                                      
{'loss': 0.4107, 'learning_rate': 0.00015489738805970146, 'epoch': 5.08}                                                                                                      
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 5700/11220 [5:56:49<59:25,  1.55it/s]***** Running Evaluation *****
  Num examples = 8688
  Batch size = 8

^CTraceback (most recent call last):‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                            | 334/1086 [01:42<04:10,  3.00it/s]
  File "/home/or/Desktop/wav2vec2/main_portu.py", line 316, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2974, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 3217, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2540, in compute_loss
    outputs = model(**inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1693, in forward
    if labels.max() >= self.config.vocab_size:
KeyboardInterrupt
 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                               | 5700/11220 [5:58:33<5:47:13,  3.77s/it]

(base) or@anidjar:~/Desktop/wav2vec2$                                                                                                                                         


