(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Using custom data configuration default-21d5287079b7cf06
Found cached dataset csv (/home/or/.cache/huggingface/datasets/csv/default-21d5287079b7cf06/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1008.25it/s]
Using custom data configuration default-dbc311f39ef5c13c
Found cached dataset csv (/home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1205.26it/s]
Loading cached processed dataset at /home/or/.cache/huggingface/datasets/csv/default-21d5287079b7cf06/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-be29a9cb30c479ad.arrow
Loading cached processed dataset at /home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2fff3bd7311f6c7f.arrow
----------------- Loading Datasets complete. -----------------


----------------- Removing special characters... -----------------
Parameter 'function'=<function remove_special_characters at 0x7f92dc892dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33649/33649 [00:00<00:00, 35017.48ex/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10438/10438 [00:00<00:00, 34716.31ex/s]
----------------- Removing special characters complete. -----------------


train columns: ['path', 'audio', 'sentence']
test columns: ['path', 'audio', 'sentence']
----------------- saving datasets pre-vocab... -----------------
----------------- saving datasets pre-vocab complete. -----------


----------------- Extracting all characters... -----------------
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
----------------- Extracting all characters complete. -----------------


----------------- Preparing vocab... -----------------
Vocab_dict: {'ا': 0, 'خ': 1, 'ۘ': 2, 'ظ': 3, 'ھ': 4, 'چ': 5, ' ': 6, 'ؤ': 7, 'ع': 8, 'ل': 9, 'آ': 10, 'ق': 11, 'ه': 12, '-': 13, 'ﺃ': 14, 'ۗ': 15, 'ش': 16, 'ط': 17, 't': 18, 'ی': 19, 'ث': 20, 'ڨ': 21, 'm': 22, 'ز': 23, 'ب': 24, 'م': 25, 'ت': 26, 'أ': 27, 'ك': 28, 'ى': 29, 'ـ': 30, 'ي': 31, 'ئ': 32, 'ح': 33, 'a': 34, 'ر': 35, 'ۛ': 36, 'ة': 37, 'ن': 38, 'إ': 39, 'س': 40, 'ذ': 41, 'ج': 42, 'ء': 43, 'غ': 44, 'ف': 45, 'ک': 46, 'ﻻ': 47, 'h': 48, 'ص': 49, 'د': 50, 'ض': 51, 'و': 52}
Vocab_len: 55
----------------- Preparing vocab complete. -----------------


----------------- Saving vocab to jason... -----------------
----------------- Saving vocab to jason complete. -----------------


sample path: common_voice_ar_24089467.mp3

Sanity Check sampling rate is 48khz: {'path': '/home/or/Desktop/arabic_new_dataset/train/common_voice_ar_24089467.mp3', 'array': array([-2.84865515e-14, -4.09750879e-12, -5.45291286e-12, ...,
        5.32540653e-05,  9.98841097e-06, -1.20470195e-05], dtype=float32), 'sampling_rate': 48000}

----------------- Resampling to 16khz... -----------------
Making sure the sampling rate changed to 16khz {'path': '/home/or/Desktop/arabic_new_dataset/train/common_voice_ar_24089467.mp3', 'array': array([-1.9609438e-12, -1.9440803e-12, -1.2969034e-12, ...,
        6.6676068e-05,  6.1965700e-05,  2.8741959e-05], dtype=float32), 'sampling_rate': 16000}
----------------- Resampling complete. -----------------


----------------- Preparing datasets... -----------------
#0:   0%|                                                                                                                                            | 0/8413 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/8412 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [10:45<00:00, 13.03ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8413/8413 [10:46<00:00, 13.00ex/s]
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [10:56<00:00, 12.81ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [10:57<00:00, 12.80ex/s]
#0:   0%|                                                                                                                                            | 0/2610 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/2609 [00:00<?, ?ex/s]
  warnings.warn(

/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(



/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed i
#0:   0%|                                                                                                                                    | 1/2610 [00:00<05:43,  7.60ex/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                    | 1/2609 [00:00<05:35,  7.76ex/s]
  warnings.warn(
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [04:18<00:00, 10.11ex/s]
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [04:20<00:00, 10.03ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [04:24<00:00,  9.88ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [04:24<00:00,  9.88ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [04:24<00:00, 13.79ex/s]

----------------- Preparing datasets complete. -----------------


----------------- saving datasets... -----------------
----------------- saving datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:248: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 33649
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 10520
  Number of trainable parameters = 311284919
  0%|                                                                                                                                               | 0/10520 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 26.5486, 'learning_rate': 4.8e-06, 'epoch': 0.01}                                                                                                                    
{'loss': 29.0567, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.02}                                                                                                     
{'loss': 30.2315, 'learning_rate': 1.6199999999999997e-05, 'epoch': 0.03}                                                                                                     
{'loss': 31.1567, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.04}                                                                                                     
{'loss': 27.7737, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.05}                                                                                                     
{'loss': 21.8198, 'learning_rate': 3.36e-05, 'epoch': 0.06}                                                                                                                   
{'loss': 18.5644, 'learning_rate': 3.96e-05, 'epoch': 0.07}                                                                                                                   
{'loss': 14.5421, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.08}                                                                                                     
{'loss': 10.8673, 'learning_rate': 5.1e-05, 'epoch': 0.09}                                                                                                                    
{'loss': 8.114, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.1}                                                                                                        
  1%|█▏                                                                                                                                 | 100/10520 [02:24<2:10:16,  1.33it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 6.531461238861084, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 350.1421, 'eval_samples_per_second': 29.811, 'eval_steps_per_second': 3.727, 'epoch': 0.1}                                                                                                                                                                  
{'loss': 6.1699, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.1}                                                                                                        
{'loss': 5.0978, 'learning_rate': 6.9e-05, 'epoch': 0.11}                                                                                                                     
{'loss': 4.5927, 'learning_rate': 7.5e-05, 'epoch': 0.12}                                                                                                                     
{'loss': 4.173, 'learning_rate': 8.1e-05, 'epoch': 0.13}                                                                                                                      
{'loss': 3.8494, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.14}                                                                                                       
{'loss': 3.4812, 'learning_rate': 9.3e-05, 'epoch': 0.15}                                                                                                                     
{'loss': 3.4227, 'learning_rate': 9.9e-05, 'epoch': 0.16}                                                                                                                     
{'loss': 3.4267, 'learning_rate': 0.00010499999999999999, 'epoch': 0.17}                                                                                                      
{'loss': 3.4094, 'learning_rate': 0.00011099999999999999, 'epoch': 0.18}                                                                                                      
{'loss': 3.4472, 'learning_rate': 0.000117, 'epoch': 0.19}                                                                                                                    
  2%|██▍                                                                                                                                | 200/10520 [10:33<2:09:47,  1.33it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.370983839035034, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 346.6873, 'eval_samples_per_second': 30.108, 'eval_steps_per_second': 3.764, 'epoch': 0.19}                                                                                                                                                                 
{'loss': 3.28, 'learning_rate': 0.00012299999999999998, 'epoch': 0.2}                                                                                                         
{'loss': 3.3119, 'learning_rate': 0.000129, 'epoch': 0.21}                                                                                                                    
{'loss': 3.363, 'learning_rate': 0.000135, 'epoch': 0.22}                                                                                                                     
{'loss': 3.4014, 'learning_rate': 0.00014099999999999998, 'epoch': 0.23}                                                                                                      
{'loss': 3.4149, 'learning_rate': 0.000147, 'epoch': 0.24}                                                                                                                    
{'loss': 3.2755, 'learning_rate': 0.00015299999999999998, 'epoch': 0.25}                                                                                                      
{'loss': 3.3001, 'learning_rate': 0.000159, 'epoch': 0.26}                                                                                                                    
{'loss': 3.3385, 'learning_rate': 0.000165, 'epoch': 0.27}                                                                                                                    
{'loss': 3.3261, 'learning_rate': 0.00017099999999999998, 'epoch': 0.28}                                                                                                      
{'loss': 3.3169, 'learning_rate': 0.00017699999999999997, 'epoch': 0.29}                                                                                                      
  3%|███▋                                                                                                                               | 300/10520 [18:38<2:08:50,  1.32it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.2703347206115723, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 347.6547, 'eval_samples_per_second': 30.024, 'eval_steps_per_second': 3.754, 'epoch': 0.29}                                                                                                                                                                
{'loss': 3.218, 'learning_rate': 0.00018299999999999998, 'epoch': 0.29}                                                                                                       
{'loss': 3.2466, 'learning_rate': 0.00018899999999999999, 'epoch': 0.3}                                                                                                       
{'loss': 3.2062, 'learning_rate': 0.000195, 'epoch': 0.31}                                                                                                                    
{'loss': 3.1649, 'learning_rate': 0.000201, 'epoch': 0.32}                                                                                                                    
{'loss': 3.1738, 'learning_rate': 0.00020699999999999996, 'epoch': 0.33}                                                                                                      
{'loss': 3.131, 'learning_rate': 0.00021299999999999997, 'epoch': 0.34}                                                                                                       
{'loss': 3.1298, 'learning_rate': 0.00021899999999999998, 'epoch': 0.35}                                                                                                      
{'loss': 3.1349, 'learning_rate': 0.000225, 'epoch': 0.36}                                                                                                                    
{'loss': 3.1361, 'learning_rate': 0.00023099999999999998, 'epoch': 0.37}                                                                                                      
{'loss': 3.0812, 'learning_rate': 0.000237, 'epoch': 0.38}                                                                                                                    
  4%|████▉                                                                                                                              | 400/10520 [26:47<2:11:59,  1.28it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.173868179321289, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 349.815, 'eval_samples_per_second': 29.839, 'eval_steps_per_second': 3.731, 'epoch': 0.38}                                                                                                                                                                  
{'loss': 3.1611, 'learning_rate': 0.000243, 'epoch': 0.39}                                                                                                                    
{'loss': 3.1151, 'learning_rate': 0.000249, 'epoch': 0.4}                                                                                                                     
{'loss': 3.1263, 'learning_rate': 0.00025499999999999996, 'epoch': 0.41}                                                                                                      
{'loss': 3.117, 'learning_rate': 0.000261, 'epoch': 0.42}                                                                                                                     
{'loss': 3.0926, 'learning_rate': 0.000267, 'epoch': 0.43}                                                                                                                    
{'loss': 3.1035, 'learning_rate': 0.00027299999999999997, 'epoch': 0.44}                                                                                                      
{'loss': 3.116, 'learning_rate': 0.000279, 'epoch': 0.45}                                                                                                                     
{'loss': 3.1138, 'learning_rate': 0.000285, 'epoch': 0.46}                                                                                                                    
{'loss': 3.0814, 'learning_rate': 0.00029099999999999997, 'epoch': 0.47}                                                                                                      
{'loss': 3.063, 'learning_rate': 0.00029699999999999996, 'epoch': 0.48}                                                                                                       
  5%|██████▏                                                                                                                            | 500/10520 [34:56<2:09:46,  1.29it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.161830186843872, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 351.3207, 'eval_samples_per_second': 29.711, 'eval_steps_per_second': 3.715, 'epoch': 0.48}                                                                                                                                                                 
{'loss': 3.1081, 'learning_rate': 0.00029985029940119757, 'epoch': 0.48}                                                                                                      
{'loss': 3.0849, 'learning_rate': 0.0002995508982035928, 'epoch': 0.49}                                                                                                       
{'loss': 3.0803, 'learning_rate': 0.000299251497005988, 'epoch': 0.5}                                                                                                         
{'loss': 3.0515, 'learning_rate': 0.0002989520958083832, 'epoch': 0.51}                                                                                                       
{'loss': 3.0394, 'learning_rate': 0.00029865269461077845, 'epoch': 0.52}                                                                                                      
{'loss': 3.1217, 'learning_rate': 0.00029835329341317365, 'epoch': 0.53}                                                                                                      
{'loss': 3.0834, 'learning_rate': 0.00029805389221556884, 'epoch': 0.54}                                                                                                      
{'loss': 3.0469, 'learning_rate': 0.00029775449101796404, 'epoch': 0.55}                                                                                                      
{'loss': 3.0226, 'learning_rate': 0.0002974550898203593, 'epoch': 0.56}                                                                                                       
{'loss': 2.946, 'learning_rate': 0.0002971556886227545, 'epoch': 0.57}                                                                                                        
  6%|███████▍                                                                                                                           | 600/10520 [43:07<2:10:46,  1.26it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 2.8410658836364746, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 351.9245, 'eval_samples_per_second': 29.66, 'eval_steps_per_second': 3.708, 'epoch': 0.57}                                                                                                                                                                 
{'loss': 2.8501, 'learning_rate': 0.00029685628742514967, 'epoch': 0.58}                                                                                                      
{'loss': 2.6888, 'learning_rate': 0.00029655688622754487, 'epoch': 0.59}                                                                                                      
{'loss': 2.5161, 'learning_rate': 0.00029625748502994006, 'epoch': 0.6}                                                                                                       
{'loss': 2.3577, 'learning_rate': 0.0002959580838323353, 'epoch': 0.61}                                                                                                       
{'loss': 2.3361, 'learning_rate': 0.0002956586826347305, 'epoch': 0.62}                                                                                                       
{'loss': 1.9202, 'learning_rate': 0.0002953592814371257, 'epoch': 0.63}                                                                                                       
{'loss': 1.6602, 'learning_rate': 0.00029505988023952094, 'epoch': 0.64}                                                                                                      
{'loss': 1.6766, 'learning_rate': 0.00029476047904191614, 'epoch': 0.65}                                                                                                      
{'loss': 1.5926, 'learning_rate': 0.00029446107784431133, 'epoch': 0.66}                                                                                                      
{'loss': 1.7574, 'learning_rate': 0.0002941616766467066, 'epoch': 0.67}                                                                                                       
  7%|████████▋                                                                                                                          | 700/10520 [51:21<2:08:51,  1.27it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.1182804107666016, 'eval_wer': 1.99185667752443, 'eval_cer': 0.37636207005231864, 'eval_runtime': 352.7775, 'eval_samples_per_second': 29.588, 'eval_steps_per_second': 3.699, 'epoch': 0.67}                                                                                                                                                
{'loss': 1.2904, 'learning_rate': 0.0002938622754491018, 'epoch': 0.67}                                                                                                       
{'loss': 1.3493, 'learning_rate': 0.00029356287425149697, 'epoch': 0.68}                                                                                                      
{'loss': 1.3733, 'learning_rate': 0.0002932634730538922, 'epoch': 0.69}                                                                                                       
{'loss': 1.3307, 'learning_rate': 0.0002929640718562874, 'epoch': 0.7}                                                                                                        
{'loss': 1.5502, 'learning_rate': 0.0002926646706586826, 'epoch': 0.71}                                                                                                       
{'loss': 1.1801, 'learning_rate': 0.00029236526946107785, 'epoch': 0.72}                                                                                                      
{'loss': 1.133, 'learning_rate': 0.00029206586826347304, 'epoch': 0.73}                                                                                                       
{'loss': 1.1256, 'learning_rate': 0.00029176646706586824, 'epoch': 0.74}                                                                                                      
{'loss': 1.167, 'learning_rate': 0.00029146706586826343, 'epoch': 0.75}                                                                                                       
{'loss': 1.3934, 'learning_rate': 0.0002911676646706587, 'epoch': 0.76}                                                                                                       
  8%|█████████▉                                                                                                                         | 800/10520 [59:35<2:06:05,  1.28it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7539576292037964, 'eval_wer': 1.9689595707990037, 'eval_cer': 0.29920366440442925, 'eval_runtime': 352.1678, 'eval_samples_per_second': 29.639, 'eval_steps_per_second': 3.706, 'epoch': 0.76}                                                                                                                                              
{'loss': 0.9436, 'learning_rate': 0.0002908682634730539, 'epoch': 0.77}                                                                                                       
{'loss': 0.9578, 'learning_rate': 0.00029056886227544907, 'epoch': 0.78}                                                                                                      
{'loss': 1.0025, 'learning_rate': 0.00029026946107784426, 'epoch': 0.79}                                                                                                      
{'loss': 1.0815, 'learning_rate': 0.00028997005988023946, 'epoch': 0.8}                                                                                                       
{'loss': 1.2859, 'learning_rate': 0.0002896706586826347, 'epoch': 0.81}                                                                                                       
{'loss': 0.9195, 'learning_rate': 0.0002893712574850299, 'epoch': 0.82}                                                                                                       
{'loss': 0.9006, 'learning_rate': 0.0002890718562874251, 'epoch': 0.83}                                                                                                       
{'loss': 0.9358, 'learning_rate': 0.00028877245508982034, 'epoch': 0.84}                                                                                                      
{'loss': 1.0297, 'learning_rate': 0.00028847305389221553, 'epoch': 0.85}                                                                                                      
{'loss': 1.2631, 'learning_rate': 0.0002881736526946108, 'epoch': 0.86}                                                                                                       
  9%|███████████                                                                                                                      | 900/10520 [1:07:52<2:06:01,  1.27it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6473617553710938, 'eval_wer': 1.9375359264226863, 'eval_cer': 0.26866976236001094, 'eval_runtime': 353.6278, 'eval_samples_per_second': 29.517, 'eval_steps_per_second': 3.69, 'epoch': 0.86}                                                                                                                                               
{'loss': 0.8226, 'learning_rate': 0.000287874251497006, 'epoch': 0.87}                                                                                                        
{'loss': 0.8804, 'learning_rate': 0.00028757485029940117, 'epoch': 0.87}                                                                                                      
{'loss': 0.8877, 'learning_rate': 0.0002872754491017964, 'epoch': 0.88}                                                                                                       
{'loss': 0.9448, 'learning_rate': 0.0002869760479041916, 'epoch': 0.89}                                                                                                       
{'loss': 1.1603, 'learning_rate': 0.0002866766467065868, 'epoch': 0.9}                                                                                                        
{'loss': 0.7426, 'learning_rate': 0.000286377245508982, 'epoch': 0.91}                                                                                                        
{'loss': 0.8459, 'learning_rate': 0.00028607784431137725, 'epoch': 0.92}                                                                                                      
{'loss': 0.8594, 'learning_rate': 0.00028577844311377244, 'epoch': 0.93}                                                                                                      
{'loss': 0.8663, 'learning_rate': 0.00028547904191616764, 'epoch': 0.94}                                                                                                      
{'loss': 1.1565, 'learning_rate': 0.00028517964071856283, 'epoch': 0.95}                                                                                                      
 10%|████████████▏                                                                                                                   | 1000/10520 [1:16:06<2:07:23,  1.25it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5661130547523499, 'eval_wer': 1.9132017627898066, 'eval_cer': 0.24955770806631228, 'eval_runtime': 356.9751, 'eval_samples_per_second': 29.24, 'eval_steps_per_second': 3.656, 'epoch': 0.95}                                                                                                                                               
{'loss': 0.8236, 'learning_rate': 0.000284880239520958, 'epoch': 0.96}                                                                                                        
{'loss': 0.7727, 'learning_rate': 0.00028458083832335327, 'epoch': 0.97}                                                                                                      
{'loss': 0.8434, 'learning_rate': 0.00028428143712574846, 'epoch': 0.98}                                                                                                      
{'loss': 0.8992, 'learning_rate': 0.00028398203592814366, 'epoch': 0.99}                                                                                                      
{'loss': 1.1754, 'learning_rate': 0.0002836826347305389, 'epoch': 1.0}                                                                                                        
 10%|████████████▊                                                                                                                   | 1052/10520 [1:23:16<2:46:36,  1.06s/it]Saving model checkpoint to ./20_band/checkpoint-1052
Configuration saved in ./20_band/checkpoint-1052/config.json
Model weights saved in ./20_band/checkpoint-1052/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-1052/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1749, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2508, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2540, in compute_loss
    outputs = model(**inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1677, in forward
    outputs = self.wav2vec2(
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1297, in forward
    extract_features = self.feature_extractor(input_values)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 456, in forward
    hidden_states = conv_layer(hidden_states)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 331, in forward
    hidden_states = self.layer_norm(hidden_states)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 15.75 GiB total capacity; 10.12 GiB already allocated; 2.14 GiB free; 12.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 10%|████████████▋                                                                                                                  | 1052/10520 [1:23:21<12:30:12,  4.75s/it]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading datasets... -----------------
----------------- Loading datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:248: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'project_q.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 33649
  Num Epochs = 10
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 10520
  Number of trainable parameters = 311284919
  0%|                                                                                                                                               | 0/10520 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 26.0718, 'learning_rate': 4.8e-06, 'epoch': 0.01}                                                                                                                    
{'loss': 28.5372, 'learning_rate': 1.02e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 29.6307, 'learning_rate': 1.6199999999999997e-05, 'epoch': 0.03}                                                                                                     
{'loss': 30.4269, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.04}                                                                                                     
{'loss': 26.7291, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.05}                                                                                                     
{'loss': 20.1918, 'learning_rate': 3.36e-05, 'epoch': 0.06}                                                                                                                   
{'loss': 16.1532, 'learning_rate': 3.9e-05, 'epoch': 0.07}                                                                                                                    
{'loss': 12.1587, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.08}                                                                                                     
{'loss': 8.7384, 'learning_rate': 5.1e-05, 'epoch': 0.09}                                                                                                                     
{'loss': 6.7078, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.1}                                                                                                       
  1%|█▏                                                                                                                                 | 100/10520 [02:20<2:11:08,  1.32it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 5.475041389465332, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 352.465, 'eval_samples_per_second': 29.614, 'eval_steps_per_second': 3.702, 'epoch': 0.1}                                                                                                                                                                   
{'loss': 5.253, 'learning_rate': 6.299999999999999e-05, 'epoch': 0.1}                                                                                                         
{'loss': 4.5253, 'learning_rate': 6.9e-05, 'epoch': 0.11}                                                                                                                     
{'loss': 4.1693, 'learning_rate': 7.5e-05, 'epoch': 0.12}                                                                                                                     
{'loss': 3.8815, 'learning_rate': 8.1e-05, 'epoch': 0.13}                                                                                                                     
{'loss': 3.6926, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.14}                                                                                                       
{'loss': 3.4052, 'learning_rate': 9.3e-05, 'epoch': 0.15}                                                                                                                     
{'loss': 3.3849, 'learning_rate': 9.9e-05, 'epoch': 0.16}                                                                                                                     
{'loss': 3.4045, 'learning_rate': 0.00010499999999999999, 'epoch': 0.17}                                                                                                      
{'loss': 3.3984, 'learning_rate': 0.00011099999999999999, 'epoch': 0.18}                                                                                                      
{'loss': 3.4394, 'learning_rate': 0.000117, 'epoch': 0.19}                                                                                                                    
  2%|██▍                                                                                                                                | 200/10520 [10:31<2:10:54,  1.31it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.3711233139038086, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 348.6591, 'eval_samples_per_second': 29.938, 'eval_steps_per_second': 3.743, 'epoch': 0.19}                                                                                                                                                                
{'loss': 3.2744, 'learning_rate': 0.00012299999999999998, 'epoch': 0.2}                                                                                                       
{'loss': 3.3128, 'learning_rate': 0.000129, 'epoch': 0.21}                                                                                                                    
{'loss': 3.3638, 'learning_rate': 0.000135, 'epoch': 0.22}                                                                                                                    
{'loss': 3.398, 'learning_rate': 0.00014099999999999998, 'epoch': 0.23}                                                                                                       
{'loss': 3.4052, 'learning_rate': 0.000147, 'epoch': 0.24}                                                                                                                    
{'loss': 3.2751, 'learning_rate': 0.00015299999999999998, 'epoch': 0.25}                                                                                                      
{'loss': 3.2812, 'learning_rate': 0.000159, 'epoch': 0.26}                                                                                                                    
{'loss': 3.2738, 'learning_rate': 0.000165, 'epoch': 0.27}                                                                                                                    
{'loss': 3.2398, 'learning_rate': 0.00017099999999999998, 'epoch': 0.28}                                                                                                      
{'loss': 3.1831, 'learning_rate': 0.00017699999999999997, 'epoch': 0.29}                                                                                                      
  3%|███▋                                                                                                                               | 300/10520 [18:39<2:12:00,  1.29it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.15828800201416, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 368.0909, 'eval_samples_per_second': 28.357, 'eval_steps_per_second': 3.545, 'epoch': 0.29}                                                                                                                                                                  
{'loss': 3.1627, 'learning_rate': 0.00018299999999999998, 'epoch': 0.29}                                                                                                      
{'loss': 3.179, 'learning_rate': 0.00018899999999999999, 'epoch': 0.3}                                                                                                        
{'loss': 3.1622, 'learning_rate': 0.000195, 'epoch': 0.31}                                                                                                                    
{'loss': 3.1096, 'learning_rate': 0.000201, 'epoch': 0.32}                                                                                                                    
{'loss': 3.1045, 'learning_rate': 0.00020699999999999996, 'epoch': 0.33}                                                                                                      
{'loss': 3.1026, 'learning_rate': 0.00021299999999999997, 'epoch': 0.34}                                                                                                      
{'loss': 3.1171, 'learning_rate': 0.00021899999999999998, 'epoch': 0.35}                                                                                                      
{'loss': 3.1393, 'learning_rate': 0.000225, 'epoch': 0.36}                                                                                                                    
{'loss': 3.1381, 'learning_rate': 0.00023099999999999998, 'epoch': 0.37}                                                                                                      
{'loss': 3.0686, 'learning_rate': 0.000237, 'epoch': 0.38}                                                                                                                    
  4%|████▉                                                                                                                              | 400/10520 [27:10<2:14:21,  1.26it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.134550094604492, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 358.1195, 'eval_samples_per_second': 29.147, 'eval_steps_per_second': 3.644, 'epoch': 0.38}                                                                                                                                                                 
{'loss': 3.1271, 'learning_rate': 0.000243, 'epoch': 0.39}                                                                                                                    
{'loss': 3.1011, 'learning_rate': 0.000249, 'epoch': 0.4}                                                                                                                     
{'loss': 3.1069, 'learning_rate': 0.00025499999999999996, 'epoch': 0.41}                                                                                                      
{'loss': 3.0894, 'learning_rate': 0.000261, 'epoch': 0.42}                                                                                                                    
{'loss': 3.0824, 'learning_rate': 0.000267, 'epoch': 0.43}                                                                                                                    
{'loss': 3.0868, 'learning_rate': 0.00027299999999999997, 'epoch': 0.44}                                                                                                      
{'loss': 3.0989, 'learning_rate': 0.000279, 'epoch': 0.45}                                                                                                                    
{'loss': 3.0967, 'learning_rate': 0.000285, 'epoch': 0.46}                                                                                                                    
{'loss': 3.1, 'learning_rate': 0.00029099999999999997, 'epoch': 0.47}                                                                                                         
{'loss': 3.0293, 'learning_rate': 0.00029699999999999996, 'epoch': 0.48}                                                                                                      
  5%|██████▏                                                                                                                            | 500/10520 [35:28<2:12:45,  1.26it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.0602099895477295, 'eval_wer': 1.0, 'eval_cer': 0.95951505473494, 'eval_runtime': 347.9275, 'eval_samples_per_second': 30.0, 'eval_steps_per_second': 3.751, 'epoch': 0.48}                                                                                                                                                                  
{'loss': 3.0443, 'learning_rate': 0.00029985029940119757, 'epoch': 0.48}                                                                                                      
{'loss': 2.9475, 'learning_rate': 0.0002995508982035928, 'epoch': 0.49}                                                                                                       
  5%|██████▌                                                                                                                            | 526/10520 [42:02<3:42:05,  1.33s/it]Saving model checkpoint to ./20_band/checkpoint-526
Configuration saved in ./20_band/checkpoint-526/config.json
Model weights saved in ./20_band/checkpoint-526/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-526/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 2.8256, 'learning_rate': 0.000299251497005988, 'epoch': 0.5}                                                                                                         
{'loss': 2.8434, 'learning_rate': 0.0002989520958083832, 'epoch': 0.51}                                                                                                       
{'loss': 2.6436, 'learning_rate': 0.00029865269461077845, 'epoch': 0.52}                                                                                                      
{'loss': 2.6549, 'learning_rate': 0.00029835329341317365, 'epoch': 0.53}                                                                                                      
{'loss': 2.2856, 'learning_rate': 0.00029805389221556884, 'epoch': 0.54}                                                                                                      
{'loss': 2.034, 'learning_rate': 0.00029775449101796404, 'epoch': 0.55}                                                                                                       
{'loss': 1.9991, 'learning_rate': 0.0002974550898203593, 'epoch': 0.56}                                                                                                       
{'loss': 1.9661, 'learning_rate': 0.0002971556886227545, 'epoch': 0.57}                                                                                                       
  6%|███████▍                                                                                                                           | 600/10520 [43:39<2:11:06,  1.26it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.3027195930480957, 'eval_wer': 1.997125886185093, 'eval_cer': 0.4623537074779905, 'eval_runtime': 349.8338, 'eval_samples_per_second': 29.837, 'eval_steps_per_second': 3.73, 'epoch': 0.57}                                                                                                                                                 
{'loss': 1.5199, 'learning_rate': 0.00029685628742514967, 'epoch': 0.58}                                                                                                      
{'loss': 1.4286, 'learning_rate': 0.00029655688622754487, 'epoch': 0.59}                                                                                                      
{'loss': 1.3013, 'learning_rate': 0.00029625748502994006, 'epoch': 0.6}                                                                                                       
{'loss': 1.4247, 'learning_rate': 0.0002959580838323353, 'epoch': 0.61}                                                                                                       
{'loss': 1.6173, 'learning_rate': 0.0002956586826347305, 'epoch': 0.62}                                                                                                       
{'loss': 1.1585, 'learning_rate': 0.0002953592814371257, 'epoch': 0.63}                                                                                                       
{'loss': 1.1217, 'learning_rate': 0.00029505988023952094, 'epoch': 0.64}                                                                                                      
{'loss': 1.1425, 'learning_rate': 0.00029476047904191614, 'epoch': 0.65}                                                                                                      
{'loss': 1.1672, 'learning_rate': 0.00029446107784431133, 'epoch': 0.66}                                                                                                      
{'loss': 1.3856, 'learning_rate': 0.0002941616766467066, 'epoch': 0.67}                                                                                                       
  7%|████████▋                                                                                                                          | 700/10520 [51:50<2:09:30,  1.26it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.8029273152351379, 'eval_wer': 1.9679057290668711, 'eval_cer': 0.30633916752463597, 'eval_runtime': 350.9485, 'eval_samples_per_second': 29.742, 'eval_steps_per_second': 3.718, 'epoch': 0.67}                                                                                                                                              
{'loss': 0.9857, 'learning_rate': 0.0002938622754491018, 'epoch': 0.67}                                                                                                       
{'loss': 1.0578, 'learning_rate': 0.00029356287425149697, 'epoch': 0.68}                                                                                                      
{'loss': 1.0925, 'learning_rate': 0.0002932634730538922, 'epoch': 0.69}                                                                                                       
{'loss': 1.1202, 'learning_rate': 0.0002929640718562874, 'epoch': 0.7}                                                                                                        
{'loss': 1.3422, 'learning_rate': 0.0002926646706586826, 'epoch': 0.71}                                                                                                       
{'loss': 0.9641, 'learning_rate': 0.00029236526946107785, 'epoch': 0.72}                                                                                                      
{'loss': 0.939, 'learning_rate': 0.00029206586826347304, 'epoch': 0.73}                                                                                                       
{'loss': 0.9631, 'learning_rate': 0.00029176646706586824, 'epoch': 0.74}                                                                                                      
{'loss': 1.0188, 'learning_rate': 0.00029146706586826343, 'epoch': 0.75}                                                                                                      
{'loss': 1.2399, 'learning_rate': 0.0002911676646706587, 'epoch': 0.76}                                                                                                       
  8%|█████████▊                                                                                                                       | 800/10520 [1:00:02<2:06:04,  1.28it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6398778557777405, 'eval_wer': 1.9391645909178004, 'eval_cer': 0.26416489819931505, 'eval_runtime': 352.3529, 'eval_samples_per_second': 29.624, 'eval_steps_per_second': 3.704, 'epoch': 0.76}                                                                                                                                              
{'loss': 0.8245, 'learning_rate': 0.0002908682634730539, 'epoch': 0.77}                                                                                                       
{'loss': 0.8361, 'learning_rate': 0.00029056886227544907, 'epoch': 0.78}                                                                                                      
{'loss': 0.9021, 'learning_rate': 0.00029026946107784426, 'epoch': 0.79}                                                                                                      
{'loss': 0.9514, 'learning_rate': 0.00028997005988023946, 'epoch': 0.8}                                                                                                       
{'loss': 1.1876, 'learning_rate': 0.0002896706586826347, 'epoch': 0.81}                                                                                                       
{'loss': 0.8094, 'learning_rate': 0.0002893712574850299, 'epoch': 0.82}                                                                                                       
{'loss': 0.8046, 'learning_rate': 0.0002890718562874251, 'epoch': 0.83}                                                                                                       
{'loss': 0.8426, 'learning_rate': 0.00028877245508982034, 'epoch': 0.84}                                                                                                      
{'loss': 0.927, 'learning_rate': 0.00028847305389221553, 'epoch': 0.85}                                                                                                       
{'loss': 1.186, 'learning_rate': 0.0002881736526946108, 'epoch': 0.86}                                                                                                        
  9%|███████████                                                                                                                      | 900/10520 [1:08:18<2:05:28,  1.28it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5410522818565369, 'eval_wer': 1.906112282046369, 'eval_cer': 0.24120774063412687, 'eval_runtime': 352.2672, 'eval_samples_per_second': 29.631, 'eval_steps_per_second': 3.705, 'epoch': 0.86}                                                                                                                                               
{'loss': 0.7304, 'learning_rate': 0.000287874251497006, 'epoch': 0.87}                                                                                                        
{'loss': 0.804, 'learning_rate': 0.00028757485029940117, 'epoch': 0.87}                                                                                                       
{'loss': 0.7963, 'learning_rate': 0.0002872754491017964, 'epoch': 0.88}                                                                                                       
{'loss': 0.8851, 'learning_rate': 0.0002869760479041916, 'epoch': 0.89}                                                                                                       
{'loss': 1.0802, 'learning_rate': 0.0002866766467065868, 'epoch': 0.9}                                                                                                        
{'loss': 0.6713, 'learning_rate': 0.000286377245508982, 'epoch': 0.91}                                                                                                        
{'loss': 0.7868, 'learning_rate': 0.00028607784431137725, 'epoch': 0.92}                                                                                                      
{'loss': 0.7835, 'learning_rate': 0.00028577844311377244, 'epoch': 0.93}                                                                                                      
{'loss': 0.8074, 'learning_rate': 0.00028547904191616764, 'epoch': 0.94}                                                                                                      
{'loss': 1.0976, 'learning_rate': 0.00028517964071856283, 'epoch': 0.95}                                                                                                      
 10%|████████████▏                                                                                                                   | 1000/10520 [1:16:30<2:06:50,  1.25it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5214154124259949, 'eval_wer': 1.8937535926422686, 'eval_cer': 0.23209295484630094, 'eval_runtime': 353.9954, 'eval_samples_per_second': 29.486, 'eval_steps_per_second': 3.686, 'epoch': 0.95}                                                                                                                                              
{'loss': 0.7513, 'learning_rate': 0.000284880239520958, 'epoch': 0.96}                                                                                                        
{'loss': 0.7212, 'learning_rate': 0.00028458083832335327, 'epoch': 0.97}                                                                                                      
{'loss': 0.7952, 'learning_rate': 0.00028428143712574846, 'epoch': 0.98}                                                                                                      
{'loss': 0.8567, 'learning_rate': 0.00028398203592814366, 'epoch': 0.99}                                                                                                      
{'loss': 1.1251, 'learning_rate': 0.0002836826347305389, 'epoch': 1.0}                                                                                                        
 10%|████████████▊                                                                                                                   | 1052/10520 [1:23:37<2:45:38,  1.05s/it]Saving model checkpoint to ./20_band/checkpoint-1052
Configuration saved in ./20_band/checkpoint-1052/config.json
Model weights saved in ./20_band/checkpoint-1052/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-1052/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Traceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1749, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2508, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2540, in compute_loss
    outputs = model(**inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1677, in forward
    outputs = self.wav2vec2(
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1297, in forward
    extract_features = self.feature_extractor(input_values)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 456, in forward
    hidden_states = conv_layer(hidden_states)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 331, in forward
    hidden_states = self.layer_norm(hidden_states)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 190, in forward
    return F.layer_norm(
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 15.75 GiB total capacity; 10.12 GiB already allocated; 2.14 GiB free; 12.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 10%|████████████▋                                                                                                                  | 1052/10520 [1:23:41<12:33:14,  4.77s/it]
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading datasets... -----------------
----------------- Loading datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:248: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'project_q.bias', 'project_hid.weight', 'quantizer.weight_proj.weight', 'quantizer.codevectors']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
Loading model from ./20_band/checkpoint-1052.
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 33649
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 21030
  Number of trainable parameters = 311284919
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 0
  Continuing training from global step 1052
  Will skip the first 0 epochs then the first 2104 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.
Skipping the first batches:   0%|                                                                                                                    | 0/2104 [00:00<?, ?it/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
Skipping the first batches: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2104/2104 [03:50<00:00,  9.13it/s]
{'loss': 0.6609, 'learning_rate': 0.0002918899171943497, 'epoch': 0.5}
{'loss': 0.7682, 'learning_rate': 0.0002917437895762299, 'epoch': 0.51}                                                                                                       
{'loss': 0.8017, 'learning_rate': 0.00029159766195811006, 'epoch': 0.51}                                                                                                      
{'loss': 0.8822, 'learning_rate': 0.00029145153433999025, 'epoch': 0.52}                                                                                                      
{'loss': 1.0103, 'learning_rate': 0.00029130540672187043, 'epoch': 0.52}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1100/21030 [04:26<2:39:10,  2.09it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5361487865447998, 'eval_wer': 1.901992718911669, 'eval_cer': 0.23990502805034353, 'eval_runtime': 350.9296, 'eval_samples_per_second': 29.744, 'eval_steps_per_second': 3.719, 'epoch': 0.52}                                                                                                                                               
{'loss': 0.6725, 'learning_rate': 0.00029115927910375056, 'epoch': 0.53}                                                                                                      
{'loss': 0.7339, 'learning_rate': 0.00029101315148563075, 'epoch': 0.53}                                                                                                      
{'loss': 0.7542, 'learning_rate': 0.00029086702386751093, 'epoch': 0.54}                                                                                                      
{'loss': 0.8237, 'learning_rate': 0.0002907208962493911, 'epoch': 0.54}                                                                                                       
{'loss': 0.9832, 'learning_rate': 0.00029057476863127125, 'epoch': 0.55}                                                                                                      
{'loss': 0.7207, 'learning_rate': 0.0002904286410131515, 'epoch': 0.55}                                                                                                       
{'loss': 0.767, 'learning_rate': 0.00029028251339503167, 'epoch': 0.56}                                                                                                       
{'loss': 0.772, 'learning_rate': 0.0002901363857769118, 'epoch': 0.56}                                                                                                        
{'loss': 0.8421, 'learning_rate': 0.000289990258158792, 'epoch': 0.57}                                                                                                        
{'loss': 1.0126, 'learning_rate': 0.00028984413054067217, 'epoch': 0.57}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1200/21030 [11:35<2:38:43,  2.08it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5621633529663086, 'eval_wer': 1.901226288561027, 'eval_cer': 0.24493097724455276, 'eval_runtime': 350.5979, 'eval_samples_per_second': 29.772, 'eval_steps_per_second': 3.722, 'epoch': 0.57}                                                                                                                                               
{'loss': 0.716, 'learning_rate': 0.00028969800292255235, 'epoch': 0.58}                                                                                                       
{'loss': 0.696, 'learning_rate': 0.0002895518753044325, 'epoch': 0.58}                                                                                                        
{'loss': 0.7038, 'learning_rate': 0.00028940574768631267, 'epoch': 0.58}                                                                                                      
{'loss': 0.7494, 'learning_rate': 0.00028925962006819285, 'epoch': 0.59}                                                                                                      
{'loss': 0.9226, 'learning_rate': 0.00028911349245007304, 'epoch': 0.59}                                                                                                      
{'loss': 0.6602, 'learning_rate': 0.0002889673648319532, 'epoch': 0.6}                                                                                                        
{'loss': 0.6791, 'learning_rate': 0.0002888212372138334, 'epoch': 0.6}                                                                                                        
{'loss': 0.7891, 'learning_rate': 0.00028867510959571354, 'epoch': 0.61}                                                                                                      
{'loss': 0.8214, 'learning_rate': 0.0002885289819775937, 'epoch': 0.61}                                                                                                       
{'loss': 0.9937, 'learning_rate': 0.0002883828543594739, 'epoch': 0.62}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1300/21030 [18:44<2:37:23,  2.09it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.4920240044593811, 'eval_wer': 1.8794788273615635, 'eval_cer': 0.2253314563065997, 'eval_runtime': 350.4781, 'eval_samples_per_second': 29.782, 'eval_steps_per_second': 3.723, 'epoch': 0.62}                                                                                                                                               
{'loss': 0.6221, 'learning_rate': 0.0002882367267413541, 'epoch': 0.62}                                                                                                       
{'loss': 0.761, 'learning_rate': 0.0002880905991232343, 'epoch': 0.63}                                                                                                        
{'loss': 0.6991, 'learning_rate': 0.00028794447150511446, 'epoch': 0.63}                                                                                                      
{'loss': 0.7767, 'learning_rate': 0.00028779834388699464, 'epoch': 0.64}                                                                                                      
{'loss': 0.9282, 'learning_rate': 0.0002876522162688748, 'epoch': 0.64}                                                                                                       
{'loss': 0.5925, 'learning_rate': 0.00028750608865075496, 'epoch': 0.65}                                                                                                      
{'loss': 0.6254, 'learning_rate': 0.00028735996103263514, 'epoch': 0.65}                                                                                                      
{'loss': 0.7668, 'learning_rate': 0.0002872138334145153, 'epoch': 0.66}                                                                                                       
{'loss': 0.7383, 'learning_rate': 0.0002870677057963955, 'epoch': 0.66}                                                                                                       
{'loss': 0.9173, 'learning_rate': 0.0002869215781782757, 'epoch': 0.67}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1400/21030 [25:53<2:37:56,  2.07it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.49545928835868835, 'eval_wer': 1.8763173021651658, 'eval_cer': 0.22774777803458493, 'eval_runtime': 352.1107, 'eval_samples_per_second': 29.644, 'eval_steps_per_second': 3.706, 'epoch': 0.67}                                                                                                                                             
{'loss': 0.5706, 'learning_rate': 0.0002867754505601558, 'epoch': 0.67}                                                                                                       
{'loss': 0.6295, 'learning_rate': 0.000286629322942036, 'epoch': 0.68}                                                                                                        
{'loss': 0.7122, 'learning_rate': 0.0002864831953239162, 'epoch': 0.68}                                                                                                       
{'loss': 0.8023, 'learning_rate': 0.0002863370677057964, 'epoch': 0.68}                                                                                                       
{'loss': 1.0048, 'learning_rate': 0.0002861909400876765, 'epoch': 0.69}                                                                                                       
{'loss': 0.6118, 'learning_rate': 0.0002860448124695567, 'epoch': 0.69}                                                                                                       
{'loss': 0.689, 'learning_rate': 0.00028589868485143693, 'epoch': 0.7}                                                                                                        
{'loss': 0.6913, 'learning_rate': 0.00028575255723331706, 'epoch': 0.7}                                                                                                       
{'loss': 0.7285, 'learning_rate': 0.00028560642961519725, 'epoch': 0.71}                                                                                                      
{'loss': 1.0193, 'learning_rate': 0.00028546030199707743, 'epoch': 0.71}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1500/21030 [33:04<2:31:19,  2.15it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.46061035990715027, 'eval_wer': 1.8773711438972984, 'eval_cer': 0.22464648162544912, 'eval_runtime': 353.4033, 'eval_samples_per_second': 29.536, 'eval_steps_per_second': 3.693, 'epoch': 0.71}                                                                                                                                             
{'loss': 0.7475, 'learning_rate': 0.0002853141743789576, 'epoch': 0.72}                                                                                                       
{'loss': 0.6531, 'learning_rate': 0.00028516804676083775, 'epoch': 0.72}                                                                                                      
{'loss': 0.6969, 'learning_rate': 0.00028502191914271793, 'epoch': 0.73}                                                                                                      
{'loss': 0.7137, 'learning_rate': 0.0002848757915245981, 'epoch': 0.73}                                                                                                       
{'loss': 1.0379, 'learning_rate': 0.0002847296639064783, 'epoch': 0.74}                                                                                                       
{'loss': 0.5869, 'learning_rate': 0.0002845835362883585, 'epoch': 0.74}                                                                                                       
{'loss': 0.5946, 'learning_rate': 0.00028443740867023867, 'epoch': 0.75}                                                                                                      
{'loss': 0.7258, 'learning_rate': 0.0002842912810521188, 'epoch': 0.75}                                                                                                       
{'loss': 0.7441, 'learning_rate': 0.000284145153433999, 'epoch': 0.76}                                                                                                        
{'loss': 0.9183, 'learning_rate': 0.00028399902581587917, 'epoch': 0.76}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1600/21030 [40:16<2:38:35,  2.04it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.4778726100921631, 'eval_wer': 1.8748802452577122, 'eval_cer': 0.22238144264912907, 'eval_runtime': 356.3924, 'eval_samples_per_second': 29.288, 'eval_steps_per_second': 3.662, 'epoch': 0.76}                                                                                                                                              
{'loss': 0.5097, 'learning_rate': 0.00028385289819775935, 'epoch': 0.77}                                                                                                      
{'loss': 0.6461, 'learning_rate': 0.00028370677057963954, 'epoch': 0.77}                                                                                                      
{'loss': 0.6729, 'learning_rate': 0.0002835606429615197, 'epoch': 0.77}                                                                                                       
{'loss': 0.7185, 'learning_rate': 0.0002834145153433999, 'epoch': 0.78}                                                                                                       
{'loss': 1.0054, 'learning_rate': 0.00028326838772528004, 'epoch': 0.78}                                                                                                      
{'loss': 0.5595, 'learning_rate': 0.0002831222601071602, 'epoch': 0.79}                                                                                                       
{'loss': 0.6125, 'learning_rate': 0.0002829761324890404, 'epoch': 0.79}                                                                                                       
{'loss': 0.648, 'learning_rate': 0.0002828300048709206, 'epoch': 0.8}                                                                                                         
{'loss': 0.7072, 'learning_rate': 0.0002826838772528007, 'epoch': 0.8}                                                                                                        
{'loss': 0.8986, 'learning_rate': 0.00028253774963468096, 'epoch': 0.81}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1700/21030 [47:33<2:38:25,  2.03it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.43751832842826843, 'eval_wer': 1.8456600881394902, 'eval_cer': 0.21662009118988088, 'eval_runtime': 350.72, 'eval_samples_per_second': 29.762, 'eval_steps_per_second': 3.721, 'epoch': 0.81}                                                                                                                                               
{'loss': 0.6377, 'learning_rate': 0.0002823916220165611, 'epoch': 0.81}                                                                                                       
{'loss': 0.6055, 'learning_rate': 0.0002822454943984413, 'epoch': 0.82}                                                                                                       
{'loss': 0.6517, 'learning_rate': 0.00028209936678032146, 'epoch': 0.82}                                                                                                      
{'loss': 0.7301, 'learning_rate': 0.00028195323916220164, 'epoch': 0.83}                                                                                                      
{'loss': 0.9707, 'learning_rate': 0.00028180711154408177, 'epoch': 0.83}                                                                                                      
{'loss': 0.5815, 'learning_rate': 0.00028166098392596196, 'epoch': 0.84}                                                                                                      
{'loss': 0.6381, 'learning_rate': 0.0002815148563078422, 'epoch': 0.84}                                                                                                       
{'loss': 0.6653, 'learning_rate': 0.0002813687286897223, 'epoch': 0.85}                                                                                                       
{'loss': 0.7138, 'learning_rate': 0.0002812226010716025, 'epoch': 0.85}                                                                                                       
{'loss': 0.9084, 'learning_rate': 0.0002810764734534827, 'epoch': 0.86}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                         | 1800/21030 [54:43<2:33:22,  2.09it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.44642284512519836, 'eval_wer': 1.858114581337421, 'eval_cer': 0.21153110751581114, 'eval_runtime': 350.4305, 'eval_samples_per_second': 29.786, 'eval_steps_per_second': 3.724, 'epoch': 0.86}                                                                                                                                              
{'loss': 0.5396, 'learning_rate': 0.0002809303458353629, 'epoch': 0.86}                                                                                                       
{'loss': 0.5983, 'learning_rate': 0.000280784218217243, 'epoch': 0.87}                                                                                                        
{'loss': 0.6218, 'learning_rate': 0.0002806380905991232, 'epoch': 0.87}                                                                                                       
{'loss': 0.7748, 'learning_rate': 0.0002804919629810034, 'epoch': 0.87}                                                                                                       
{'loss': 0.8916, 'learning_rate': 0.00028034583536288356, 'epoch': 0.88}                                                                                                      
{'loss': 0.4992, 'learning_rate': 0.00028019970774476375, 'epoch': 0.88}                                                                                                      
{'loss': 0.6141, 'learning_rate': 0.00028005358012664393, 'epoch': 0.89}                                                                                                      
{'loss': 0.7228, 'learning_rate': 0.00027990745250852406, 'epoch': 0.89}                                                                                                      
{'loss': 0.7363, 'learning_rate': 0.00027976132489040425, 'epoch': 0.9}                                                                                                       
{'loss': 0.9018, 'learning_rate': 0.00027961519727228443, 'epoch': 0.9}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 1900/21030 [1:01:51<2:32:26,  2.09it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.42327359318733215, 'eval_wer': 1.8318643418279363, 'eval_cer': 0.20758514907654488, 'eval_runtime': 352.098, 'eval_samples_per_second': 29.645, 'eval_steps_per_second': 3.706, 'epoch': 0.9}                                                                                                                                               
{'loss': 0.5492, 'learning_rate': 0.0002794690696541646, 'epoch': 0.91}                                                                                                       
{'loss': 0.5568, 'learning_rate': 0.0002793229420360448, 'epoch': 0.91}                                                                                                       
{'loss': 0.6372, 'learning_rate': 0.000279176814417925, 'epoch': 0.92}                                                                                                        
{'loss': 0.7412, 'learning_rate': 0.00027903068679980517, 'epoch': 0.92}                                                                                                      
{'loss': 0.9587, 'learning_rate': 0.0002788845591816853, 'epoch': 0.93}                                                                                                       
{'loss': 0.5572, 'learning_rate': 0.0002787384315635655, 'epoch': 0.93}                                                                                                       
{'loss': 0.5782, 'learning_rate': 0.00027859230394544567, 'epoch': 0.94}                                                                                                      
{'loss': 0.6588, 'learning_rate': 0.00027844617632732585, 'epoch': 0.94}                                                                                                      
{'loss': 0.6447, 'learning_rate': 0.000278300048709206, 'epoch': 0.95}                                                                                                        
{'loss': 0.8802, 'learning_rate': 0.0002781539210910862, 'epoch': 0.95}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2000/21030 [1:09:01<2:30:57,  2.10it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.44334834814071655, 'eval_wer': 1.8462349109024718, 'eval_cer': 0.21318681318681318, 'eval_runtime': 352.198, 'eval_samples_per_second': 29.637, 'eval_steps_per_second': 3.705, 'epoch': 0.95}                                                                                                                                              
{'loss': 0.549, 'learning_rate': 0.00027800779347296635, 'epoch': 0.96}                                                                                                       
{'loss': 0.6072, 'learning_rate': 0.00027786166585484654, 'epoch': 0.96}                                                                                                      
{'loss': 0.5714, 'learning_rate': 0.0002777155382367267, 'epoch': 0.97}                                                                                                       
{'loss': 0.6715, 'learning_rate': 0.0002775694106186069, 'epoch': 0.97}                                                                                                       
{'loss': 0.9703, 'learning_rate': 0.0002774232830004871, 'epoch': 0.97}                                                                                                       
{'loss': 0.5626, 'learning_rate': 0.0002772771553823672, 'epoch': 0.98}                                                                                                       
{'loss': 0.6117, 'learning_rate': 0.0002771310277642474, 'epoch': 0.98}                                                                                                       
{'loss': 0.6385, 'learning_rate': 0.0002769849001461276, 'epoch': 0.99}                                                                                                       
{'loss': 0.7452, 'learning_rate': 0.00027683877252800777, 'epoch': 0.99}                                                                                                      
{'loss': 0.8675, 'learning_rate': 0.00027669264490988796, 'epoch': 1.0}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2100/21030 [1:16:11<2:27:43,  2.14it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.45193755626678467, 'eval_wer': 1.8547614485533628, 'eval_cer': 0.21546025676044797, 'eval_runtime': 352.9634, 'eval_samples_per_second': 29.572, 'eval_steps_per_second': 3.697, 'epoch': 1.0}                                                                                                                                              
                                                                                                                                                                              
Saving model checkpoint to ./20_band/checkpoint-2104                                                                               | 2104/21030 [1:22:10<198:30:19, 37.76s/it]
Configuration saved in ./20_band/checkpoint-2104/config.json
Model weights saved in ./20_band/checkpoint-2104/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-2104/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.646, 'learning_rate': 0.00027654651729176814, 'epoch': 1.0}
{'loss': 0.5123, 'learning_rate': 0.00027640038967364827, 'epoch': 1.01}                                                                                                      
{'loss': 0.5976, 'learning_rate': 0.00027625426205552846, 'epoch': 1.01}                                                                                                      
{'loss': 0.6878, 'learning_rate': 0.00027610813443740864, 'epoch': 1.02}                                                                                                      
{'loss': 0.7326, 'learning_rate': 0.0002759620068192888, 'epoch': 1.02}                                                                                                       
{'loss': 0.6772, 'learning_rate': 0.000275815879201169, 'epoch': 1.03}                                                                                                        
{'loss': 0.5757, 'learning_rate': 0.0002756697515830492, 'epoch': 1.03}                                                                                                       
{'loss': 0.624, 'learning_rate': 0.0002755236239649294, 'epoch': 1.04}                                                                                                        
{'loss': 0.623, 'learning_rate': 0.0002753774963468095, 'epoch': 1.04}                                                                                                        
{'loss': 0.7147, 'learning_rate': 0.0002752313687286897, 'epoch': 1.05}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2200/21030 [1:23:28<2:47:52,  1.87it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.42590269446372986, 'eval_wer': 1.8343552404675225, 'eval_cer': 0.20845922719727691, 'eval_runtime': 349.8524, 'eval_samples_per_second': 29.835, 'eval_steps_per_second': 3.73, 'epoch': 1.05}                                                                                                                                              
{'loss': 0.6892, 'learning_rate': 0.0002750852411105699, 'epoch': 1.05}                                                                                                       
{'loss': 0.5493, 'learning_rate': 0.00027493911349245006, 'epoch': 1.06}                                                                                                      
{'loss': 0.5815, 'learning_rate': 0.00027479298587433025, 'epoch': 1.06}                                                                                                      
{'loss': 0.5754, 'learning_rate': 0.00027464685825621043, 'epoch': 1.07}                                                                                                      
{'loss': 0.782, 'learning_rate': 0.00027450073063809056, 'epoch': 1.07}                                                                                                       
{'loss': 0.6796, 'learning_rate': 0.00027435460301997075, 'epoch': 1.07}                                                                                                      
{'loss': 0.5914, 'learning_rate': 0.00027420847540185093, 'epoch': 1.08}                                                                                                      
{'loss': 0.6061, 'learning_rate': 0.0002740623477837311, 'epoch': 1.08}                                                                                                       
{'loss': 0.6198, 'learning_rate': 0.00027391622016561124, 'epoch': 1.09}                                                                                                      
{'loss': 0.7599, 'learning_rate': 0.00027377009254749143, 'epoch': 1.09}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2300/21030 [1:30:36<2:45:42,  1.88it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.41420257091522217, 'eval_wer': 1.8204636903621383, 'eval_cer': 0.20607652385855063, 'eval_runtime': 355.0342, 'eval_samples_per_second': 29.4, 'eval_steps_per_second': 3.676, 'epoch': 1.09}                                                                                                                                               
{'loss': 0.7251, 'learning_rate': 0.00027362396492937167, 'epoch': 1.1}                                                                                                       
{'loss': 0.493, 'learning_rate': 0.0002734778373112518, 'epoch': 1.1}                                                                                                         
{'loss': 0.5305, 'learning_rate': 0.000273331709693132, 'epoch': 1.11}                                                                                                        
{'loss': 0.6156, 'learning_rate': 0.00027318558207501217, 'epoch': 1.11}                                                                                                      
{'loss': 0.7154, 'learning_rate': 0.00027303945445689235, 'epoch': 1.12}                                                                                                      
{'loss': 0.6338, 'learning_rate': 0.0002728933268387725, 'epoch': 1.12}                                                                                                       
{'loss': 0.4947, 'learning_rate': 0.00027274719922065267, 'epoch': 1.13}                                                                                                      
{'loss': 0.6273, 'learning_rate': 0.00027260107160253285, 'epoch': 1.13}                                                                                                      
{'loss': 0.6055, 'learning_rate': 0.00027245494398441303, 'epoch': 1.14}                                                                                                      
{'loss': 0.7089, 'learning_rate': 0.0002723088163662932, 'epoch': 1.14}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2400/21030 [1:37:49<2:44:57,  1.88it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.3978416323661804, 'eval_wer': 1.8149070703199848, 'eval_cer': 0.20114722753346082, 'eval_runtime': 353.8357, 'eval_samples_per_second': 29.5, 'eval_steps_per_second': 3.688, 'epoch': 1.14}                                                                                                                                                
{'loss': 0.6502, 'learning_rate': 0.0002721626887481734, 'epoch': 1.15}                                                                                                       
{'loss': 0.5562, 'learning_rate': 0.00027201656113005353, 'epoch': 1.15}                                                                                                      
{'loss': 0.5165, 'learning_rate': 0.0002718704335119337, 'epoch': 1.16}                                                                                                       
{'loss': 0.6277, 'learning_rate': 0.0002717243058938139, 'epoch': 1.16}                                                                                                       
{'loss': 0.7306, 'learning_rate': 0.0002715781782756941, 'epoch': 1.16}                                                                                                       
{'loss': 0.5967, 'learning_rate': 0.00027143205065757427, 'epoch': 1.17}                                                                                                      
{'loss': 0.4825, 'learning_rate': 0.00027128592303945446, 'epoch': 1.17}                                                                                                      
{'loss': 0.5977, 'learning_rate': 0.00027113979542133464, 'epoch': 1.18}                                                                                                      
{'loss': 0.6552, 'learning_rate': 0.00027099366780321477, 'epoch': 1.18}                                                                                                      
{'loss': 0.7556, 'learning_rate': 0.00027084754018509496, 'epoch': 1.19}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2500/21030 [1:45:02<2:45:43,  1.86it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.391976535320282, 'eval_wer': 1.8084882161333589, 'eval_cer': 0.2019792826676192, 'eval_runtime': 354.6601, 'eval_samples_per_second': 29.431, 'eval_steps_per_second': 3.68, 'epoch': 1.19}                                                                                                                                                 
{'loss': 0.6373, 'learning_rate': 0.00027070141256697514, 'epoch': 1.19}                                                                                                      
{'loss': 0.5184, 'learning_rate': 0.0002705552849488553, 'epoch': 1.2}                                                                                                        
{'loss': 0.5347, 'learning_rate': 0.00027040915733073545, 'epoch': 1.2}                                                                                                       
{'loss': 0.6206, 'learning_rate': 0.0002702630297126157, 'epoch': 1.21}                                                                                                       
{'loss': 0.6645, 'learning_rate': 0.0002701169020944958, 'epoch': 1.21}                                                                                                       
{'loss': 0.6489, 'learning_rate': 0.000269970774476376, 'epoch': 1.22}                                                                                                        
{'loss': 0.5178, 'learning_rate': 0.0002698246468582562, 'epoch': 1.22}                                                                                                       
{'loss': 0.5488, 'learning_rate': 0.0002696785192401364, 'epoch': 1.23}                                                                                                       
{'loss': 0.6236, 'learning_rate': 0.0002695323916220165, 'epoch': 1.23}                                                                                                       
{'loss': 0.8168, 'learning_rate': 0.0002693862640038967, 'epoch': 1.24}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2600/21030 [1:52:16<2:46:26,  1.85it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.4007056951522827, 'eval_wer': 1.8251580762598199, 'eval_cer': 0.20900552602273442, 'eval_runtime': 353.4426, 'eval_samples_per_second': 29.532, 'eval_steps_per_second': 3.692, 'epoch': 1.24}                                                                                                                                              
{'loss': 0.6603, 'learning_rate': 0.0002692401363857769, 'epoch': 1.24}                                                                                                       
{'loss': 0.5582, 'learning_rate': 0.00026909400876765706, 'epoch': 1.25}                                                                                                      
{'loss': 0.5874, 'learning_rate': 0.00026894788114953724, 'epoch': 1.25}                                                                                                      
{'loss': 0.578, 'learning_rate': 0.00026880175353141743, 'epoch': 1.26}                                                                                                       
{'loss': 0.7398, 'learning_rate': 0.0002686556259132976, 'epoch': 1.26}                                                                                                       
{'loss': 0.6522, 'learning_rate': 0.00026850949829517774, 'epoch': 1.26}                                                                                                      
{'loss': 0.5271, 'learning_rate': 0.00026836337067705793, 'epoch': 1.27}                                                                                                      
{'loss': 0.5469, 'learning_rate': 0.0002682172430589381, 'epoch': 1.27}                                                                                                       
{'loss': 0.6609, 'learning_rate': 0.0002680711154408183, 'epoch': 1.28}                                                                                                       
{'loss': 0.7036, 'learning_rate': 0.0002679249878226985, 'epoch': 1.28}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2700/21030 [1:59:27<2:42:59,  1.87it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.3810340166091919, 'eval_wer': 1.7893274573673117, 'eval_cer': 0.192137499212069, 'eval_runtime': 353.1731, 'eval_samples_per_second': 29.555, 'eval_steps_per_second': 3.695, 'epoch': 1.28}                                                                                                                                                
{'loss': 0.6135, 'learning_rate': 0.00026777886020457867, 'epoch': 1.29}                                                                                                      
{'loss': 0.497, 'learning_rate': 0.0002676327325864588, 'epoch': 1.29}                                                                                                        
{'loss': 0.5412, 'learning_rate': 0.000267486604968339, 'epoch': 1.3}                                                                                                         
{'loss': 0.5708, 'learning_rate': 0.00026734047735021917, 'epoch': 1.3}                                                                                                       
{'loss': 0.7295, 'learning_rate': 0.00026719434973209935, 'epoch': 1.31}                                                                                                      
{'loss': 0.6923, 'learning_rate': 0.0002670482221139795, 'epoch': 1.31}                                                                                                       
{'loss': 0.5599, 'learning_rate': 0.0002669020944958597, 'epoch': 1.32}                                                                                                       
{'loss': 0.5183, 'learning_rate': 0.0002667559668777399, 'epoch': 1.32}                                                                                                       
{'loss': 0.6588, 'learning_rate': 0.00026660983925962003, 'epoch': 1.33}                                                                                                      
{'loss': 0.7895, 'learning_rate': 0.0002664637116415002, 'epoch': 1.33}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2800/21030 [2:06:39<2:41:54,  1.88it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.38527756929397583, 'eval_wer': 1.8014945391837516, 'eval_cer': 0.19670119555396803, 'eval_runtime': 353.6636, 'eval_samples_per_second': 29.514, 'eval_steps_per_second': 3.69, 'epoch': 1.33}                                                                                                                                              
{'loss': 0.5972, 'learning_rate': 0.0002663175840233804, 'epoch': 1.34}                                                                                                       
{'loss': 0.5288, 'learning_rate': 0.0002661714564052606, 'epoch': 1.34}                                                                                                       
{'loss': 0.5636, 'learning_rate': 0.0002660253287871407, 'epoch': 1.35}                                                                                                       
{'loss': 0.6054, 'learning_rate': 0.0002658792011690209, 'epoch': 1.35}                                                                                                       
{'loss': 0.7853, 'learning_rate': 0.0002657330735509011, 'epoch': 1.36}                                                                                                       
{'loss': 0.6508, 'learning_rate': 0.00026558694593278127, 'epoch': 1.36}                                                                                                      
{'loss': 0.5213, 'learning_rate': 0.00026544081831466145, 'epoch': 1.36}                                                                                                      
{'loss': 0.5438, 'learning_rate': 0.00026529469069654164, 'epoch': 1.37}                                                                                                      
{'loss': 0.568, 'learning_rate': 0.00026514856307842177, 'epoch': 1.37}                                                                                                       
{'loss': 0.6724, 'learning_rate': 0.00026500243546030195, 'epoch': 1.38}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 2900/21030 [2:13:52<2:42:07,  1.86it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.376737505197525, 'eval_wer': 1.7923931787698792, 'eval_cer': 0.19331414283613135, 'eval_runtime': 355.3601, 'eval_samples_per_second': 29.373, 'eval_steps_per_second': 3.672, 'epoch': 1.38}                                                                                                                                               
{'loss': 0.5934, 'learning_rate': 0.00026485630784218214, 'epoch': 1.38}                                                                                                      
{'loss': 0.5022, 'learning_rate': 0.0002647101802240623, 'epoch': 1.39}                                                                                                       
{'loss': 0.6129, 'learning_rate': 0.0002645640526059425, 'epoch': 1.39}                                                                                                       
{'loss': 0.607, 'learning_rate': 0.0002644179249878227, 'epoch': 1.4}                                                                                                         
{'loss': 0.6124, 'learning_rate': 0.0002642717973697029, 'epoch': 1.4}                                                                                                        
{'loss': 0.544, 'learning_rate': 0.000264125669751583, 'epoch': 1.41}                                                                                                         
{'loss': 0.5398, 'learning_rate': 0.0002639795421334632, 'epoch': 1.41}                                                                                                       
{'loss': 0.5833, 'learning_rate': 0.0002638334145153434, 'epoch': 1.42}                                                                                                       
{'loss': 0.5887, 'learning_rate': 0.00026368728689722356, 'epoch': 1.42}                                                                                                      
{'loss': 0.7384, 'learning_rate': 0.00026354115927910374, 'epoch': 1.43}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3000/21030 [2:21:08<2:42:58,  1.84it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.37743452191352844, 'eval_wer': 1.7984288177811842, 'eval_cer': 0.19634400016809195, 'eval_runtime': 355.3381, 'eval_samples_per_second': 29.375, 'eval_steps_per_second': 3.673, 'epoch': 1.43}                                                                                                                                             
{'loss': 0.5955, 'learning_rate': 0.00026339503166098393, 'epoch': 1.43}                                                                                                      
{'loss': 0.5257, 'learning_rate': 0.00026324890404286406, 'epoch': 1.44}                                                                                                      
{'loss': 0.518, 'learning_rate': 0.00026310277642474424, 'epoch': 1.44}                                                                                                       
{'loss': 0.5557, 'learning_rate': 0.00026295664880662443, 'epoch': 1.45}                                                                                                      
{'loss': 0.6667, 'learning_rate': 0.0002628105211885046, 'epoch': 1.45}                                                                                                       
{'loss': 0.5803, 'learning_rate': 0.00026266439357038474, 'epoch': 1.45}                                                                                                      
{'loss': 0.518, 'learning_rate': 0.0002625182659522649, 'epoch': 1.46}                                                                                                        
{'loss': 0.5255, 'learning_rate': 0.00026237213833414517, 'epoch': 1.46}                                                                                                      
{'loss': 0.5701, 'learning_rate': 0.0002622260107160253, 'epoch': 1.47}                                                                                                       
{'loss': 0.6897, 'learning_rate': 0.0002620798830979055, 'epoch': 1.47}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3100/21030 [2:28:24<2:38:13,  1.89it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.39124828577041626, 'eval_wer': 1.79670434949224, 'eval_cer': 0.1965331036076734, 'eval_runtime': 353.5537, 'eval_samples_per_second': 29.523, 'eval_steps_per_second': 3.691, 'epoch': 1.47}                                                                                                                                                
{'loss': 0.6957, 'learning_rate': 0.00026193375547978566, 'epoch': 1.48}                                                                                                      
{'loss': 0.4918, 'learning_rate': 0.00026178762786166585, 'epoch': 1.48}                                                                                                      
{'loss': 0.5861, 'learning_rate': 0.000261641500243546, 'epoch': 1.49}                                                                                                        
{'loss': 0.604, 'learning_rate': 0.00026149537262542616, 'epoch': 1.49}                                                                                                       
{'loss': 0.6352, 'learning_rate': 0.00026134924500730635, 'epoch': 1.5}                                                                                                       
                                                                                                                                                                              
Saving model checkpoint to ./20_band/checkpoint-3156                                                                                 | 3156/21030 [2:35:03<5:56:08,  1.20s/it]
Configuration saved in ./20_band/checkpoint-3156/config.json
Model weights saved in ./20_band/checkpoint-3156/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-3156/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6067, 'learning_rate': 0.00026120311738918653, 'epoch': 1.5}
{'loss': 0.5019, 'learning_rate': 0.0002610569897710667, 'epoch': 1.51}                                                                                                       
{'loss': 0.749, 'learning_rate': 0.0002609108621529469, 'epoch': 1.51}                                                                                                        
{'loss': 0.6294, 'learning_rate': 0.00026076473453482703, 'epoch': 1.52}                                                                                                      
{'loss': 0.7238, 'learning_rate': 0.0002606186069167072, 'epoch': 1.52}                                                                                                       
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3200/21030 [2:35:39<2:41:39,  1.84it/s]
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.3616074323654175, 'eval_wer': 1.7860701283770837, 'eval_cer': 0.19002794528607148, 'eval_runtime': 353.2426, 'eval_samples_per_second': 29.549, 'eval_steps_per_second': 3.694, 'epoch': 1.52}                                                                                                                                              
{'loss': 0.5571, 'learning_rate': 0.0002604724792985874, 'epoch': 1.53}                                                                                                       
{'loss': 0.4998, 'learning_rate': 0.0002603263516804676, 'epoch': 1.53}                                                                                                       
{'loss': 0.5299, 'learning_rate': 0.00026018022406234777, 'epoch': 1.54}                                                                                                      
{'loss': 0.5232, 'learning_rate': 0.00026003409644422795, 'epoch': 1.54}                                                                                                      
{'loss': 0.6864, 'learning_rate': 0.00025988796882610814, 'epoch': 1.55}                                                                                                      
{'loss': 0.6361, 'learning_rate': 0.00025974184120798827, 'epoch': 1.55}                                                                                                      
{'loss': 0.5143, 'learning_rate': 0.00025959571358986845, 'epoch': 1.55}                                                                                                      
{'loss': 0.5366, 'learning_rate': 0.00025944958597174864, 'epoch': 1.56}                                                                                                      
{'loss': 0.6115, 'learning_rate': 0.0002593034583536288, 'epoch': 1.56}                                                                                                       
{'loss': 0.7129, 'learning_rate': 0.00025915733073550895, 'epoch': 1.57}                                                                                                      
                                                                                                                                                                              
***** Running Evaluation *****                                                                                                       | 3300/21030 [2:42:51<2:40:44,  1.84it/s]
  Num examples = 10438
  Batch size = 8
 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 1224/1305 [05:31<00:27,  2.91it/s]^CTraceback (most recent call last):
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 318, in <module>
    trainer.train(resume_from_checkpoint=True)  # to continue training from the last checkpoint
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2964, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 61, in fetch
    return self.collate_fn(data)
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 220, in __call__
    batch = self.processor.pad(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py", line 122, in pad
    input_features = self.feature_extractor.pad(input_features, *args, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/feature_extraction_sequence_utils.py", line 178, in pad
    processed_features[key] = [to_numpy(v) for v in value]
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/feature_extraction_sequence_utils.py", line 178, in <listcomp>
    processed_features[key] = [to_numpy(v) for v in value]
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py", line 179, in to_numpy
    return np.array(obj)
KeyboardInterrupt
 16%|███████████████████▉                                                                                                           | 3300/21030 [2:48:24<15:04:46,  3.06s/it]
                                                                                                                                                                              
(base) or@anidjar:~/Desktop/wav2vec2$ cd inference/
(base) or@anidjar:~/Desktop/wav2vec2/inference$ python3 inference_script.py 
/home/or/Desktop/wav2vec2/inference/inference_script.py:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("cer")
----------------- Loading Datasets... -----------------
Using custom data configuration default-c30e8ea0b0df9377
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-c30e8ea0b0df9377/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9619.96it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3315.66it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-c30e8ea0b0df9377/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 739.74it/s]
----------------- Loading Datasets complete. ----------


----------------- Casting features... -----------------
Casting the dataset:  50%|███████████████████████████████████████████████████████████                                                           | 1/2 [00:00<00:00, 18.64ba/s]
----------------- Casting features complete. -----------


----------------- Removing columns... -----------------
----------------- Removing columns complete. -----------


----------------- Loading audio from path... -----------------
----------------- Loading audio from path complete. -----------


---------------- Loading processor and model... ---------------------
---------------- Loading processor and model complete. ---------------------


Parameter 'function'=<function speech_file_to_array_fn at 0x7f56ca470790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10438/10438 [00:44<00:00, 234.84ex/s]

---------------- Inferencing... ---------------------
---------------- Inferencing complete. ---------------------


0----------------------------------------------------------------------------------------------------
Reference: إبنك بطل.
Prediction: ابنكبتر
Result: 0.5555555555555556
1----------------------------------------------------------------------------------------------------
Reference: الواعظ الأمرد هذا الذي
Prediction: اللاعزالأمردهذاالذي
Result: 0.22727272727272727
2----------------------------------------------------------------------------------------------------
Reference: سمح له هذا بالتخصص في البرونز الصغير، الذي يتم إنتاجه بشكل رئيسي ومربح للتصدير.
Prediction: سمحلههازبالبقصأصفبرنزسريرلزيةمعلكبهشكيافاإسياوولبحللتصدير
Result: 0.5443037974683544
3----------------------------------------------------------------------------------------------------
Reference: ألديك قلم ؟
Prediction: ألديكقلم
Result: 0.2727272727272727
4----------------------------------------------------------------------------------------------------
Reference: يا نديمي قسم بي الى الصهباء
Prediction: يامديميهقسبيإليعالصحباء
Result: 0.37037037037037035
5----------------------------------------------------------------------------------------------------
Reference: إنك تكبر المشكلة.
Prediction: إنكتكبرالمشكلة
Result: 0.17647058823529413
6----------------------------------------------------------------------------------------------------
Reference: يرغب أن يلتقي بك.
Prediction: يرغبأنيلتقيبك
Result: 0.23529411764705882
7----------------------------------------------------------------------------------------------------
Reference: إنهم لا يعرفون لماذا حتى.
Prediction: إنهملايعرفونلماذاحتا
Result: 0.24
8----------------------------------------------------------------------------------------------------
Reference: سيسعدني مساعدتك أي وقت تحب.
Prediction: سيسعدلمساعةسكأيواقتحب
Result: 0.4074074074074074
9----------------------------------------------------------------------------------------------------
Reference: مات الملك!
Prediction: لاكمامدك
Result: 0.6
10----------------------------------------------------------------------------------------------------
Reference: أَحَبُّ نظريّة علمية إليّ هي أن حلقات زحل مكونة بالكامل من الأمتعة المفقودة.
Prediction: أحبنظريةعلميةإليهيأنحلقتزحالمكونبالكاملمنالأمتعالمفقودة
Result: 0.3026315789473684
11----------------------------------------------------------------------------------------------------
Reference: سأشتري له قلماً.
Prediction: شأشتريلهقلما
Result: 0.3125
12----------------------------------------------------------------------------------------------------
Reference: إن جمال ينتظر سارا.
Prediction: إنكناليينتظيرسعا
Result: 0.47368421052631576
13----------------------------------------------------------------------------------------------------
Reference: أين المشكلة ؟
Prediction: أينالمشكل
Result: 0.3076923076923077
14----------------------------------------------------------------------------------------------------
Reference: حياة المريض كانت بخطر
Prediction: هاعتالمريضكانتبخطر
Result: 0.2857142857142857
15----------------------------------------------------------------------------------------------------
Reference: وَلِلَّهِ يَسْجُدُ مَا فِي السَّمَاوَاتِ وَمَا فِي الْأَرْضِ مِنْ دَابَّةٍ وَالْمَلَائِكَةُ وَهُمْ لَا يَسْتَكْبِرُونَ
Prediction: وللهيسلدمافيالسماواتومافلأرضمندابةوالملالكتوهملايستكبرون
Result: 0.5508474576271186
16----------------------------------------------------------------------------------------------------
Reference: إن الحق معك
Prediction: ألاالأطبعت
Result: 0.7272727272727273
17----------------------------------------------------------------------------------------------------
Reference: أنا من أهوى ومن أهوى أنا
Prediction: ألامالأهوىوملأهوىألا
Result: 0.375
18----------------------------------------------------------------------------------------------------
Reference: كم كربة ضاق صدري عن تحملها
Prediction: كمكربةلقتطرأنتحاملاا
Result: 0.5
19----------------------------------------------------------------------------------------------------
Reference: لي أربعون من السنين
Prediction: هيعبعونمنعسنين
Result: 0.42105263157894735
20----------------------------------------------------------------------------------------------------
Reference: أواجه صعوبةً في التحدث عن مشاعري.
Prediction: هوايلوسعوبكافيطحدثأنرشاعه
Result: 0.5757575757575758
21----------------------------------------------------------------------------------------------------
Reference: تقوم أمي بإعداد كعكة لأجل أبي.
Prediction: تكوممعذفكركتمأجبعدي
Result: 0.6666666666666666
22----------------------------------------------------------------------------------------------------
Reference: مدحت معاشرا عررا
Prediction: دحتمعاشرالمعرراء
Result: 0.3125
23----------------------------------------------------------------------------------------------------
Reference: لك الحمد كلا يجبر الشعب كسره
Prediction: لكدخندكليجبرالشعبكسه
Result: 0.39285714285714285
24----------------------------------------------------------------------------------------------------
Reference: وَأَمّا مَن أوتِيَ كِتابَهُ بِشِمالِهِ فَيَقولُ يا لَيتَني لَم أوتَ كِتابِيَه
Prediction: وأمامنأوتيكتابهبشمالهفيقولياليتنيلمأوتكتابيه
Result: 0.42857142857142855
25----------------------------------------------------------------------------------------------------
Reference: .يجب أن لا تلعب
Prediction: يجبأنلاترعب
Result: 0.3333333333333333
26----------------------------------------------------------------------------------------------------
Reference: نزع القريض إلى حمى نقاشه
Prediction: نزعالقريبإلىحمنقاشة
Result: 0.2916666666666667
27----------------------------------------------------------------------------------------------------
Reference: رويدك قد غررت وأنت حر
Prediction: هويدكقدقريكوأنيكحرون
Result: 0.5238095238095238
28----------------------------------------------------------------------------------------------------
Reference: يعز علي قبر بعد مهد
Prediction: هيعزعليقوربعدنه
Result: 0.42105263157894735
29----------------------------------------------------------------------------------------------------
Reference: تحدثت معها خلال ساعة.
Prediction: تحدثتمعهىخلالساعة
Result: 0.23809523809523808
30----------------------------------------------------------------------------------------------------
Reference: الحقيقة هي أنّك جبان.
Prediction: الحقيقهيةأنكجبهم
Result: 0.38095238095238093
31----------------------------------------------------------------------------------------------------
Reference: ذهب الناس فاستقلوا وصرنا
Prediction: ذهبالناسأستقلغصرنها
Result: 0.375
32----------------------------------------------------------------------------------------------------
Reference: يا أعز الناس عندي
Prediction: ياعزالناسعندي
Result: 0.23529411764705882
33----------------------------------------------------------------------------------------------------
Reference: أتى عدة مرات.
Prediction: أتىعدتمراته
Result: 0.3076923076923077
34----------------------------------------------------------------------------------------------------
Reference: شغلته أشغال عن الآرام
Prediction: شغرتحأشفاطعنالآرام
Result: 0.3333333333333333
35----------------------------------------------------------------------------------------------------
Reference: وَرَوَى حُمَيْدٌ عَنْ أَنَسٍ أَنَّ النَّبِيَّ صَلَّى اللَّهُ عَلَيْهِ وَسَلَّمَ قَالَ
Prediction: وروىحميدعنأنسأنالنبيصلىاللهعليهوسلمقالا
Result: 0.5529411764705883
36----------------------------------------------------------------------------------------------------
Reference: سيدي أنت لم أقل سيدي أنت
Prediction: سيذيعللمأقسيه
Result: 0.625
37----------------------------------------------------------------------------------------------------
Reference: لا يقولون بعد الآباء اكلوا حصرما واسنان الابناء ضرست.
Prediction: لايقولنبعدالآباءأكلحصرناوأسلنلأبناءدرصت
Result: 0.39622641509433965
38----------------------------------------------------------------------------------------------------
Reference: لقد عجلتني نظرتي بهواكا
Prediction: لقدعكلتنينترتهواك
Result: 0.34782608695652173
39----------------------------------------------------------------------------------------------------
Reference: التدخين عادة سيئة.
Prediction: اتذخينعلةسيء
Result: 0.5
40----------------------------------------------------------------------------------------------------
Reference: إنكم أولاد يافعين.
Prediction: إنكمأولاليافعين
Result: 0.2222222222222222
41----------------------------------------------------------------------------------------------------
Reference: .شرب القليل
Prediction: شربالقليل
Result: 0.18181818181818182
42----------------------------------------------------------------------------------------------------
Reference: من يدرّسك الفرنسية ؟
Prediction: منيدرسكالفرنسية
Result: 0.25
43----------------------------------------------------------------------------------------------------
Reference: من سيكذب لأجلك ، سيكذب عليك.
Prediction: منسيكثبلأنلكسيكثبعليك
Result: 0.35714285714285715
44----------------------------------------------------------------------------------------------------
Reference: متى تكون مشغولاً ؟
Prediction: متاتكونمشقولا
Result: 0.3888888888888889
45----------------------------------------------------------------------------------------------------
Reference: أرسل سامي رسائل حبّ لليلى.
Prediction: أرسلساميرساءكحبلليلى
Result: 0.3076923076923077
46----------------------------------------------------------------------------------------------------
Reference: وَجَعَلنا مِن بَينِ أَيديهِم سَدًّا وَمِن خَلفِهِم سَدًّا فَأَغشَيناهُم فَهُم لا يُبصِرونَ
Prediction: وجعلابينأيديهمتنذبنخلفهمسدكأغجينهمفهملايبصرون
Result: 0.5666666666666667
47----------------------------------------------------------------------------------------------------
Reference: استيقظت الساعة السادسة و النصف هذا الصباح.
Prediction: استيطرتالساعةالساراستوالنصفعذالصباح
Result: 0.3333333333333333
48----------------------------------------------------------------------------------------------------
Reference: سرق الدهر شبابي من يدي
Prediction: سرقتدحرالشبابمنيدي
Result: 0.4090909090909091
49----------------------------------------------------------------------------------------------------
Reference: واصلت ليلى السّباحة.
Prediction: رصلتليلىالسباحة
Result: 0.3
50----------------------------------------------------------------------------------------------------
Reference: قالوا فروق الملك دار مخاوف
Prediction: قالوفربلميكدرمخول
Result: 0.46153846153846156
51----------------------------------------------------------------------------------------------------
Reference: كيف تشرح هذا "أنا لا أستطيع."
Prediction: كيذالتجراللهذابللاأستطيه
Result: 0.5862068965517241
52----------------------------------------------------------------------------------------------------
Reference: كم الساعة ؟
Prediction: تمالسع
Result: 0.5454545454545454
53----------------------------------------------------------------------------------------------------
Reference: ما لقوم اذا يقال علي
Prediction: مايقومإذايقالعلي
Result: 0.3
54----------------------------------------------------------------------------------------------------
Reference: ألا نسيا نفسي حديث البلابلِ
Prediction: ألامتيانسيحديثالبنابل
Result: 0.3333333333333333
55----------------------------------------------------------------------------------------------------
Reference: تفيء الأيور على أهلها
Prediction: تفيالأيولعلىأهلنا
Result: 0.2857142857142857
56----------------------------------------------------------------------------------------------------
Reference: عما كانوا يعملون
Prediction: هناكانوايعملون
Result: 0.25
57----------------------------------------------------------------------------------------------------
Reference: شيعي الليل وقومي استقبلي
Prediction: شيعالليلوقوماستقبلي
Result: 0.20833333333333334
58----------------------------------------------------------------------------------------------------
Reference: .أعرف تلك الفتاة
Prediction: أعرفتلكالفتع
Result: 0.3125
59----------------------------------------------------------------------------------------------------
Reference: ما زال حر الشوق يغلب صبرها
Prediction: ماجعلحرالشوقيغلبصبرحة
Result: 0.34615384615384615
60----------------------------------------------------------------------------------------------------
Reference: قطع توم النهر سباحة.
Prediction: قتعتومالنهرسبحا
Result: 0.35
61----------------------------------------------------------------------------------------------------
Reference: لا تستمع إليه ، ما يقوله هراء.
Prediction: لاتستمعإليهمايقولرهرا
Result: 0.3333333333333333
62----------------------------------------------------------------------------------------------------
Reference: لماذا تسأل ؟
Prediction: لمازاتسأل
Result: 0.3333333333333333
63----------------------------------------------------------------------------------------------------
Reference: تد يحب زوجته إلزبيث.
Prediction: تاديحبزوجاتهوهزمث ه
Result: 0.55
64----------------------------------------------------------------------------------------------------
Reference: هذا هو منزله.
Prediction: هذاهومنزلة
Result: 0.3076923076923077
65----------------------------------------------------------------------------------------------------
Reference: إنها جميلة جداً.
Prediction: إنهاجميلةجدا
Result: 0.25
66----------------------------------------------------------------------------------------------------
Reference: طوبى لمن رفض الفتاة وقد غدا
Prediction: توبلمامرفضالفاتاةوقدغلا
Result: 0.37037037037037035
67----------------------------------------------------------------------------------------------------
Reference: لا أذكر بدقة.
Prediction: لاأذكربدقة
Result: 0.23076923076923078
68----------------------------------------------------------------------------------------------------
Reference: لم أسمع شيئاً.
Prediction: 
Result: 1.0
69----------------------------------------------------------------------------------------------------
Reference: رأيتك بينا أنت خل وصاحب
Prediction: رأيتكبيناأنتخيلوصاحوأح
Result: 0.34782608695652173
70----------------------------------------------------------------------------------------------------
Reference: أم القمر؟!
Prediction: أمالقمة
Result: 0.4
71----------------------------------------------------------------------------------------------------
Reference: أمرنا بفعل ذلك.
Prediction: ومرنابفهلدلك
Result: 0.4
72----------------------------------------------------------------------------------------------------
Reference: لم أصح من لذة لا لا ولا طرب
Prediction: لمأصحملدةلالىولاطرب
Result: 0.37037037037037035
73----------------------------------------------------------------------------------------------------
Reference: اتصل بي بيل ليلة أمس.
Prediction: اتصلبيبالليلةأمس
Result: 0.2857142857142857
74----------------------------------------------------------------------------------------------------
Reference: وَقَالَ بَعْضُ الْبُلَغَاءِ مِنْ خَيْرِ خِلَالِك الصَّبْرُ عَلَى اخْتِلَالِك
Prediction: وقالبعضالبلغاءمنخيعخلالكالصبعلىاختللك
Result: 0.5263157894736842
75----------------------------------------------------------------------------------------------------
Reference: إنه يعمل في قطاع صناعة السيارات.
Prediction: إنهيعملفيختاعالصنعاتالصيرة
Result: 0.46875
76----------------------------------------------------------------------------------------------------
Reference: نحن معاني الوجود فيه
Prediction: نحنمعانيالوجودفيه
Result: 0.15
77----------------------------------------------------------------------------------------------------
Reference: فَلَمَّا أَبْصَرَنِي قَالَ
Prediction: فلماأبصرنيقاله
Result: 0.5
78----------------------------------------------------------------------------------------------------
Reference: معدلات الكولسترول في دمي عالية.
Prediction: مغدلاتالكللسترولفيدميعالية
Result: 0.22580645161290322
79----------------------------------------------------------------------------------------------------
Reference: أعارض استخدام الموت عقوبةً. وأعارض استخدامه مكافأةً كذلك.
Prediction: أعارداستخدامالموتحقوبةوأعرداستخدامهمكفأةكذلك
Result: 0.2807017543859649
80----------------------------------------------------------------------------------------------------
Reference: .لقد تم اختراع الطائرة الورقية قبل ألفَيْ عام
Prediction: لقدتماأختراعالطائرالورقيةقبلألفيعام
Result: 0.26666666666666666
81----------------------------------------------------------------------------------------------------
Reference: مات مليون شخص في الحرب.
Prediction: ماتمليونشخصفيالحرر
Result: 0.2608695652173913
82----------------------------------------------------------------------------------------------------
Reference: ما استب قط اثنان إلا غلبا
Prediction: مستبقطتإفنانإلاغلب
Result: 0.4
83----------------------------------------------------------------------------------------------------
Reference: لا قضى الله بيننا بالفراق
Prediction: آقداللهبيلنابالفراء
Result: 0.4
84----------------------------------------------------------------------------------------------------
Reference: لا أريد الذهاب إلى النوم الآن.
Prediction: لاأريدالزهارإلىالنومالآن
Result: 0.26666666666666666
85----------------------------------------------------------------------------------------------------
Reference: كانت ليلى تسرق البنوك.
Prediction: تعمكليلىكسرقلبنوك
Result: 0.45454545454545453
86----------------------------------------------------------------------------------------------------
Reference: فَلَمْ يَلْبَثْ بَعْدَ ذَلِكَ إلَّا يَسِيرًا حَتَّى مَاتَ رَحِمَهُ اللَّهُ
Prediction: فلميلبثبعدذلكإلاينسيراحتىعبعتارحمهالله
Result: 0.5540540540540541
Final: 0.3833821707083171
(base) or@anidjar:~/Desktop/wav2vec2/inference$ cd ..
(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading Datasets... -----------------
Using custom data configuration default-af094a25dc770174
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-af094a25dc770174/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13530.01it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2613.27it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-af094a25dc770174/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 914.59it/s]
Using custom data configuration default-dbc311f39ef5c13c
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8559.80it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3151.24it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1301.77it/s]
Casting the dataset:  75%|████████████████████████████████████████████████████████████████████████████████████████▌                             | 3/4 [00:00<00:00, 17.86ba/s]
Casting the dataset:  50%|███████████████████████████████████████████████████████████                                                           | 1/2 [00:00<00:00, 22.95ba/s]
----------------- Loading Datasets complete. -----------------


----------------- Removing special characters... -----------------
Parameter 'function'=<function remove_special_characters at 0x7f84ec66aa60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39261/39261 [00:01<00:00, 34528.55ex/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10438/10438 [00:00<00:00, 35197.37ex/s]
----------------- Removing special characters complete. -----------------


train columns: ['path', 'audio', 'sentence']
test columns: ['path', 'audio', 'sentence']
----------------- saving datasets pre-vocab... -----------------
----------------- saving datasets pre-vocab complete. -----------


----------------- Extracting all characters... -----------------
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
----------------- Extracting all characters complete. -----------------


----------------- Preparing vocab... -----------------
Vocab_dict: {'س': 0, 'ا': 1, 'ز': 2, 'ظ': 3, 'ک': 4, 'ـ': 5, 'a': 6, 'ن': 7, 'ح': 8, 'ع': 9, 'ى': 10, 'ی': 11, 'غ': 12, 'ه': 13, 'ۛ': 14, 'چ': 15, 'ج': 16, 'ط': 17, 'ث': 18, 'ك': 19, 'م': 20, 'ﺃ': 21, 'ض': 22, '-': 23, 'ب': 24, 'ف': 25, 'ش': 26, 'ر': 27, 'ء': 28, ' ': 29, 'أ': 30, 'آ': 31, 'ۘ': 32, 'ق': 33, 'خ': 34, 'إ': 35, 'ة': 36, 'm': 37, 'ذ': 38, 'ي': 39, 'ئ': 40, 'ل': 41, 's': 42, 'ھ': 43, 'ص': 44, 't': 45, 'ۗ': 46, 'ﻻ': 47, 'ڨ': 48, 'و': 49, 'د': 50, 'ؤ': 51, 'ت': 52, 'h': 53}
Vocab_len: 56
----------------- Preparing vocab complete. -----------------


----------------- Saving vocab to jason... -----------------
----------------- Saving vocab to jason complete. -----------------


sample path: common_voice_ar_24089467.mp3

Sanity Check sampling rate is 48khz: {'path': '/home/or/Desktop/wav2vec2/augmentations/pitchShift/augmented_PitchShift_audio/common_voice_ar_24089467.mp3', 'array': array([-5.9817457e-06, -2.8711067e-06, -2.1505141e-06, ...,
        2.4527960e-04,  2.2401643e-04,  2.0270131e-04], dtype=float32), 'sampling_rate': 48000}

----------------- Resampling to 16khz... -----------------
Making sure the sampling rate changed to 16khz {'path': '/home/or/Desktop/wav2vec2/augmentations/pitchShift/augmented_PitchShift_audio/common_voice_ar_24089467.mp3', 'array': array([-2.0031134e-06, -7.3064207e-06, -4.5118213e-06, ...,
        3.9896526e-04,  3.1525159e-04,  3.0002964e-04], dtype=float32), 'sampling_rate': 16000}
----------------- Resampling complete. -----------------


----------------- Preparing datasets... -----------------
#0:   0%|                                                                                                                                            | 0/9816 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/9815 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#0:   0%|                                                                                                                                    | 2/9816 [00:00<17:09,  9.54ex/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9815/9815 [11:05<00:00, 14.74ex/s]
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9815/9815 [11:06<00:00, 14.72ex/s]
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9815/9815 [11:08<00:00, 14.69ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9816/9816 [11:08<00:00, 14.68ex/s]
#0:   0%|                                                                                                                                            | 0/2610 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/2609 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [03:12<00:00, 13.53ex/s]
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [03:15<00:00, 13.34ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [03:16<00:00, 13.29ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [03:16<00:00, 13.28ex/s]
#2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2607/2609 [03:16<00:00, 18.85ex/s]

----------------- Preparing datasets complete. -----------------


----------------- saving datasets... -----------------
----------------- saving datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:248: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'quantizer.weight_proj.bias', 'project_q.bias', 'project_hid.bias', 'project_q.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39261
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 24540
  Number of trainable parameters = 311285944
  0%|                                                                                                                                               | 0/24540 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 16.9832, 'learning_rate': 5.399999999999999e-06, 'epoch': 0.0}                                                                                                       
{'loss': 18.0131, 'learning_rate': 1.14e-05, 'epoch': 0.01}                                                                                                                   
{'loss': 20.6646, 'learning_rate': 1.74e-05, 'epoch': 0.01}                                                                                                                   
{'loss': 20.5963, 'learning_rate': 2.1599999999999996e-05, 'epoch': 0.02}                                                                                                     
{'loss': 22.1182, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.02}                                                                                                     
{'loss': 14.4842, 'learning_rate': 3.36e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 11.544, 'learning_rate': 3.96e-05, 'epoch': 0.03}                                                                                                                    
{'loss': 8.9664, 'learning_rate': 4.56e-05, 'epoch': 0.03}                                                                                                                    
{'loss': 7.5223, 'learning_rate': 5.1599999999999994e-05, 'epoch': 0.04}                                                                                                      
{'loss': 5.7961, 'learning_rate': 5.76e-05, 'epoch': 0.04}                                                                                                                    
  0%|▌                                                                                                                                  | 100/24540 [01:00<2:43:53,  2.49it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8

^CTraceback (most recent call last):████████████████████████████████████████████████████████▎                                              | 850/1305 [03:54<02:10,  3.49it/s]
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2964, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2343, in __getitem__
    return self._getitem(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2328, in _getitem
    formatted_output = format_table(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 517, in format_table
    formatted_output = formatter(pa_table_to_format, query_type=query_type)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 282, in __call__
    return self.format_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 311, in format_row
    row = self.python_arrow_extractor().extract_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 141, in extract_row
    return _unnest(pa_table.to_pydict())
KeyboardInterrupt
  0%|▌                                                                                                                                 | 100/24540 [04:55<20:05:15,  2.96s/it]

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


----------------- Loading datasets... -----------------
----------------- Loading datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:248: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_q.weight', 'quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_hid.weight']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 39261
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 24540
  Number of trainable parameters = 311285944
  0%|                                                                                                                                               | 0/24540 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 16.7204, 'learning_rate': 4.8e-06, 'epoch': 0.0}                                                                                                                     
{'loss': 17.731, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.01}                                                                                                      
{'loss': 20.328, 'learning_rate': 1.68e-05, 'epoch': 0.01}                                                                                                                    
{'loss': 20.1737, 'learning_rate': 2.2199999999999998e-05, 'epoch': 0.02}                                                                                                     
{'loss': 21.2148, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.02}                                                                                                     
{'loss': 13.7267, 'learning_rate': 3.42e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 10.8972, 'learning_rate': 4.02e-05, 'epoch': 0.03}                                                                                                                   
{'loss': 8.663, 'learning_rate': 4.62e-05, 'epoch': 0.03}                                                                                                                     
{'loss': 7.5228, 'learning_rate': 5.2199999999999995e-05, 'epoch': 0.04}                                                                                                      
{'loss': 5.9831, 'learning_rate': 5.82e-05, 'epoch': 0.04}                                                                                                                    
  0%|▌                                                                                                                                  | 100/24540 [00:59<2:48:07,  2.42it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 6.092557907104492, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 356.254, 'eval_samples_per_second': 29.299, 'eval_steps_per_second': 3.663, 'epoch': 0.04}                                                                                                                                                                
{'loss': 4.4127, 'learning_rate': 6.419999999999999e-05, 'epoch': 0.04}                                                                                                       
{'loss': 4.2216, 'learning_rate': 6.96e-05, 'epoch': 0.05}                                                                                                                    
{'loss': 3.9079, 'learning_rate': 7.56e-05, 'epoch': 0.05}                                                                                                                    
{'loss': 3.7241, 'learning_rate': 8.16e-05, 'epoch': 0.06}                                                                                                                    
{'loss': 3.669, 'learning_rate': 8.759999999999999e-05, 'epoch': 0.06}                                                                                                        
{'loss': 3.4388, 'learning_rate': 9.36e-05, 'epoch': 0.07}                                                                                                                    
{'loss': 3.3771, 'learning_rate': 9.96e-05, 'epoch': 0.07}                                                                                                                    
{'loss': 3.4025, 'learning_rate': 0.00010559999999999998, 'epoch': 0.07}                                                                                                      
{'loss': 3.4087, 'learning_rate': 0.00011159999999999999, 'epoch': 0.08}                                                                                                      
{'loss': 3.4151, 'learning_rate': 0.0001176, 'epoch': 0.08}                                                                                                                   
  1%|█                                                                                                                                  | 200/24540 [07:53<2:39:33,  2.54it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.448927164077759, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 351.9345, 'eval_samples_per_second': 29.659, 'eval_steps_per_second': 3.708, 'epoch': 0.08}                                                                                                                                                               
{'loss': 3.3092, 'learning_rate': 0.0001236, 'epoch': 0.09}                                                                                                                   
{'loss': 3.3264, 'learning_rate': 0.00012959999999999998, 'epoch': 0.09}                                                                                                      
{'loss': 3.3696, 'learning_rate': 0.0001356, 'epoch': 0.09}                                                                                                                   
{'loss': 3.3723, 'learning_rate': 0.00014159999999999997, 'epoch': 0.1}                                                                                                       
{'loss': 3.3772, 'learning_rate': 0.00014759999999999998, 'epoch': 0.1}                                                                                                       
{'loss': 3.3007, 'learning_rate': 0.0001536, 'epoch': 0.11}                                                                                                                   
{'loss': 3.2901, 'learning_rate': 0.0001596, 'epoch': 0.11}                                                                                                                   
{'loss': 3.32, 'learning_rate': 0.0001656, 'epoch': 0.11}                                                                                                                     
{'loss': 3.2621, 'learning_rate': 0.00017159999999999997, 'epoch': 0.12}                                                                                                      
{'loss': 3.2179, 'learning_rate': 0.00017759999999999998, 'epoch': 0.12}                                                                                                      
  1%|█▌                                                                                                                                 | 300/24540 [14:45<2:53:16,  2.33it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.1977386474609375, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 353.5152, 'eval_samples_per_second': 29.526, 'eval_steps_per_second': 3.691, 'epoch': 0.12}                                                                                                                                                              
{'loss': 3.1932, 'learning_rate': 0.0001836, 'epoch': 0.13}                                                                                                                   
{'loss': 3.1575, 'learning_rate': 0.00018959999999999997, 'epoch': 0.13}                                                                                                      
{'loss': 3.1648, 'learning_rate': 0.00019559999999999998, 'epoch': 0.13}                                                                                                      
{'loss': 3.1371, 'learning_rate': 0.0002016, 'epoch': 0.14}                                                                                                                   
{'loss': 3.1191, 'learning_rate': 0.00020759999999999998, 'epoch': 0.14}                                                                                                      
{'loss': 3.1507, 'learning_rate': 0.00021359999999999996, 'epoch': 0.15}                                                                                                      
{'loss': 3.113, 'learning_rate': 0.00021959999999999997, 'epoch': 0.15}                                                                                                       
{'loss': 3.1873, 'learning_rate': 0.00022559999999999998, 'epoch': 0.15}                                                                                                      
{'loss': 3.1118, 'learning_rate': 0.0002316, 'epoch': 0.16}                                                                                                                   
{'loss': 3.1376, 'learning_rate': 0.0002376, 'epoch': 0.16}                                                                                                                   
  2%|██▏                                                                                                                                | 400/24540 [21:37<2:50:09,  2.36it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.1421732902526855, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 351.4628, 'eval_samples_per_second': 29.699, 'eval_steps_per_second': 3.713, 'epoch': 0.16}                                                                                                                                                              
{'loss': 3.1646, 'learning_rate': 0.00024359999999999999, 'epoch': 0.17}                                                                                                      
{'loss': 3.1372, 'learning_rate': 0.00024959999999999994, 'epoch': 0.17}                                                                                                      
{'loss': 3.1098, 'learning_rate': 0.0002556, 'epoch': 0.18}                                                                                                                   
{'loss': 3.1225, 'learning_rate': 0.00026159999999999996, 'epoch': 0.18}                                                                                                      
{'loss': 3.1258, 'learning_rate': 0.0002676, 'epoch': 0.18}                                                                                                                   
{'loss': 3.127, 'learning_rate': 0.0002736, 'epoch': 0.19}                                                                                                                    
{'loss': 3.13, 'learning_rate': 0.00027959999999999997, 'epoch': 0.19}                                                                                                        
{'loss': 3.101, 'learning_rate': 0.00028559999999999995, 'epoch': 0.2}                                                                                                        
{'loss': 3.112, 'learning_rate': 0.0002916, 'epoch': 0.2}                                                                                                                     
{'loss': 3.0977, 'learning_rate': 0.00029759999999999997, 'epoch': 0.2}                                                                                                       
  2%|██▋                                                                                                                                | 500/24540 [28:27<2:42:57,  2.46it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.2610421180725098, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 355.8978, 'eval_samples_per_second': 29.329, 'eval_steps_per_second': 3.667, 'epoch': 0.2}                                                                                                                                                               
{'loss': 3.1392, 'learning_rate': 0.0002999251247920133, 'epoch': 0.21}                                                                                                       
{'loss': 3.1543, 'learning_rate': 0.00029980033277870213, 'epoch': 0.21}                                                                                                      
{'loss': 3.1641, 'learning_rate': 0.00029967554076539097, 'epoch': 0.22}                                                                                                      
{'loss': 3.1077, 'learning_rate': 0.0002995507487520798, 'epoch': 0.22}                                                                                                       
{'loss': 3.0573, 'learning_rate': 0.0002994259567387687, 'epoch': 0.22}                                                                                                       
{'loss': 3.093, 'learning_rate': 0.00029930116472545753, 'epoch': 0.23}                                                                                                       
{'loss': 3.102, 'learning_rate': 0.00029917637271214637, 'epoch': 0.23}                                                                                                       
{'loss': 3.1056, 'learning_rate': 0.00029905158069883526, 'epoch': 0.24}                                                                                                      
{'loss': 3.0809, 'learning_rate': 0.0002989267886855241, 'epoch': 0.24}                                                                                                       
{'loss': 3.0606, 'learning_rate': 0.00029880199667221293, 'epoch': 0.24}                                                                                                      
  2%|███▏                                                                                                                               | 600/24540 [35:22<2:44:38,  2.42it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.1896395683288574, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 351.8551, 'eval_samples_per_second': 29.666, 'eval_steps_per_second': 3.709, 'epoch': 0.24}                                                                                                                                                              
{'loss': 3.1034, 'learning_rate': 0.0002986772046589018, 'epoch': 0.25}                                                                                                       
{'loss': 3.1276, 'learning_rate': 0.00029855241264559065, 'epoch': 0.25}                                                                                                      
{'loss': 3.1158, 'learning_rate': 0.0002984276206322795, 'epoch': 0.26}                                                                                                       
{'loss': 3.0783, 'learning_rate': 0.0002983028286189683, 'epoch': 0.26}                                                                                                       
{'loss': 3.0588, 'learning_rate': 0.0002981780366056572, 'epoch': 0.26}                                                                                                       
{'loss': 3.0909, 'learning_rate': 0.00029805324459234605, 'epoch': 0.27}                                                                                                      
{'loss': 3.0868, 'learning_rate': 0.0002979284525790349, 'epoch': 0.27}                                                                                                       
{'loss': 3.1142, 'learning_rate': 0.0002978036605657238, 'epoch': 0.28}                                                                                                       
{'loss': 3.0689, 'learning_rate': 0.0002976788685524126, 'epoch': 0.28}                                                                                                       
{'loss': 3.0659, 'learning_rate': 0.00029755407653910145, 'epoch': 0.29}                                                                                                      
  3%|███▋                                                                                                                               | 700/24540 [42:11<2:40:46,  2.47it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.1023669242858887, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 353.8077, 'eval_samples_per_second': 29.502, 'eval_steps_per_second': 3.688, 'epoch': 0.29}                                                                                                                                                              
{'loss': 3.0901, 'learning_rate': 0.0002974292845257903, 'epoch': 0.29}                                                                                                       
{'loss': 3.1128, 'learning_rate': 0.0002973044925124792, 'epoch': 0.29}                                                                                                       
{'loss': 3.0947, 'learning_rate': 0.000297179700499168, 'epoch': 0.3}                                                                                                         
{'loss': 3.047, 'learning_rate': 0.00029705490848585685, 'epoch': 0.3}                                                                                                        
{'loss': 3.0282, 'learning_rate': 0.00029693011647254574, 'epoch': 0.31}                                                                                                      
{'loss': 3.0996, 'learning_rate': 0.0002968053244592346, 'epoch': 0.31}                                                                                                       
{'loss': 3.1051, 'learning_rate': 0.0002966805324459234, 'epoch': 0.31}                                                                                                       
{'loss': 3.0947, 'learning_rate': 0.0002965557404326123, 'epoch': 0.32}                                                                                                       
{'loss': 3.1032, 'learning_rate': 0.00029643094841930114, 'epoch': 0.32}                                                                                                      
{'loss': 3.0721, 'learning_rate': 0.00029630615640598997, 'epoch': 0.33}                                                                                                      
  3%|████▎                                                                                                                              | 800/24540 [49:04<2:38:03,  2.50it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.2728757858276367, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 351.3379, 'eval_samples_per_second': 29.709, 'eval_steps_per_second': 3.714, 'epoch': 0.33}                                                                                                                                                              
{'loss': 3.1071, 'learning_rate': 0.0002961813643926788, 'epoch': 0.33}                                                                                                       
{'loss': 3.0908, 'learning_rate': 0.0002960565723793677, 'epoch': 0.33}                                                                                                       
{'loss': 3.0834, 'learning_rate': 0.00029593178036605653, 'epoch': 0.34}                                                                                                      
{'loss': 3.0605, 'learning_rate': 0.00029580698835274537, 'epoch': 0.34}                                                                                                      
{'loss': 3.0389, 'learning_rate': 0.00029568219633943426, 'epoch': 0.35}                                                                                                      
{'loss': 3.0867, 'learning_rate': 0.0002955574043261231, 'epoch': 0.35}                                                                                                       
{'loss': 3.0743, 'learning_rate': 0.00029543261231281193, 'epoch': 0.35}                                                                                                      
{'loss': 3.0474, 'learning_rate': 0.0002953078202995008, 'epoch': 0.36}                                                                                                       
{'loss': 3.0419, 'learning_rate': 0.00029518302828618966, 'epoch': 0.36}                                                                                                      
{'loss': 3.0358, 'learning_rate': 0.0002950582362728785, 'epoch': 0.37}                                                                                                       
  4%|████▊                                                                                                                              | 900/24540 [55:54<2:39:18,  2.47it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.1155591011047363, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 353.279, 'eval_samples_per_second': 29.546, 'eval_steps_per_second': 3.694, 'epoch': 0.37}                                                                                                                                                               
{'loss': 3.0699, 'learning_rate': 0.00029493344425956733, 'epoch': 0.37}                                                                                                      
{'loss': 3.049, 'learning_rate': 0.0002948086522462562, 'epoch': 0.37}                                                                                                        
{'loss': 3.0443, 'learning_rate': 0.00029468386023294505, 'epoch': 0.38}                                                                                                      
{'loss': 3.0513, 'learning_rate': 0.0002945590682196339, 'epoch': 0.38}                                                                                                       
{'loss': 3.0284, 'learning_rate': 0.0002944342762063228, 'epoch': 0.39}                                                                                                       
{'loss': 3.0658, 'learning_rate': 0.0002943094841930116, 'epoch': 0.39}                                                                                                       
{'loss': 3.0401, 'learning_rate': 0.00029418469217970045, 'epoch': 0.4}                                                                                                       
{'loss': 2.9952, 'learning_rate': 0.00029405990016638934, 'epoch': 0.4}                                                                                                       
{'loss': 2.9522, 'learning_rate': 0.0002939351081530782, 'epoch': 0.4}                                                                                                        
{'loss': 2.9483, 'learning_rate': 0.000293810316139767, 'epoch': 0.41}                                                                                                        
  4%|█████▏                                                                                                                          | 1000/24540 [1:02:46<2:36:50,  2.50it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.058274030685425, 'eval_wer': 1.0, 'eval_cer': 0.9816821801525435, 'eval_runtime': 352.662, 'eval_samples_per_second': 29.598, 'eval_steps_per_second': 3.7, 'epoch': 0.41}                                                                                                                                                                  
{'loss': 3.1561, 'learning_rate': 0.00029368552412645585, 'epoch': 0.41}                                                                                                      
{'loss': 3.0158, 'learning_rate': 0.00029356073211314474, 'epoch': 0.42}                                                                                                      
{'loss': 2.9791, 'learning_rate': 0.0002934359400998336, 'epoch': 0.42}                                                                                                       
{'loss': 2.9553, 'learning_rate': 0.0002933111480865224, 'epoch': 0.42}                                                                                                       
{'loss': 2.9158, 'learning_rate': 0.0002931863560732113, 'epoch': 0.43}                                                                                                       
{'loss': 3.0061, 'learning_rate': 0.00029306156405990014, 'epoch': 0.43}                                                                                                      
{'loss': 2.917, 'learning_rate': 0.000292936772046589, 'epoch': 0.44}                                                                                                         
{'loss': 2.9026, 'learning_rate': 0.00029281198003327786, 'epoch': 0.44}                                                                                                      
{'loss': 2.8528, 'learning_rate': 0.0002926871880199667, 'epoch': 0.44}                                                                                                       
{'loss': 2.8298, 'learning_rate': 0.00029256239600665554, 'epoch': 0.45}                                                                                                      
  4%|█████▋                                                                                                                          | 1100/24540 [1:09:38<2:42:10,  2.41it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 2.8510468006134033, 'eval_wer': 1.0004790189691513, 'eval_cer': 0.9816821801525435, 'eval_runtime': 354.8235, 'eval_samples_per_second': 29.417, 'eval_steps_per_second': 3.678, 'epoch': 0.45}                                                                                                                                               
{'loss': 2.7747, 'learning_rate': 0.00029243760399334437, 'epoch': 0.45}                                                                                                      
{'loss': 2.7136, 'learning_rate': 0.00029231281198003326, 'epoch': 0.46}                                                                                                      
{'loss': 2.6698, 'learning_rate': 0.0002921880199667221, 'epoch': 0.46}                                                                                                       
{'loss': 2.6141, 'learning_rate': 0.00029206322795341093, 'epoch': 0.46}                                                                                                      
{'loss': 2.5971, 'learning_rate': 0.0002919384359400998, 'epoch': 0.47}                                                                                                       
{'loss': 2.6117, 'learning_rate': 0.00029181364392678866, 'epoch': 0.47}                                                                                                      
{'loss': 2.4339, 'learning_rate': 0.0002916888519134775, 'epoch': 0.48}                                                                                                       
{'loss': 2.4447, 'learning_rate': 0.00029156405990016633, 'epoch': 0.48}                                                                                                      
{'loss': 2.4362, 'learning_rate': 0.0002914392678868552, 'epoch': 0.48}                                                                                                       
{'loss': 2.4734, 'learning_rate': 0.00029131447587354406, 'epoch': 0.49}                                                                                                      
  5%|██████▎                                                                                                                         | 1200/24540 [1:16:32<2:37:29,  2.47it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 2.1475751399993896, 'eval_wer': 1.997988120329565, 'eval_cer': 0.6743218540541677, 'eval_runtime': 355.5546, 'eval_samples_per_second': 29.357, 'eval_steps_per_second': 3.67, 'epoch': 0.49}                                                                                                                                                 
{'loss': 2.2962, 'learning_rate': 0.0002911896838602329, 'epoch': 0.49}                                                                                                       
{'loss': 2.2634, 'learning_rate': 0.0002910648918469218, 'epoch': 0.5}                                                                                                        
{'loss': 2.2557, 'learning_rate': 0.0002909400998336106, 'epoch': 0.5}                                                                                                        
{'loss': 2.271, 'learning_rate': 0.00029081530782029946, 'epoch': 0.51}                                                                                                       
{'loss': 2.2264, 'learning_rate': 0.00029069051580698835, 'epoch': 0.51}                                                                                                      
{'loss': 2.1452, 'learning_rate': 0.0002905657237936772, 'epoch': 0.51}                                                                                                       
{'loss': 2.146, 'learning_rate': 0.000290440931780366, 'epoch': 0.52}                                                                                                         
{'loss': 2.1521, 'learning_rate': 0.00029031613976705485, 'epoch': 0.52}                                                                                                      
{'loss': 2.1884, 'learning_rate': 0.00029019134775374374, 'epoch': 0.53}                                                                                                      
{'loss': 2.1691, 'learning_rate': 0.0002900665557404326, 'epoch': 0.53}                                                                                                       
  5%|██████▊                                                                                                                         | 1300/24540 [1:23:26<2:37:57,  2.45it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.9580258131027222, 'eval_wer': 1.9981797279172255, 'eval_cer': 0.597171853003593, 'eval_runtime': 356.9233, 'eval_samples_per_second': 29.244, 'eval_steps_per_second': 3.656, 'epoch': 0.53}                                                                                                                                                
{'loss': 1.9905, 'learning_rate': 0.0002899417637271214, 'epoch': 0.53}                                                                                                       
{'loss': 1.9866, 'learning_rate': 0.0002898169717138103, 'epoch': 0.54}                                                                                                       
{'loss': 1.9779, 'learning_rate': 0.00028969217970049914, 'epoch': 0.54}                                                                                                      
{'loss': 2.0035, 'learning_rate': 0.000289567387687188, 'epoch': 0.55}                                                                                                        
{'loss': 2.2238, 'learning_rate': 0.00028944259567387687, 'epoch': 0.55}                                                                                                      
{'loss': 1.8753, 'learning_rate': 0.0002893178036605657, 'epoch': 0.55}                                                                                                       
{'loss': 1.9804, 'learning_rate': 0.00028919301164725454, 'epoch': 0.56}                                                                                                      
{'loss': 1.8694, 'learning_rate': 0.0002890682196339434, 'epoch': 0.56}                                                                                                       
{'loss': 1.9911, 'learning_rate': 0.00028894342762063226, 'epoch': 0.57}                                                                                                      
{'loss': 2.0203, 'learning_rate': 0.0002888186356073211, 'epoch': 0.57}                                                                                                       
  6%|███████▎                                                                                                                        | 1400/24540 [1:30:22<2:33:59,  2.50it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.5535637140274048, 'eval_wer': 1.9968384748036023, 'eval_cer': 0.5272792217342887, 'eval_runtime': 354.3708, 'eval_samples_per_second': 29.455, 'eval_steps_per_second': 3.683, 'epoch': 0.57}                                                                                                                                               
{'loss': 1.8829, 'learning_rate': 0.00028869384359400994, 'epoch': 0.57}                                                                                                      
{'loss': 1.7734, 'learning_rate': 0.0002885690515806988, 'epoch': 0.58}                                                                                                       
{'loss': 1.8653, 'learning_rate': 0.00028844425956738766, 'epoch': 0.58}                                                                                                      
{'loss': 1.9347, 'learning_rate': 0.0002883194675540765, 'epoch': 0.59}                                                                                                       
{'loss': 1.9703, 'learning_rate': 0.0002881946755407654, 'epoch': 0.59}                                                                                                       
{'loss': 1.7558, 'learning_rate': 0.0002880698835274542, 'epoch': 0.59}                                                                                                       
{'loss': 1.7254, 'learning_rate': 0.00028794509151414306, 'epoch': 0.6}                                                                                                       
{'loss': 1.8013, 'learning_rate': 0.0002878202995008319, 'epoch': 0.6}                                                                                                        
{'loss': 1.7907, 'learning_rate': 0.0002876955074875208, 'epoch': 0.61}                                                                                                       
{'loss': 1.8942, 'learning_rate': 0.0002875707154742096, 'epoch': 0.61}                                                                                                       
  6%|███████▊                                                                                                                        | 1500/24540 [1:37:14<2:32:07,  2.52it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.4362202882766724, 'eval_wer': 1.9968384748036023, 'eval_cer': 0.49159330153594016, 'eval_runtime': 356.7752, 'eval_samples_per_second': 29.257, 'eval_steps_per_second': 3.658, 'epoch': 0.61}                                                                                                                                              
{'loss': 1.6762, 'learning_rate': 0.00028744592346089846, 'epoch': 0.62}                                                                                                      
{'loss': 1.6943, 'learning_rate': 0.00028732113144758735, 'epoch': 0.62}                                                                                                      
{'loss': 1.8059, 'learning_rate': 0.0002871963394342762, 'epoch': 0.62}                                                                                                       
{'loss': 1.7267, 'learning_rate': 0.000287071547420965, 'epoch': 0.63}                                                                                                        
{'loss': 1.8975, 'learning_rate': 0.0002869467554076539, 'epoch': 0.63}                                                                                                       
{'loss': 1.609, 'learning_rate': 0.00028682196339434275, 'epoch': 0.64}                                                                                                       
{'loss': 1.6126, 'learning_rate': 0.0002866971713810316, 'epoch': 0.64}                                                                                                       
{'loss': 1.7193, 'learning_rate': 0.0002865723793677204, 'epoch': 0.64}                                                                                                       
{'loss': 1.7304, 'learning_rate': 0.0002864475873544093, 'epoch': 0.65}                                                                                                       
{'loss': 1.9088, 'learning_rate': 0.00028632279534109814, 'epoch': 0.65}                                                                                                      
  7%|████████▎                                                                                                                       | 1600/24540 [1:44:09<2:33:27,  2.49it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.2692394256591797, 'eval_wer': 1.9946349875455067, 'eval_cer': 0.4609333305318009, 'eval_runtime': 355.0628, 'eval_samples_per_second': 29.398, 'eval_steps_per_second': 3.675, 'epoch': 0.65}                                                                                                                                               
{'loss': 1.4474, 'learning_rate': 0.000286198003327787, 'epoch': 0.66}                                                                                                        
{'loss': 1.5542, 'learning_rate': 0.00028607321131447587, 'epoch': 0.66}                                                                                                      
{'loss': 1.6302, 'learning_rate': 0.0002859484193011647, 'epoch': 0.66}                                                                                                       
{'loss': 1.7004, 'learning_rate': 0.00028582362728785354, 'epoch': 0.67}                                                                                                      
{'loss': 1.7689, 'learning_rate': 0.0002856988352745424, 'epoch': 0.67}                                                                                                       
{'loss': 1.551, 'learning_rate': 0.00028557404326123127, 'epoch': 0.68}                                                                                                       
{'loss': 1.5057, 'learning_rate': 0.0002854492512479201, 'epoch': 0.68}                                                                                                       
{'loss': 1.585, 'learning_rate': 0.00028532445923460894, 'epoch': 0.68}                                                                                                       
{'loss': 1.6096, 'learning_rate': 0.00028519966722129783, 'epoch': 0.69}                                                                                                      
{'loss': 1.8479, 'learning_rate': 0.00028507487520798667, 'epoch': 0.69}                                                                                                      
  7%|████████▊                                                                                                                       | 1700/24540 [1:51:04<2:34:45,  2.46it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.1982271671295166, 'eval_wer': 1.9939643609886952, 'eval_cer': 0.4430735612379972, 'eval_runtime': 352.0962, 'eval_samples_per_second': 29.645, 'eval_steps_per_second': 3.706, 'epoch': 0.69}                                                                                                                                               
{'loss': 1.4482, 'learning_rate': 0.0002849500831946755, 'epoch': 0.7}                                                                                                        
{'loss': 1.5149, 'learning_rate': 0.0002848252911813644, 'epoch': 0.7}                                                                                                        
{'loss': 1.5486, 'learning_rate': 0.00028470049916805323, 'epoch': 0.7}                                                                                                       
{'loss': 1.625, 'learning_rate': 0.00028457570715474206, 'epoch': 0.71}                                                                                                       
{'loss': 1.8491, 'learning_rate': 0.0002844509151414309, 'epoch': 0.71}                                                                                                       
{'loss': 1.4274, 'learning_rate': 0.0002843261231281198, 'epoch': 0.72}                                                                                                       
{'loss': 1.468, 'learning_rate': 0.0002842013311148086, 'epoch': 0.72}                                                                                                        
{'loss': 1.4753, 'learning_rate': 0.00028407653910149746, 'epoch': 0.73}                                                                                                      
{'loss': 1.6219, 'learning_rate': 0.00028395174708818635, 'epoch': 0.73}                                                                                                      
{'loss': 1.6943, 'learning_rate': 0.0002838269550748752, 'epoch': 0.73}                                                                                                       
  7%|█████████▍                                                                                                                      | 1800/24540 [1:57:54<2:32:42,  2.48it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.2635763883590698, 'eval_wer': 1.9937727534010348, 'eval_cer': 0.4509738827138445, 'eval_runtime': 354.431, 'eval_samples_per_second': 29.45, 'eval_steps_per_second': 3.682, 'epoch': 0.73}                                                                                                                                                 
{'loss': 1.4571, 'learning_rate': 0.000283702163061564, 'epoch': 0.74}                                                                                                        
{'loss': 1.4652, 'learning_rate': 0.0002835773710482529, 'epoch': 0.74}                                                                                                       
{'loss': 1.5294, 'learning_rate': 0.00028345257903494175, 'epoch': 0.75}                                                                                                      
{'loss': 1.594, 'learning_rate': 0.0002833277870216306, 'epoch': 0.75}                                                                                                        
{'loss': 1.6731, 'learning_rate': 0.0002832029950083194, 'epoch': 0.75}                                                                                                       
{'loss': 1.3063, 'learning_rate': 0.0002830782029950083, 'epoch': 0.76}                                                                                                       
{'loss': 1.3858, 'learning_rate': 0.00028295341098169715, 'epoch': 0.76}                                                                                                      
{'loss': 1.5042, 'learning_rate': 0.000282828618968386, 'epoch': 0.77}                                                                                                        
{'loss': 1.6317, 'learning_rate': 0.00028270382695507487, 'epoch': 0.77}                                                                                                      
{'loss': 1.5862, 'learning_rate': 0.0002825790349417637, 'epoch': 0.77}                                                                                                       
  8%|█████████▉                                                                                                                      | 1900/24540 [2:04:47<2:33:59,  2.45it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.1096456050872803, 'eval_wer': 1.9908986395861277, 'eval_cer': 0.4099678524152712, 'eval_runtime': 355.291, 'eval_samples_per_second': 29.379, 'eval_steps_per_second': 3.673, 'epoch': 0.77}                                                                                                                                                
{'loss': 1.3168, 'learning_rate': 0.00028245424292845254, 'epoch': 0.78}                                                                                                      
{'loss': 1.3887, 'learning_rate': 0.00028232945091514143, 'epoch': 0.78}                                                                                                      
{'loss': 1.4622, 'learning_rate': 0.00028220465890183027, 'epoch': 0.79}                                                                                                      
{'loss': 1.5388, 'learning_rate': 0.0002820798668885191, 'epoch': 0.79}                                                                                                       
{'loss': 1.751, 'learning_rate': 0.00028195507487520794, 'epoch': 0.79}                                                                                                       
{'loss': 1.3758, 'learning_rate': 0.00028183028286189683, 'epoch': 0.8}                                                                                                       
{'loss': 1.4207, 'learning_rate': 0.00028170549084858567, 'epoch': 0.8}                                                                                                       
{'loss': 1.491, 'learning_rate': 0.0002815806988352745, 'epoch': 0.81}                                                                                                        
{'loss': 1.5688, 'learning_rate': 0.0002814559068219634, 'epoch': 0.81}                                                                                                       
{'loss': 1.7267, 'learning_rate': 0.00028133111480865223, 'epoch': 0.81}                                                                                                      
  8%|██████████▍                                                                                                                     | 2000/24540 [2:11:41<2:29:20,  2.52it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.1301900148391724, 'eval_wer': 1.9911860509676182, 'eval_cer': 0.4186035761561574, 'eval_runtime': 355.3281, 'eval_samples_per_second': 29.376, 'eval_steps_per_second': 3.673, 'epoch': 0.81}                                                                                                                                               
{'loss': 1.2905, 'learning_rate': 0.00028120632279534107, 'epoch': 0.82}                                                                                                      
{'loss': 1.3499, 'learning_rate': 0.00028108153078202996, 'epoch': 0.82}                                                                                                      
{'loss': 1.4531, 'learning_rate': 0.0002809567387687188, 'epoch': 0.83}                                                                                                       
{'loss': 1.5059, 'learning_rate': 0.00028083194675540763, 'epoch': 0.83}                                                                                                      
{'loss': 1.5988, 'learning_rate': 0.00028070715474209646, 'epoch': 0.84}                                                                                                      
{'loss': 1.2798, 'learning_rate': 0.00028058236272878535, 'epoch': 0.84}                                                                                                      
{'loss': 1.2948, 'learning_rate': 0.0002804575707154742, 'epoch': 0.84}                                                                                                       
{'loss': 1.4004, 'learning_rate': 0.000280332778702163, 'epoch': 0.85}                                                                                                        
{'loss': 1.4353, 'learning_rate': 0.0002802079866888519, 'epoch': 0.85}                                                                                                       
{'loss': 1.6908, 'learning_rate': 0.00028008319467554075, 'epoch': 0.86}                                                                                                      
  9%|██████████▉                                                                                                                     | 2100/24540 [2:18:35<2:29:38,  2.50it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8

^CTraceback (most recent call last):█████████████████████▎                                                                                 | 509/1305 [02:18<03:11,  4.16it/s]
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 317, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2964, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 671, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2343, in __getitem__
    return self._getitem(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py", line 2328, in _getitem
    formatted_output = format_table(
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 517, in format_table
    formatted_output = formatter(pa_table_to_format, query_type=query_type)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 282, in __call__
    return self.format_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 311, in format_row
    row = self.python_arrow_extractor().extract_row(pa_table)
  File "/home/or/anaconda3/lib/python3.9/site-packages/datasets/formatting/formatting.py", line 141, in extract_row
    return _unnest(pa_table.to_pydict())
KeyboardInterrupt
  9%|██████████▊                                                                                                                    | 2100/24540 [2:20:54<25:05:40,  4.03s/it]

(base) or@anidjar:~/Desktop/wav2vec2$ python3 main_with_aug.py 
----------------- Checking if cuda is available... -----------------
Cuda Available = True


Using custom data configuration default-21d5287079b7cf06
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-21d5287079b7cf06/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9986.44it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1627.59it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-21d5287079b7cf06/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 492.87it/s]
Using custom data configuration default-dbc311f39ef5c13c
Downloading and preparing dataset csv/default to /home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...
Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12595.51it/s]
Extracting data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2131.25it/s]
Dataset csv downloaded and prepared to /home/or/.cache/huggingface/datasets/csv/default-dbc311f39ef5c13c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1005.83it/s]
Casting the dataset:  75%|████████████████████████████████████████████████████████████████████████████████████████▌                             | 3/4 [00:00<00:00, 21.52ba/s]
Casting the dataset:  50%|███████████████████████████████████████████████████████████                                                           | 1/2 [00:00<00:00, 21.43ba/s]
----------------- Loading Datasets complete. -----------------


----------------- Removing special characters... -----------------
Parameter 'function'=<function remove_special_characters at 0x7efc3c0998b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33649/33649 [00:00<00:00, 39696.59ex/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10438/10438 [00:00<00:00, 40296.88ex/s]
----------------- Removing special characters complete. -----------------


train columns: ['path', 'audio', 'sentence']
test columns: ['path', 'audio', 'sentence']
----------------- Extracting all characters... -----------------
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
  0%|                                                                                                                                                   | 0/1 [00:00<?, ?ba/s]
----------------- Extracting all characters complete. -----------------


----------------- Preparing vocab... -----------------
Vocab_dict: {'ۖ': 0, 'ّ': 1, 'َ': 2, '-': 3, 'ل': 4, 'أ': 5, 'د': 6, 'ه': 7, 'ت': 8, 'ْ': 9, 'ى': 10, 'ؤ': 11, 'd': 12, 'ۛ': 13, 'ض': 14, ' ': 15, 'إ': 16, 'ئ': 17, 'ﻻ': 18, 'ش': 19, 'ة': 20, 'a': 21, 'c': 22, 'ب': 23, 'ۗ': 24, 'i': 25, 'ۚ': 26, 'ی': 27, 'ك': 28, 'm': 29, 'آ': 30, 'ي': 31, 'ک': 32, 't': 33, 'ھ': 34, 'x': 35, 'ـ': 36, 'ن': 37, '☭': 38, 'ٌ': 39, 'ف': 40, 'l': 41, 'م': 42, 'خ': 43, 'ث': 44, 'ُ': 45, 'ء': 46, 'ذ': 47, 'ص': 48, 'ح': 49, 'ٍ': 50, 'ر': 51, 'e': 52, 'ً': 53, 'o': 54, 'n': 55, 'h': 56, 'ۘ': 57, 'ج': 58, 'س': 59, 'ع': 60, 'و': 61, 'ا': 62, 'ٰ': 63, 'ِ': 64, 'ز': 65, 'ڨ': 66, 'چ': 67, 'r': 68, 'ط': 69, 'ﺃ': 70, 'ق': 71, 'ظ': 72, 'غ': 73, 's': 74}
Vocab_len: 77
----------------- Preparing vocab complete. -----------------


----------------- Saving vocab to jason... -----------------
----------------- Saving vocab to jason complete. -----------------


sample path: common_voice_ar_24089467.mp3

Sanity Check sampling rate is 48khz: {'path': '/home/or/Desktop/arabic_new_dataset/train/common_voice_ar_24089467.mp3', 'array': array([-2.84865515e-14, -4.09750879e-12, -5.45291286e-12, ...,
        5.32540653e-05,  9.98841097e-06, -1.20470195e-05], dtype=float32), 'sampling_rate': 48000}

----------------- Resampling to 16khz... -----------------
Making sure the sampling rate changed to 16khz {'path': '/home/or/Desktop/arabic_new_dataset/train/common_voice_ar_24089467.mp3', 'array': array([-1.9609438e-12, -1.9440803e-12, -1.2969034e-12, ...,
        6.6676068e-05,  6.1965700e-05,  2.8741959e-05], dtype=float32), 'sampling_rate': 16000}
----------------- Resampling complete. -----------------


----------------- Preparing datasets... -----------------
#0:   0%|                                                                                                                                            | 0/8413 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/8412 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(

/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                    | 1/8412 [00:00<14:44,  9.51ex/s]
  warnings.warn(
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [14:27<00:00,  9.69ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8413/8413 [14:32<00:00,  9.64ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [14:37<00:00,  9.58ex/s]
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8412/8412 [14:37<00:00,  9.58ex/s]
#0:   0%|                                                                                                                                            | 0/2610 [00:00<?, ?ex/s]
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.%|                                                                                                                                            | 0/2609 [00:00<?, ?ex/s]
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
#3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [04:00<00:00, 10.85ex/s]
#1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [04:01<00:00, 10.80ex/s]
#2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2609/2609 [04:02<00:00, 10.74ex/s]
#0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2610/2610 [04:03<00:00, 10.72ex/s]
#2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2608/2609 [04:02<00:00, 15.25ex/s]

----------------- Preparing datasets complete. -----------------


----------------- saving datasets... -----------------
----------------- saving datasets complete. -----------


----------------- Loading Metrics... -----------------
/home/or/Desktop/wav2vec2/main_with_aug.py:245: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  wer_metric = load_metric("wer")
----------------- Loading Metrics complete. -----------------


----------------- Loading Model... -----------------
Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.bias', 'project_hid.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
----------------- Loading Model complete. -----------------


Using cuda_amp half precision backend
----------------- Training... -----------------
/home/or/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 33649
  Num Epochs = 10
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 21030
  Number of trainable parameters = 311307469
  0%|                                                                                                                                               | 0/21030 [00:00<?, ?it/s]/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 19.4046, 'learning_rate': 4.8e-06, 'epoch': 0.0}                                                                                                                     
{'loss': 21.5384, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.01}                                                                                                     
{'loss': 23.619, 'learning_rate': 1.68e-05, 'epoch': 0.01}                                                                                                                    
{'loss': 25.8732, 'learning_rate': 2.28e-05, 'epoch': 0.02}                                                                                                                   
{'loss': 23.4977, 'learning_rate': 2.7599999999999997e-05, 'epoch': 0.02}                                                                                                     
{'loss': 17.5108, 'learning_rate': 3.36e-05, 'epoch': 0.03}                                                                                                                   
{'loss': 18.913, 'learning_rate': 3.96e-05, 'epoch': 0.03}                                                                                                                    
{'loss': 14.9596, 'learning_rate': 4.56e-05, 'epoch': 0.04}                                                                                                                   
{'loss': 10.7653, 'learning_rate': 5.1599999999999994e-05, 'epoch': 0.04}                                                                                                     
{'loss': 7.9482, 'learning_rate': 5.76e-05, 'epoch': 0.05}                                                                                                                    
  0%|▌                                                                                                                                  | 100/21030 [01:22<2:39:26,  2.19it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 6.398304462432861, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 356.0397, 'eval_samples_per_second': 29.317, 'eval_steps_per_second': 3.665, 'epoch': 0.05}                                                                                                                                                
{'loss': 5.0247, 'learning_rate': 6.359999999999999e-05, 'epoch': 0.05}                                                                                                       
{'loss': 4.8182, 'learning_rate': 6.96e-05, 'epoch': 0.06}                                                                                                                    
{'loss': 4.6666, 'learning_rate': 7.56e-05, 'epoch': 0.06}                                                                                                                    
{'loss': 4.1581, 'learning_rate': 8.16e-05, 'epoch': 0.07}                                                                                                                    
{'loss': 3.9423, 'learning_rate': 8.759999999999999e-05, 'epoch': 0.07}                                                                                                       
{'loss': 3.5647, 'learning_rate': 9.36e-05, 'epoch': 0.08}                                                                                                                    
{'loss': 3.5691, 'learning_rate': 9.96e-05, 'epoch': 0.08}                                                                                                                    
{'loss': 3.4994, 'learning_rate': 0.00010559999999999998, 'epoch': 0.09}                                                                                                      
{'loss': 3.6079, 'learning_rate': 0.00011159999999999999, 'epoch': 0.09}                                                                                                      
{'loss': 3.5212, 'learning_rate': 0.0001176, 'epoch': 0.1}                                                                                                                    
  1%|█▏                                                                                                                                 | 200/21030 [08:37<2:45:58,  2.09it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.433087110519409, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 353.8227, 'eval_samples_per_second': 29.501, 'eval_steps_per_second': 3.688, 'epoch': 0.1}                                                                                                                                                 
{'loss': 3.4201, 'learning_rate': 0.0001236, 'epoch': 0.1}                                                                                                                    
{'loss': 3.4067, 'learning_rate': 0.00012959999999999998, 'epoch': 0.1}                                                                                                       
{'loss': 3.4231, 'learning_rate': 0.0001356, 'epoch': 0.11}                                                                                                                   
{'loss': 3.4722, 'learning_rate': 0.00014159999999999997, 'epoch': 0.11}                                                                                                      
{'loss': 3.4698, 'learning_rate': 0.00014759999999999998, 'epoch': 0.12}                                                                                                      
{'loss': 3.3933, 'learning_rate': 0.0001536, 'epoch': 0.12}                                                                                                                   
{'loss': 3.428, 'learning_rate': 0.0001596, 'epoch': 0.13}                                                                                                                    
{'loss': 3.4305, 'learning_rate': 0.0001656, 'epoch': 0.13}                                                                                                                   
{'loss': 3.4304, 'learning_rate': 0.00017159999999999997, 'epoch': 0.14}                                                                                                      
{'loss': 3.5218, 'learning_rate': 0.00017759999999999998, 'epoch': 0.14}                                                                                                      
  1%|█▊                                                                                                                                 | 300/21030 [15:48<2:43:30,  2.11it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.3916893005371094, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 354.6186, 'eval_samples_per_second': 29.434, 'eval_steps_per_second': 3.68, 'epoch': 0.14}                                                                                                                                                
{'loss': 3.4262, 'learning_rate': 0.0001836, 'epoch': 0.15}                                                                                                                   
{'loss': 3.4067, 'learning_rate': 0.00018959999999999997, 'epoch': 0.15}                                                                                                      
{'loss': 3.3971, 'learning_rate': 0.00019559999999999998, 'epoch': 0.16}                                                                                                      
{'loss': 3.4061, 'learning_rate': 0.0002016, 'epoch': 0.16}                                                                                                                   
{'loss': 3.4538, 'learning_rate': 0.00020759999999999998, 'epoch': 0.17}                                                                                                      
{'loss': 3.4184, 'learning_rate': 0.00021359999999999996, 'epoch': 0.17}                                                                                                      
{'loss': 3.3625, 'learning_rate': 0.00021959999999999997, 'epoch': 0.18}                                                                                                      
{'loss': 3.3828, 'learning_rate': 0.00022559999999999998, 'epoch': 0.18}                                                                                                      
{'loss': 3.4205, 'learning_rate': 0.0002316, 'epoch': 0.19}                                                                                                                   
{'loss': 3.4798, 'learning_rate': 0.0002376, 'epoch': 0.19}                                                                                                                   
  2%|██▍                                                                                                                                | 400/21030 [23:02<2:42:28,  2.12it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.3920040130615234, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 355.3132, 'eval_samples_per_second': 29.377, 'eval_steps_per_second': 3.673, 'epoch': 0.19}                                                                                                                                               
{'loss': 3.3689, 'learning_rate': 0.00024359999999999999, 'epoch': 0.19}                                                                                                      
{'loss': 3.3687, 'learning_rate': 0.00024959999999999994, 'epoch': 0.2}                                                                                                       
{'loss': 3.4009, 'learning_rate': 0.0002556, 'epoch': 0.2}                                                                                                                    
{'loss': 3.426, 'learning_rate': 0.00026159999999999996, 'epoch': 0.21}                                                                                                       
{'loss': 3.3973, 'learning_rate': 0.0002676, 'epoch': 0.21}                                                                                                                   
{'loss': 3.3703, 'learning_rate': 0.0002736, 'epoch': 0.22}                                                                                                                   
{'loss': 3.3422, 'learning_rate': 0.00027959999999999997, 'epoch': 0.22}                                                                                                      
{'loss': 3.3517, 'learning_rate': 0.00028559999999999995, 'epoch': 0.23}                                                                                                      
{'loss': 3.3952, 'learning_rate': 0.0002916, 'epoch': 0.23}                                                                                                                   
{'loss': 3.4155, 'learning_rate': 0.00029759999999999997, 'epoch': 0.24}                                                                                                      
  2%|███                                                                                                                                | 500/21030 [30:17<2:40:51,  2.13it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.3614649772644043, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 352.7879, 'eval_samples_per_second': 29.587, 'eval_steps_per_second': 3.699, 'epoch': 0.24}                                                                                                                                               
{'loss': 3.3434, 'learning_rate': 0.0002999123234291281, 'epoch': 0.24}                                                                                                       
{'loss': 3.3549, 'learning_rate': 0.0002997661958110082, 'epoch': 0.25}                                                                                                       
{'loss': 3.3643, 'learning_rate': 0.0002996200681928884, 'epoch': 0.25}                                                                                                       
{'loss': 3.3216, 'learning_rate': 0.0002994739405747686, 'epoch': 0.26}                                                                                                       
{'loss': 3.3283, 'learning_rate': 0.0002993278129566488, 'epoch': 0.26}                                                                                                       
{'loss': 3.3212, 'learning_rate': 0.00029918168533852896, 'epoch': 0.27}                                                                                                      
{'loss': 3.3016, 'learning_rate': 0.00029903555772040915, 'epoch': 0.27}                                                                                                      
{'loss': 3.3121, 'learning_rate': 0.00029888943010228933, 'epoch': 0.28}                                                                                                      
{'loss': 3.3078, 'learning_rate': 0.00029874330248416946, 'epoch': 0.28}                                                                                                      
{'loss': 3.2255, 'learning_rate': 0.00029859717486604965, 'epoch': 0.29}                                                                                                      
  3%|███▋                                                                                                                               | 600/21030 [37:28<2:44:19,  2.07it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.2649288177490234, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 355.5136, 'eval_samples_per_second': 29.36, 'eval_steps_per_second': 3.671, 'epoch': 0.29}                                                                                                                                                
{'loss': 3.3066, 'learning_rate': 0.00029845104724792983, 'epoch': 0.29}                                                                                                      
{'loss': 3.2799, 'learning_rate': 0.00029830491962981, 'epoch': 0.29}                                                                                                         
{'loss': 3.3188, 'learning_rate': 0.0002981587920116902, 'epoch': 0.3}                                                                                                        
{'loss': 3.2845, 'learning_rate': 0.0002980126643935704, 'epoch': 0.3}                                                                                                        
{'loss': 3.284, 'learning_rate': 0.0002978665367754505, 'epoch': 0.31}                                                                                                        
{'loss': 3.2911, 'learning_rate': 0.0002977204091573307, 'epoch': 0.31}                                                                                                       
{'loss': 3.2875, 'learning_rate': 0.0002975742815392109, 'epoch': 0.32}                                                                                                       
{'loss': 3.2599, 'learning_rate': 0.00029742815392109107, 'epoch': 0.32}                                                                                                      
{'loss': 3.2466, 'learning_rate': 0.0002972820263029712, 'epoch': 0.33}                                                                                                       
{'loss': 3.2657, 'learning_rate': 0.00029713589868485144, 'epoch': 0.33}                                                                                                      
  3%|████▎                                                                                                                              | 700/21030 [44:43<2:40:07,  2.12it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.2435526847839355, 'eval_wer': 0.9999472397601168, 'eval_cer': 0.9999912795514201, 'eval_runtime': 355.3029, 'eval_samples_per_second': 29.378, 'eval_steps_per_second': 3.673, 'epoch': 0.33}                                                                                                                                               
{'loss': 3.2754, 'learning_rate': 0.0002969897710667316, 'epoch': 0.34}                                                                                                       
{'loss': 3.2517, 'learning_rate': 0.00029684364344861175, 'epoch': 0.34}                                                                                                      
{'loss': 3.2887, 'learning_rate': 0.00029669751583049194, 'epoch': 0.35}                                                                                                      
{'loss': 3.2326, 'learning_rate': 0.0002965513882123721, 'epoch': 0.35}                                                                                                       
{'loss': 3.2492, 'learning_rate': 0.0002964052605942523, 'epoch': 0.36}                                                                                                       
{'loss': 3.2157, 'learning_rate': 0.00029625913297613244, 'epoch': 0.36}                                                                                                      
{'loss': 3.2211, 'learning_rate': 0.0002961130053580126, 'epoch': 0.37}                                                                                                       
{'loss': 3.1535, 'learning_rate': 0.0002959668777398928, 'epoch': 0.37}                                                                                                       
{'loss': 3.0837, 'learning_rate': 0.000295820750121773, 'epoch': 0.38}                                                                                                        
{'loss': 3.0439, 'learning_rate': 0.0002956746225036532, 'epoch': 0.38}                                                                                                       
  4%|████▉                                                                                                                              | 800/21030 [51:58<2:37:26,  2.14it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 3.258882522583008, 'eval_wer': 1.0000879337331388, 'eval_cer': 0.9211874925512835, 'eval_runtime': 355.4605, 'eval_samples_per_second': 29.365, 'eval_steps_per_second': 3.671, 'epoch': 0.38}                                                                                                                                                
{'loss': 3.0277, 'learning_rate': 0.00029552849488553336, 'epoch': 0.39}                                                                                                      
{'loss': 2.8581, 'learning_rate': 0.0002953823672674135, 'epoch': 0.39}                                                                                                       
{'loss': 2.669, 'learning_rate': 0.00029523623964929367, 'epoch': 0.39}                                                                                                       
{'loss': 2.6397, 'learning_rate': 0.00029509011203117386, 'epoch': 0.4}                                                                                                       
{'loss': 2.6171, 'learning_rate': 0.00029494398441305404, 'epoch': 0.4}                                                                                                       
{'loss': 2.4107, 'learning_rate': 0.0002947978567949342, 'epoch': 0.41}                                                                                                       
{'loss': 2.2118, 'learning_rate': 0.0002946517291768144, 'epoch': 0.41}                                                                                                       
{'loss': 2.0832, 'learning_rate': 0.0002945056015586946, 'epoch': 0.42}                                                                                                       
{'loss': 2.0608, 'learning_rate': 0.0002943594739405747, 'epoch': 0.42}                                                                                                       
{'loss': 2.2483, 'learning_rate': 0.0002942133463224549, 'epoch': 0.43}                                                                                                       
  4%|█████▌                                                                                                                             | 900/21030 [59:12<2:38:28,  2.12it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.7354234457015991, 'eval_wer': 1.0346810643499058, 'eval_cer': 0.5447518887038216, 'eval_runtime': 357.5581, 'eval_samples_per_second': 29.192, 'eval_steps_per_second': 3.65, 'epoch': 0.43}                                                                                                                                                
{'loss': 1.9164, 'learning_rate': 0.0002940672187043351, 'epoch': 0.43}                                                                                                       
{'loss': 1.8301, 'learning_rate': 0.0002939210910862153, 'epoch': 0.44}                                                                                                       
{'loss': 1.7909, 'learning_rate': 0.00029377496346809546, 'epoch': 0.44}                                                                                                      
{'loss': 1.8266, 'learning_rate': 0.00029362883584997565, 'epoch': 0.45}                                                                                                      
{'loss': 1.9285, 'learning_rate': 0.0002934827082318558, 'epoch': 0.45}                                                                                                       
{'loss': 1.7177, 'learning_rate': 0.00029333658061373596, 'epoch': 0.46}                                                                                                      
{'loss': 1.6688, 'learning_rate': 0.00029319045299561615, 'epoch': 0.46}                                                                                                      
{'loss': 1.5995, 'learning_rate': 0.00029304432537749633, 'epoch': 0.47}                                                                                                      
{'loss': 1.6001, 'learning_rate': 0.00029289819775937646, 'epoch': 0.47}                                                                                                      
{'loss': 1.9464, 'learning_rate': 0.00029275207014125665, 'epoch': 0.48}                                                                                                      
  5%|██████                                                                                                                          | 1000/21030 [1:06:28<2:38:16,  2.11it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.2993113994598389, 'eval_wer': 0.9677459066847224, 'eval_cer': 0.46003854438272307, 'eval_runtime': 356.6546, 'eval_samples_per_second': 29.266, 'eval_steps_per_second': 3.659, 'epoch': 0.48}                                                                                                                                              
{'loss': 1.5885, 'learning_rate': 0.0002926059425231369, 'epoch': 0.48}                                                                                                       
{'loss': 1.5077, 'learning_rate': 0.000292459814905017, 'epoch': 0.48}                                                                                                        
{'loss': 1.5078, 'learning_rate': 0.0002923136872868972, 'epoch': 0.49}                                                                                                       
{'loss': 1.5366, 'learning_rate': 0.0002921675596687774, 'epoch': 0.49}                                                                                                       
{'loss': 1.7933, 'learning_rate': 0.00029202143205065757, 'epoch': 0.5}                                                                                                       
{'loss': 1.4468, 'learning_rate': 0.0002918753044325377, 'epoch': 0.5}                                                                                                        
{'loss': 1.394, 'learning_rate': 0.0002917291768144179, 'epoch': 0.51}                                                                                                        
{'loss': 1.3934, 'learning_rate': 0.00029158304919629807, 'epoch': 0.51}                                                                                                      
{'loss': 1.4996, 'learning_rate': 0.00029143692157817825, 'epoch': 0.52}                                                                                                      
{'loss': 1.6919, 'learning_rate': 0.00029129079396005844, 'epoch': 0.52}                                                                                                      
  5%|██████▋                                                                                                                         | 1100/21030 [1:13:44<2:39:11,  2.09it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.0789254903793335, 'eval_wer': 0.9566486695626176, 'eval_cer': 0.41752054392344606, 'eval_runtime': 355.8576, 'eval_samples_per_second': 29.332, 'eval_steps_per_second': 3.667, 'epoch': 0.52}                                                                                                                                              
{'loss': 1.3065, 'learning_rate': 0.0002911446663419386, 'epoch': 0.53}                                                                                                       
{'loss': 1.3816, 'learning_rate': 0.00029099853872381875, 'epoch': 0.53}                                                                                                      
{'loss': 1.3181, 'learning_rate': 0.00029085241110569893, 'epoch': 0.54}                                                                                                      
{'loss': 1.4434, 'learning_rate': 0.0002907062834875791, 'epoch': 0.54}                                                                                                       
{'loss': 1.6618, 'learning_rate': 0.0002905601558694593, 'epoch': 0.55}                                                                                                       
{'loss': 1.3044, 'learning_rate': 0.0002904140282513395, 'epoch': 0.55}                                                                                                       
{'loss': 1.2676, 'learning_rate': 0.00029026790063321967, 'epoch': 0.56}                                                                                                      
{'loss': 1.3306, 'learning_rate': 0.00029012177301509986, 'epoch': 0.56}                                                                                                      
{'loss': 1.3271, 'learning_rate': 0.00028997564539698, 'epoch': 0.57}                                                                                                         
{'loss': 1.5634, 'learning_rate': 0.00028982951777886017, 'epoch': 0.57}                                                                                                      
  6%|███████▎                                                                                                                        | 1200/21030 [1:20:59<2:43:00,  2.03it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.9978443384170532, 'eval_wer': 0.9341552206257364, 'eval_cer': 0.3872838418808264, 'eval_runtime': 358.3893, 'eval_samples_per_second': 29.125, 'eval_steps_per_second': 3.641, 'epoch': 0.57}                                                                                                                                               
{'loss': 1.26, 'learning_rate': 0.00028968339016074036, 'epoch': 0.58}                                                                                                        
{'loss': 1.1768, 'learning_rate': 0.00028953726254262054, 'epoch': 0.58}                                                                                                      
{'loss': 1.2246, 'learning_rate': 0.00028939113492450067, 'epoch': 0.58}                                                                                                      
{'loss': 1.2703, 'learning_rate': 0.0002892450073063809, 'epoch': 0.59}                                                                                                       
{'loss': 1.4573, 'learning_rate': 0.00028909887968826104, 'epoch': 0.59}                                                                                                      
{'loss': 1.2034, 'learning_rate': 0.0002889527520701412, 'epoch': 0.6}                                                                                                        
{'loss': 1.1432, 'learning_rate': 0.0002888066244520214, 'epoch': 0.6}                                                                                                        
{'loss': 1.248, 'learning_rate': 0.0002886604968339016, 'epoch': 0.61}                                                                                                        
{'loss': 1.2869, 'learning_rate': 0.0002885143692157817, 'epoch': 0.61}                                                                                                       
{'loss': 1.5394, 'learning_rate': 0.0002883682415976619, 'epoch': 0.62}                                                                                                       
  6%|███████▉                                                                                                                        | 1300/21030 [1:28:17<2:46:29,  1.98it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 1.0045530796051025, 'eval_wer': 0.9229172895306097, 'eval_cer': 0.36754946674456934, 'eval_runtime': 356.6993, 'eval_samples_per_second': 29.263, 'eval_steps_per_second': 3.659, 'epoch': 0.62}                                                                                                                                              
{'loss': 1.1957, 'learning_rate': 0.00028822211397954215, 'epoch': 0.62}                                                                                                      
{'loss': 1.2211, 'learning_rate': 0.0002880759863614223, 'epoch': 0.63}                                                                                                       
{'loss': 1.1885, 'learning_rate': 0.00028792985874330246, 'epoch': 0.63}                                                                                                      
{'loss': 1.2126, 'learning_rate': 0.00028778373112518265, 'epoch': 0.64}                                                                                                      
{'loss': 1.4165, 'learning_rate': 0.00028763760350706283, 'epoch': 0.64}                                                                                                      
{'loss': 1.1123, 'learning_rate': 0.00028749147588894296, 'epoch': 0.65}                                                                                                      
{'loss': 1.1804, 'learning_rate': 0.00028734534827082314, 'epoch': 0.65}                                                                                                      
{'loss': 1.2008, 'learning_rate': 0.00028719922065270333, 'epoch': 0.66}                                                                                                      
{'loss': 1.1784, 'learning_rate': 0.0002870530930345835, 'epoch': 0.66}                                                                                                       
{'loss': 1.3815, 'learning_rate': 0.0002869069654164637, 'epoch': 0.67}                                                                                                       
  7%|████████▌                                                                                                                       | 1400/21030 [1:35:34<2:42:54,  2.01it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.8646702170372009, 'eval_wer': 0.8952709238318004, 'eval_cer': 0.3503759966746023, 'eval_runtime': 356.4138, 'eval_samples_per_second': 29.286, 'eval_steps_per_second': 3.661, 'epoch': 0.67}                                                                                                                                               
{'loss': 1.0418, 'learning_rate': 0.0002867608377983439, 'epoch': 0.67}                                                                                                       
{'loss': 1.1134, 'learning_rate': 0.000286614710180224, 'epoch': 0.68}                                                                                                        
{'loss': 1.1684, 'learning_rate': 0.0002864685825621042, 'epoch': 0.68}                                                                                                       
{'loss': 1.1931, 'learning_rate': 0.0002863224549439844, 'epoch': 0.68}                                                                                                       
{'loss': 1.3818, 'learning_rate': 0.00028617632732586457, 'epoch': 0.69}                                                                                                      
{'loss': 1.1146, 'learning_rate': 0.0002860301997077447, 'epoch': 0.69}                                                                                                       
{'loss': 1.121, 'learning_rate': 0.00028588407208962494, 'epoch': 0.7}                                                                                                        
{'loss': 1.115, 'learning_rate': 0.0002857379444715051, 'epoch': 0.7}                                                                                                         
{'loss': 1.1938, 'learning_rate': 0.00028559181685338525, 'epoch': 0.71}                                                                                                      
{'loss': 1.4328, 'learning_rate': 0.00028544568923526543, 'epoch': 0.71}                                                                                                      
  7%|█████████▏                                                                                                                      | 1500/21030 [1:42:50<2:37:40,  2.06it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.8292858004570007, 'eval_wer': 0.8870403264100174, 'eval_cer': 0.3552391001659792, 'eval_runtime': 358.5361, 'eval_samples_per_second': 29.113, 'eval_steps_per_second': 3.64, 'epoch': 0.71}                                                                                                                                                
{'loss': 1.1704, 'learning_rate': 0.0002852995616171456, 'epoch': 0.72}                                                                                                       
{'loss': 1.0811, 'learning_rate': 0.0002851534339990258, 'epoch': 0.72}                                                                                                       
{'loss': 1.0759, 'learning_rate': 0.00028500730638090593, 'epoch': 0.73}                                                                                                      
{'loss': 1.0834, 'learning_rate': 0.00028486117876278617, 'epoch': 0.73}                                                                                                      
{'loss': 1.4626, 'learning_rate': 0.0002847150511446663, 'epoch': 0.74}                                                                                                       
{'loss': 1.064, 'learning_rate': 0.0002845689235265465, 'epoch': 0.74}                                                                                                        
{'loss': 1.0493, 'learning_rate': 0.00028442279590842667, 'epoch': 0.75}                                                                                                      
{'loss': 1.0819, 'learning_rate': 0.00028427666829030686, 'epoch': 0.75}                                                                                                      
{'loss': 1.1453, 'learning_rate': 0.000284130540672187, 'epoch': 0.76}                                                                                                        
{'loss': 1.3566, 'learning_rate': 0.00028398441305406717, 'epoch': 0.76}                                                                                                      
  8%|█████████▋                                                                                                                      | 1600/21030 [1:50:09<2:40:43,  2.01it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.9383103847503662, 'eval_wer': 0.9042929248518317, 'eval_cer': 0.34031841264581314, 'eval_runtime': 358.284, 'eval_samples_per_second': 29.133, 'eval_steps_per_second': 3.642, 'epoch': 0.76}                                                                                                                                               
{'loss': 1.0078, 'learning_rate': 0.00028383828543594735, 'epoch': 0.77}                                                                                                      
{'loss': 1.072, 'learning_rate': 0.00028369215781782754, 'epoch': 0.77}                                                                                                       
{'loss': 1.0368, 'learning_rate': 0.0002835460301997077, 'epoch': 0.77}                                                                                                       
{'loss': 1.1053, 'learning_rate': 0.0002833999025815879, 'epoch': 0.78}                                                                                                       
{'loss': 1.3999, 'learning_rate': 0.0002832537749634681, 'epoch': 0.78}                                                                                                       
{'loss': 1.0461, 'learning_rate': 0.0002831076473453482, 'epoch': 0.79}                                                                                                       
{'loss': 0.9996, 'learning_rate': 0.0002829615197272284, 'epoch': 0.79}                                                                                                       
{'loss': 1.0641, 'learning_rate': 0.0002828153921091086, 'epoch': 0.8}                                                                                                        
{'loss': 1.046, 'learning_rate': 0.0002826692644909888, 'epoch': 0.8}                                                                                                         
{'loss': 1.2868, 'learning_rate': 0.00028252313687286896, 'epoch': 0.81}                                                                                                      
  8%|██████████▎                                                                                                                     | 1700/21030 [1:57:28<2:41:29,  1.99it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7429038286209106, 'eval_wer': 0.8533617066178928, 'eval_cer': 0.3282696595246193, 'eval_runtime': 358.3035, 'eval_samples_per_second': 29.132, 'eval_steps_per_second': 3.642, 'epoch': 0.81}                                                                                                                                               
{'loss': 1.0218, 'learning_rate': 0.00028237700925474914, 'epoch': 0.81}                                                                                                      
{'loss': 0.9769, 'learning_rate': 0.00028223088163662933, 'epoch': 0.82}                                                                                                      
{'loss': 0.987, 'learning_rate': 0.00028208475401850946, 'epoch': 0.82}                                                                                                       
{'loss': 1.1297, 'learning_rate': 0.00028193862640038964, 'epoch': 0.83}                                                                                                      
{'loss': 1.3488, 'learning_rate': 0.00028179249878226983, 'epoch': 0.83}                                                                                                      
{'loss': 0.9846, 'learning_rate': 0.00028164637116415, 'epoch': 0.84}                                                                                                         
{'loss': 1.0168, 'learning_rate': 0.0002815002435460302, 'epoch': 0.84}                                                                                                       
{'loss': 1.0747, 'learning_rate': 0.0002813541159279104, 'epoch': 0.85}                                                                                                       
{'loss': 1.1314, 'learning_rate': 0.0002812079883097905, 'epoch': 0.85}                                                                                                       
{'loss': 1.2251, 'learning_rate': 0.0002810618606916707, 'epoch': 0.86}                                                                                                       
  9%|██████████▉                                                                                                                     | 1800/21030 [2:04:47<2:40:19,  2.00it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7541772127151489, 'eval_wer': 0.8536430945639366, 'eval_cer': 0.33016199686645215, 'eval_runtime': 357.7513, 'eval_samples_per_second': 29.177, 'eval_steps_per_second': 3.648, 'epoch': 0.86}                                                                                                                                              
{'loss': 0.9377, 'learning_rate': 0.0002809157330735509, 'epoch': 0.86}                                                                                                       
{'loss': 0.9387, 'learning_rate': 0.00028076960545543107, 'epoch': 0.87}                                                                                                      
{'loss': 1.0227, 'learning_rate': 0.0002806234778373112, 'epoch': 0.87}                                                                                                       
{'loss': 1.0831, 'learning_rate': 0.0002804773502191914, 'epoch': 0.87}                                                                                                       
{'loss': 1.2262, 'learning_rate': 0.0002803312226010716, 'epoch': 0.88}                                                                                                       
{'loss': 0.9308, 'learning_rate': 0.00028018509498295175, 'epoch': 0.88}                                                                                                      
{'loss': 1.0331, 'learning_rate': 0.00028003896736483193, 'epoch': 0.89}                                                                                                      
{'loss': 1.0003, 'learning_rate': 0.0002798928397467121, 'epoch': 0.89}                                                                                                       
{'loss': 0.9948, 'learning_rate': 0.0002797467121285923, 'epoch': 0.9}                                                                                                        
{'loss': 1.324, 'learning_rate': 0.00027960058451047243, 'epoch': 0.9}                                                                                                        
  9%|███████████▌                                                                                                                    | 1900/21030 [2:12:03<2:37:31,  2.02it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.816363513469696, 'eval_wer': 0.8625419883575738, 'eval_cer': 0.3186219365790843, 'eval_runtime': 358.7917, 'eval_samples_per_second': 29.092, 'eval_steps_per_second': 3.637, 'epoch': 0.9}                                                                                                                                                 
{'loss': 0.9799, 'learning_rate': 0.0002794544568923526, 'epoch': 0.91}                                                                                                       
{'loss': 0.9566, 'learning_rate': 0.0002793083292742328, 'epoch': 0.91}                                                                                                       
{'loss': 0.961, 'learning_rate': 0.000279162201656113, 'epoch': 0.92}                                                                                                         
{'loss': 1.0471, 'learning_rate': 0.00027901607403799317, 'epoch': 0.92}                                                                                                      
{'loss': 1.1563, 'learning_rate': 0.00027886994641987335, 'epoch': 0.93}                                                                                                      
{'loss': 0.9284, 'learning_rate': 0.0002787238188017535, 'epoch': 0.93}                                                                                                       
{'loss': 0.9212, 'learning_rate': 0.00027857769118363367, 'epoch': 0.94}                                                                                                      
{'loss': 0.9813, 'learning_rate': 0.00027843156356551385, 'epoch': 0.94}                                                                                                      
{'loss': 0.9526, 'learning_rate': 0.00027828543594739404, 'epoch': 0.95}                                                                                                      
{'loss': 1.3104, 'learning_rate': 0.0002781393083292742, 'epoch': 0.95}                                                                                                       
 10%|████████████▏                                                                                                                   | 2000/21030 [2:19:21<2:35:14,  2.04it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7698689699172974, 'eval_wer': 0.836214628655845, 'eval_cer': 0.30342219470436227, 'eval_runtime': 358.4252, 'eval_samples_per_second': 29.122, 'eval_steps_per_second': 3.641, 'epoch': 0.95}                                                                                                                                               
{'loss': 0.9172, 'learning_rate': 0.0002779931807111544, 'epoch': 0.96}                                                                                                       
{'loss': 0.917, 'learning_rate': 0.0002778470530930346, 'epoch': 0.96}                                                                                                        
{'loss': 0.9569, 'learning_rate': 0.0002777009254749147, 'epoch': 0.97}                                                                                                       
{'loss': 1.0091, 'learning_rate': 0.0002775547978567949, 'epoch': 0.97}                                                                                                       
{'loss': 1.3156, 'learning_rate': 0.0002774086702386751, 'epoch': 0.97}                                                                                                       
{'loss': 0.9284, 'learning_rate': 0.0002772625426205553, 'epoch': 0.98}                                                                                                       
{'loss': 0.9328, 'learning_rate': 0.0002771164150024354, 'epoch': 0.98}                                                                                                       
{'loss': 0.9655, 'learning_rate': 0.00027697028738431564, 'epoch': 0.99}                                                                                                      
{'loss': 1.0738, 'learning_rate': 0.0002768241597661958, 'epoch': 0.99}                                                                                                       
{'loss': 1.2211, 'learning_rate': 0.00027667803214807596, 'epoch': 1.0}                                                                                                       
 10%|████████████▊                                                                                                                   | 2100/21030 [2:26:39<2:31:08,  2.09it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.853345513343811, 'eval_wer': 0.849949877772111, 'eval_cer': 0.3229269313613492, 'eval_runtime': 357.4832, 'eval_samples_per_second': 29.199, 'eval_steps_per_second': 3.651, 'epoch': 1.0}                                                                                                                                                  
 10%|████████████▌                                                                                                                 | 2104/21030 [2:32:43<201:08:15, 38.26s/it]Saving model checkpoint to ./20_band/checkpoint-2104                                                                                                                          
Configuration saved in ./20_band/checkpoint-2104/config.json
Model weights saved in ./20_band/checkpoint-2104/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-2104/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.1111, 'learning_rate': 0.00027653190452995614, 'epoch': 1.0}                                                                                                       
{'loss': 0.8081, 'learning_rate': 0.00027638577691183633, 'epoch': 1.01}                                                                                                      
{'loss': 0.8894, 'learning_rate': 0.00027623964929371646, 'epoch': 1.01}                                                                                                      
{'loss': 1.0425, 'learning_rate': 0.00027609352167559664, 'epoch': 1.02}                                                                                                      
{'loss': 1.0079, 'learning_rate': 0.0002759473940574768, 'epoch': 1.02}                                                                                                       
{'loss': 1.0553, 'learning_rate': 0.000275801266439357, 'epoch': 1.03}                                                                                                        
{'loss': 0.9141, 'learning_rate': 0.0002756551388212372, 'epoch': 1.03}                                                                                                       
{'loss': 0.8716, 'learning_rate': 0.0002755090112031174, 'epoch': 1.04}                                                                                                       
{'loss': 0.9368, 'learning_rate': 0.00027536288358499756, 'epoch': 1.04}                                                                                                      
{'loss': 0.9317, 'learning_rate': 0.0002752167559668777, 'epoch': 1.05}                                                                                                       
 10%|█████████████▍                                                                                                                  | 2200/21030 [2:34:02<2:53:28,  1.81it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6996815800666809, 'eval_wer': 0.8316948347725154, 'eval_cer': 0.30523895482516955, 'eval_runtime': 359.775, 'eval_samples_per_second': 29.013, 'eval_steps_per_second': 3.627, 'epoch': 1.05}                                                                                                                                               
{'loss': 1.0313, 'learning_rate': 0.0002750706283487579, 'epoch': 1.05}                                                                                                       
{'loss': 0.8188, 'learning_rate': 0.00027492450073063806, 'epoch': 1.06}                                                                                                      
{'loss': 0.8424, 'learning_rate': 0.00027477837311251825, 'epoch': 1.06}                                                                                                      
{'loss': 0.8632, 'learning_rate': 0.00027463224549439843, 'epoch': 1.07}                                                                                                      
{'loss': 1.0947, 'learning_rate': 0.0002744861178762786, 'epoch': 1.07}                                                                                                       
{'loss': 1.0571, 'learning_rate': 0.00027433999025815875, 'epoch': 1.07}                                                                                                      
{'loss': 0.9041, 'learning_rate': 0.00027419386264003893, 'epoch': 1.08}                                                                                                      
{'loss': 0.8781, 'learning_rate': 0.0002740477350219191, 'epoch': 1.08}                                                                                                       
{'loss': 0.9466, 'learning_rate': 0.0002739016074037993, 'epoch': 1.09}                                                                                                       
{'loss': 1.0352, 'learning_rate': 0.00027375547978567943, 'epoch': 1.09}                                                                                                      
 11%|█████████████▉                                                                                                                  | 2300/21030 [2:41:23<2:47:08,  1.87it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7117627859115601, 'eval_wer': 0.8185047748017095, 'eval_cer': 0.30249782715489554, 'eval_runtime': 359.3837, 'eval_samples_per_second': 29.044, 'eval_steps_per_second': 3.631, 'epoch': 1.09}                                                                                                                                              
{'loss': 1.054, 'learning_rate': 0.00027360935216755967, 'epoch': 1.1}                                                                                                        
{'loss': 0.8171, 'learning_rate': 0.00027346322454943985, 'epoch': 1.1}                                                                                                       
{'loss': 0.8395, 'learning_rate': 0.00027331709693132, 'epoch': 1.11}                                                                                                         
{'loss': 0.8365, 'learning_rate': 0.00027317096931320017, 'epoch': 1.11}                                                                                                      
{'loss': 0.9848, 'learning_rate': 0.00027302484169508035, 'epoch': 1.12}                                                                                                      
{'loss': 0.9274, 'learning_rate': 0.00027287871407696054, 'epoch': 1.12}                                                                                                      
{'loss': 0.8023, 'learning_rate': 0.00027273258645884067, 'epoch': 1.13}                                                                                                      
{'loss': 0.8902, 'learning_rate': 0.00027258645884072085, 'epoch': 1.13}                                                                                                      
{'loss': 0.9038, 'learning_rate': 0.00027244033122260104, 'epoch': 1.14}                                                                                                      
{'loss': 0.9317, 'learning_rate': 0.0002722942036044812, 'epoch': 1.14}                                                                                                       
 11%|██████████████▌                                                                                                                 | 2400/21030 [2:48:42<2:51:37,  1.81it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6763779520988464, 'eval_wer': 0.8210900265559874, 'eval_cer': 0.30257921800830767, 'eval_runtime': 359.4018, 'eval_samples_per_second': 29.043, 'eval_steps_per_second': 3.631, 'epoch': 1.14}                                                                                                                                              
{'loss': 0.9823, 'learning_rate': 0.0002721480759863614, 'epoch': 1.15}                                                                                                       
{'loss': 0.8972, 'learning_rate': 0.0002720019483682416, 'epoch': 1.15}                                                                                                       
{'loss': 0.8659, 'learning_rate': 0.0002718558207501217, 'epoch': 1.16}                                                                                                       
{'loss': 0.9361, 'learning_rate': 0.0002717096931320019, 'epoch': 1.16}                                                                                                       
{'loss': 0.9787, 'learning_rate': 0.0002715635655138821, 'epoch': 1.16}                                                                                                       
{'loss': 0.9076, 'learning_rate': 0.0002714174378957623, 'epoch': 1.17}                                                                                                       
{'loss': 0.8181, 'learning_rate': 0.00027127131027764246, 'epoch': 1.17}                                                                                                      
{'loss': 0.868, 'learning_rate': 0.00027112518265952264, 'epoch': 1.18}                                                                                                       
{'loss': 0.9375, 'learning_rate': 0.00027097905504140283, 'epoch': 1.18}                                                                                                      
{'loss': 1.0229, 'learning_rate': 0.00027083292742328296, 'epoch': 1.19}                                                                                                      
 12%|███████████████▏                                                                                                                | 2500/21030 [2:56:02<2:53:27,  1.78it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.657703697681427, 'eval_wer': 0.8008652679340849, 'eval_cer': 0.30015493330310244, 'eval_runtime': 359.1765, 'eval_samples_per_second': 29.061, 'eval_steps_per_second': 3.633, 'epoch': 1.19}                                                                                                                                               
{'loss': 0.9608, 'learning_rate': 0.00027068679980516314, 'epoch': 1.19}                                                                                                      
{'loss': 0.8763, 'learning_rate': 0.0002705406721870433, 'epoch': 1.2}                                                                                                        
{'loss': 0.8447, 'learning_rate': 0.0002703945445689235, 'epoch': 1.2}                                                                                                        
{'loss': 0.8392, 'learning_rate': 0.0002702484169508037, 'epoch': 1.21}                                                                                                       
{'loss': 0.8932, 'learning_rate': 0.0002701022893326839, 'epoch': 1.21}                                                                                                       
{'loss': 0.8955, 'learning_rate': 0.000269956161714564, 'epoch': 1.22}                                                                                                        
{'loss': 0.8444, 'learning_rate': 0.0002698100340964442, 'epoch': 1.22}                                                                                                       
{'loss': 0.8457, 'learning_rate': 0.0002696639064783244, 'epoch': 1.23}                                                                                                       
{'loss': 0.8604, 'learning_rate': 0.00026951777886020456, 'epoch': 1.23}                                                                                                      
{'loss': 1.0113, 'learning_rate': 0.0002693716512420847, 'epoch': 1.24}                                                                                                       
 12%|███████████████▊                                                                                                                | 2600/21030 [3:03:21<2:54:20,  1.76it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6522579789161682, 'eval_wer': 0.8127890821476935, 'eval_cer': 0.3004804967167511, 'eval_runtime': 358.0632, 'eval_samples_per_second': 29.151, 'eval_steps_per_second': 3.645, 'epoch': 1.24}                                                                                                                                               
{'loss': 1.0181, 'learning_rate': 0.0002692255236239649, 'epoch': 1.24}                                                                                                       
{'loss': 0.8338, 'learning_rate': 0.0002690793960058451, 'epoch': 1.25}                                                                                                       
{'loss': 0.8127, 'learning_rate': 0.00026893326838772525, 'epoch': 1.25}                                                                                                      
{'loss': 0.8411, 'learning_rate': 0.00026878714076960543, 'epoch': 1.26}                                                                                                      
{'loss': 1.0231, 'learning_rate': 0.0002686410131514856, 'epoch': 1.26}                                                                                                       
{'loss': 1.0322, 'learning_rate': 0.0002684948855333658, 'epoch': 1.26}                                                                                                       
{'loss': 0.8611, 'learning_rate': 0.00026834875791524593, 'epoch': 1.27}                                                                                                      
{'loss': 0.8555, 'learning_rate': 0.0002682026302971261, 'epoch': 1.27}                                                                                                       
{'loss': 0.9521, 'learning_rate': 0.0002680565026790063, 'epoch': 1.28}                                                                                                       
{'loss': 1.0387, 'learning_rate': 0.0002679103750608865, 'epoch': 1.28}                                                                                                       
 13%|████████████████▍                                                                                                               | 2700/21030 [3:10:40<2:47:27,  1.82it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6734634637832642, 'eval_wer': 0.8119625050561896, 'eval_cer': 0.3058464794095675, 'eval_runtime': 358.0677, 'eval_samples_per_second': 29.151, 'eval_steps_per_second': 3.645, 'epoch': 1.28}                                                                                                                                               
{'loss': 0.9744, 'learning_rate': 0.00026776424744276667, 'epoch': 1.29}                                                                                                      
{'loss': 0.7977, 'learning_rate': 0.00026761811982464685, 'epoch': 1.29}                                                                                                      
{'loss': 0.8208, 'learning_rate': 0.000267471992206527, 'epoch': 1.3}                                                                                                         
{'loss': 0.8526, 'learning_rate': 0.00026732586458840717, 'epoch': 1.3}                                                                                                       
{'loss': 0.9948, 'learning_rate': 0.00026717973697028735, 'epoch': 1.31}                                                                                                      
{'loss': 1.0519, 'learning_rate': 0.00026703360935216754, 'epoch': 1.31}                                                                                                      
{'loss': 0.8152, 'learning_rate': 0.0002668874817340477, 'epoch': 1.32}                                                                                                       
{'loss': 0.8144, 'learning_rate': 0.0002667413541159279, 'epoch': 1.32}                                                                                                       
{'loss': 0.9498, 'learning_rate': 0.0002665952264978081, 'epoch': 1.33}                                                                                                       
{'loss': 1.0055, 'learning_rate': 0.0002664490988796882, 'epoch': 1.33}                                                                                                       
 13%|█████████████████                                                                                                               | 2800/21030 [3:17:58<2:49:37,  1.79it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6743152141571045, 'eval_wer': 0.7846502875433073, 'eval_cer': 0.2888852069217107, 'eval_runtime': 359.6684, 'eval_samples_per_second': 29.021, 'eval_steps_per_second': 3.628, 'epoch': 1.33}                                                                                                                                               
{'loss': 0.9434, 'learning_rate': 0.0002663029712615684, 'epoch': 1.34}                                                                                                       
{'loss': 0.8586, 'learning_rate': 0.0002661568436434486, 'epoch': 1.34}                                                                                                       
{'loss': 0.7501, 'learning_rate': 0.0002660107160253288, 'epoch': 1.35}                                                                                                       
{'loss': 0.9815, 'learning_rate': 0.0002658645884072089, 'epoch': 1.35}                                                                                                       
{'loss': 1.0151, 'learning_rate': 0.00026571846078908914, 'epoch': 1.36}                                                                                                      
{'loss': 0.947, 'learning_rate': 0.00026557233317096927, 'epoch': 1.36}                                                                                                       
{'loss': 0.7603, 'learning_rate': 0.00026542620555284946, 'epoch': 1.36}                                                                                                      
{'loss': 0.802, 'learning_rate': 0.00026528007793472964, 'epoch': 1.37}                                                                                                       
{'loss': 0.8529, 'learning_rate': 0.0002651339503166098, 'epoch': 1.37}                                                                                                       
{'loss': 0.9261, 'learning_rate': 0.00026498782269848996, 'epoch': 1.38}                                                                                                      
 14%|█████████████████▋                                                                                                              | 2900/21030 [3:25:18<2:44:36,  1.84it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6754121780395508, 'eval_wer': 0.816218497740103, 'eval_cer': 0.29159145279766524, 'eval_runtime': 360.9509, 'eval_samples_per_second': 28.918, 'eval_steps_per_second': 3.615, 'epoch': 1.38}                                                                                                                                               
{'loss': 0.9473, 'learning_rate': 0.00026484169508037014, 'epoch': 1.38}                                                                                                      
{'loss': 0.7652, 'learning_rate': 0.0002646955674622504, 'epoch': 1.39}                                                                                                       
{'loss': 0.8367, 'learning_rate': 0.0002645494398441305, 'epoch': 1.39}                                                                                                       
{'loss': 0.8893, 'learning_rate': 0.0002644033122260107, 'epoch': 1.4}                                                                                                        
{'loss': 0.9129, 'learning_rate': 0.0002642571846078909, 'epoch': 1.4}                                                                                                        
{'loss': 0.9135, 'learning_rate': 0.00026411105698977106, 'epoch': 1.41}                                                                                                      
{'loss': 0.8562, 'learning_rate': 0.0002639649293716512, 'epoch': 1.41}                                                                                                       
{'loss': 0.7782, 'learning_rate': 0.0002638188017535314, 'epoch': 1.42}                                                                                                       
{'loss': 0.8355, 'learning_rate': 0.00026367267413541156, 'epoch': 1.42}                                                                                                      
{'loss': 0.8953, 'learning_rate': 0.00026352654651729175, 'epoch': 1.43}                                                                                                      
 14%|██████████████████▎                                                                                                             | 3000/21030 [3:32:41<2:51:01,  1.76it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.7341075539588928, 'eval_wer': 0.8341393925537715, 'eval_cer': 0.31143628694926734, 'eval_runtime': 358.4365, 'eval_samples_per_second': 29.121, 'eval_steps_per_second': 3.641, 'epoch': 1.43}                                                                                                                                              
{'loss': 0.9528, 'learning_rate': 0.00026338041889917193, 'epoch': 1.43}                                                                                                      
{'loss': 0.796, 'learning_rate': 0.0002632342912810521, 'epoch': 1.44}                                                                                                        
{'loss': 0.812, 'learning_rate': 0.00026308816366293225, 'epoch': 1.44}                                                                                                       
{'loss': 0.7471, 'learning_rate': 0.00026294203604481243, 'epoch': 1.45}                                                                                                      
{'loss': 0.9273, 'learning_rate': 0.0002627959084266926, 'epoch': 1.45}                                                                                                       
{'loss': 0.8979, 'learning_rate': 0.0002626497808085728, 'epoch': 1.45}                                                                                                       
{'loss': 0.7927, 'learning_rate': 0.00026250365319045293, 'epoch': 1.46}                                                                                                      
{'loss': 0.8115, 'learning_rate': 0.00026235752557233317, 'epoch': 1.46}                                                                                                      
{'loss': 0.8111, 'learning_rate': 0.00026221139795421335, 'epoch': 1.47}                                                                                                      
{'loss': 0.917, 'learning_rate': 0.0002620652703360935, 'epoch': 1.47}                                                                                                        
 15%|██████████████████▊                                                                                                             | 3100/21030 [3:40:02<2:39:41,  1.87it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6120343208312988, 'eval_wer': 0.7763141696417579, 'eval_cer': 0.28723122850772775, 'eval_runtime': 360.0199, 'eval_samples_per_second': 28.993, 'eval_steps_per_second': 3.625, 'epoch': 1.47}                                                                                                                                              
{'loss': 0.9553, 'learning_rate': 0.00026191914271797367, 'epoch': 1.48}                                                                                                      
{'loss': 0.7635, 'learning_rate': 0.00026177301509985385, 'epoch': 1.48}                                                                                                      
{'loss': 0.7858, 'learning_rate': 0.00026162688748173404, 'epoch': 1.49}                                                                                                      
{'loss': 0.8564, 'learning_rate': 0.00026148075986361417, 'epoch': 1.49}                                                                                                      
{'loss': 0.8379, 'learning_rate': 0.0002613346322454944, 'epoch': 1.5}                                                                                                        
{'loss': 0.8665, 'learning_rate': 0.00026118850462737453, 'epoch': 1.5}                                                                                                       
{'loss': 0.7766, 'learning_rate': 0.0002610423770092547, 'epoch': 1.51}                                                                                                       
{'loss': 0.951, 'learning_rate': 0.0002608962493911349, 'epoch': 1.51}                                                                                                        
{'loss': 0.8162, 'learning_rate': 0.0002607501217730151, 'epoch': 1.52}                                                                                                       
{'loss': 0.9032, 'learning_rate': 0.00026060399415489527, 'epoch': 1.52}                                                                                                      
 15%|███████████████████▍                                                                                                            | 3200/21030 [3:47:23<2:45:00,  1.80it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6857979893684387, 'eval_wer': 0.7946219728812367, 'eval_cer': 0.2891293794819472, 'eval_runtime': 360.1362, 'eval_samples_per_second': 28.983, 'eval_steps_per_second': 3.624, 'epoch': 1.52}                                                                                                                                               
{'loss': 0.8458, 'learning_rate': 0.0002604578665367754, 'epoch': 1.53}                                                                                                       
{'loss': 0.7432, 'learning_rate': 0.0002603117389186556, 'epoch': 1.53}                                                                                                       
{'loss': 0.8401, 'learning_rate': 0.00026016561130053577, 'epoch': 1.54}                                                                                                      
{'loss': 0.7763, 'learning_rate': 0.00026001948368241596, 'epoch': 1.54}                                                                                                      
{'loss': 0.8988, 'learning_rate': 0.00025987335606429614, 'epoch': 1.55}                                                                                                      
{'loss': 0.8257, 'learning_rate': 0.0002597272284461763, 'epoch': 1.55}                                                                                                       
{'loss': 0.7782, 'learning_rate': 0.00025958110082805646, 'epoch': 1.55}                                                                                                      
{'loss': 0.779, 'learning_rate': 0.00025943497320993664, 'epoch': 1.56}                                                                                                       
{'loss': 0.8978, 'learning_rate': 0.0002592888455918168, 'epoch': 1.56}                                                                                                       
{'loss': 0.8937, 'learning_rate': 0.000259142717973697, 'epoch': 1.57}                                                                                                        
 16%|████████████████████                                                                                                            | 3300/21030 [3:54:43<2:45:05,  1.79it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6343885064125061, 'eval_wer': 0.7852306501820229, 'eval_cer': 0.28201930707315587, 'eval_runtime': 358.8251, 'eval_samples_per_second': 29.089, 'eval_steps_per_second': 3.637, 'epoch': 1.57}                                                                                                                                              
{'loss': 0.9079, 'learning_rate': 0.0002589965903555772, 'epoch': 1.57}                                                                                                       
{'loss': 0.7929, 'learning_rate': 0.0002588504627374574, 'epoch': 1.58}                                                                                                       
{'loss': 0.797, 'learning_rate': 0.00025870433511933756, 'epoch': 1.58}                                                                                                       
{'loss': 0.8004, 'learning_rate': 0.0002585582075012177, 'epoch': 1.59}                                                                                                       
{'loss': 0.9501, 'learning_rate': 0.0002584120798830979, 'epoch': 1.59}                                                                                                       
{'loss': 0.9033, 'learning_rate': 0.00025826595226497806, 'epoch': 1.6}                                                                                                       
{'loss': 0.8471, 'learning_rate': 0.00025811982464685825, 'epoch': 1.6}                                                                                                       
{'loss': 0.7741, 'learning_rate': 0.00025797369702873843, 'epoch': 1.61}                                                                                                      
{'loss': 0.9355, 'learning_rate': 0.0002578275694106186, 'epoch': 1.61}                                                                                                       
{'loss': 0.926, 'learning_rate': 0.00025768144179249874, 'epoch': 1.62}                                                                                                       
 16%|████████████████████▋                                                                                                           | 3400/21030 [4:02:02<2:42:33,  1.81it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.667274534702301, 'eval_wer': 0.8104852183394594, 'eval_cer': 0.2917077254453969, 'eval_runtime': 362.0015, 'eval_samples_per_second': 28.834, 'eval_steps_per_second': 3.605, 'epoch': 1.62}                                                                                                                                                
{'loss': 0.8335, 'learning_rate': 0.00025753531417437893, 'epoch': 1.62}                                                                                                      
{'loss': 0.7868, 'learning_rate': 0.0002573891865562591, 'epoch': 1.63}                                                                                                       
{'loss': 0.8192, 'learning_rate': 0.0002572430589381393, 'epoch': 1.63}                                                                                                       
{'loss': 0.7663, 'learning_rate': 0.00025709693132001943, 'epoch': 1.64}                                                                                                      
{'loss': 0.9058, 'learning_rate': 0.0002569508037018996, 'epoch': 1.64}                                                                                                       
{'loss': 0.9498, 'learning_rate': 0.00025680467608377985, 'epoch': 1.65}                                                                                                      
{'loss': 0.7424, 'learning_rate': 0.00025665854846566, 'epoch': 1.65}                                                                                                         
{'loss': 0.7189, 'learning_rate': 0.00025651242084754017, 'epoch': 1.65}                                                                                                      
{'loss': 0.7497, 'learning_rate': 0.00025636629322942035, 'epoch': 1.66}                                                                                                      
{'loss': 0.9564, 'learning_rate': 0.00025622016561130054, 'epoch': 1.66}                                                                                                      
 17%|█████████████████████▎                                                                                                          | 3500/21030 [4:09:24<2:37:46,  1.85it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5959028005599976, 'eval_wer': 0.7643551819348938, 'eval_cer': 0.2760864952226476, 'eval_runtime': 359.1139, 'eval_samples_per_second': 29.066, 'eval_steps_per_second': 3.634, 'epoch': 1.66}                                                                                                                                               
{'loss': 0.8567, 'learning_rate': 0.00025607403799318067, 'epoch': 1.67}                                                                                                      
{'loss': 0.841, 'learning_rate': 0.00025592791037506085, 'epoch': 1.67}                                                                                                       
{'loss': 0.7793, 'learning_rate': 0.00025578178275694103, 'epoch': 1.68}                                                                                                      
{'loss': 0.8112, 'learning_rate': 0.0002556356551388212, 'epoch': 1.68}                                                                                                       
{'loss': 0.9021, 'learning_rate': 0.0002554895275207014, 'epoch': 1.69}                                                                                                       
{'loss': 1.0113, 'learning_rate': 0.0002553433999025816, 'epoch': 1.69}                                                                                                       
{'loss': 0.8175, 'learning_rate': 0.0002551972722844617, 'epoch': 1.7}                                                                                                        
{'loss': 0.7414, 'learning_rate': 0.0002550511446663419, 'epoch': 1.7}                                                                                                        
{'loss': 0.8064, 'learning_rate': 0.0002549050170482221, 'epoch': 1.71}                                                                                                       
{'loss': 0.9209, 'learning_rate': 0.00025475888943010227, 'epoch': 1.71}                                                                                                      
 17%|█████████████████████▉                                                                                                          | 3600/21030 [4:16:44<2:40:17,  1.81it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6084566116333008, 'eval_wer': 0.786848630871775, 'eval_cer': 0.284961005060767, 'eval_runtime': 359.383, 'eval_samples_per_second': 29.044, 'eval_steps_per_second': 3.631, 'epoch': 1.71}                                                                                                                                                  
{'loss': 0.8639, 'learning_rate': 0.00025461276181198246, 'epoch': 1.72}                                                                                                      
{'loss': 0.7785, 'learning_rate': 0.00025446663419386264, 'epoch': 1.72}                                                                                                      
{'loss': 0.8159, 'learning_rate': 0.0002543205065757428, 'epoch': 1.73}                                                                                                       
{'loss': 0.801, 'learning_rate': 0.00025417437895762295, 'epoch': 1.73}                                                                                                       
{'loss': 0.933, 'learning_rate': 0.00025402825133950314, 'epoch': 1.74}                                                                                                       
{'loss': 0.8738, 'learning_rate': 0.0002538821237213833, 'epoch': 1.74}                                                                                                       
{'loss': 0.7916, 'learning_rate': 0.0002537359961032635, 'epoch': 1.74}                                                                                                       
{'loss': 0.7139, 'learning_rate': 0.00025358986848514364, 'epoch': 1.75}                                                                                                      
{'loss': 0.8201, 'learning_rate': 0.0002534437408670239, 'epoch': 1.75}                                                                                                       
{'loss': 0.9722, 'learning_rate': 0.000253297613248904, 'epoch': 1.76}                                                                                                        
 18%|██████████████████████▌                                                                                                         | 3700/21030 [4:24:04<2:42:08,  1.78it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.643670916557312, 'eval_wer': 0.783173000826577, 'eval_cer': 0.28423430101244407, 'eval_runtime': 359.2134, 'eval_samples_per_second': 29.058, 'eval_steps_per_second': 3.633, 'epoch': 1.76}                                                                                                                                                
{'loss': 0.8443, 'learning_rate': 0.0002531514856307842, 'epoch': 1.76}                                                                                                       
{'loss': 0.7279, 'learning_rate': 0.0002530053580126644, 'epoch': 1.77}                                                                                                       
{'loss': 0.737, 'learning_rate': 0.00025285923039454456, 'epoch': 1.77}                                                                                                       
{'loss': 0.7656, 'learning_rate': 0.0002527131027764247, 'epoch': 1.78}                                                                                                       
{'loss': 0.8917, 'learning_rate': 0.0002525669751583049, 'epoch': 1.78}                                                                                                       
{'loss': 0.8874, 'learning_rate': 0.0002524208475401851, 'epoch': 1.79}                                                                                                       
{'loss': 0.783, 'learning_rate': 0.00025227471992206524, 'epoch': 1.79}                                                                                                       
{'loss': 0.7388, 'learning_rate': 0.00025212859230394543, 'epoch': 1.8}                                                                                                       
{'loss': 0.8234, 'learning_rate': 0.0002519824646858256, 'epoch': 1.8}                                                                                                        
{'loss': 0.8535, 'learning_rate': 0.0002518363370677058, 'epoch': 1.81}                                                                                                       
 18%|███████████████████████▏                                                                                                        | 3800/21030 [4:31:22<2:38:39,  1.81it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5894458293914795, 'eval_wer': 0.7538910676913878, 'eval_cer': 0.2902107151058517, 'eval_runtime': 359.2862, 'eval_samples_per_second': 29.052, 'eval_steps_per_second': 3.632, 'epoch': 1.81}                                                                                                                                               
{'loss': 0.8742, 'learning_rate': 0.00025169020944958593, 'epoch': 1.81}                                                                                                      
{'loss': 0.6692, 'learning_rate': 0.0002515440818314661, 'epoch': 1.82}                                                                                                       
{'loss': 0.716, 'learning_rate': 0.0002513979542133463, 'epoch': 1.82}                                                                                                        
{'loss': 0.8019, 'learning_rate': 0.0002512518265952265, 'epoch': 1.83}                                                                                                       
{'loss': 0.8403, 'learning_rate': 0.00025110569897710667, 'epoch': 1.83}                                                                                                      
{'loss': 0.8336, 'learning_rate': 0.00025095957135898685, 'epoch': 1.84}                                                                                                      
{'loss': 0.6608, 'learning_rate': 0.000250813443740867, 'epoch': 1.84}                                                                                                        
{'loss': 0.6834, 'learning_rate': 0.00025066731612274716, 'epoch': 1.84}                                                                                                      
{'loss': 0.6979, 'learning_rate': 0.00025052118850462735, 'epoch': 1.85}                                                                                                      
{'loss': 0.9046, 'learning_rate': 0.00025037506088650753, 'epoch': 1.85}                                                                                                      
 19%|███████████████████████▋                                                                                                        | 3900/21030 [4:38:41<2:38:39,  1.80it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6133179068565369, 'eval_wer': 0.7772814407062838, 'eval_cer': 0.2818623389987181, 'eval_runtime': 358.765, 'eval_samples_per_second': 29.094, 'eval_steps_per_second': 3.637, 'epoch': 1.85}                                                                                                                                                
{'loss': 0.7987, 'learning_rate': 0.00025022893326838766, 'epoch': 1.86}                                                                                                      
{'loss': 0.6616, 'learning_rate': 0.0002500828056502679, 'epoch': 1.86}                                                                                                       
{'loss': 0.7507, 'learning_rate': 0.0002499366780321481, 'epoch': 1.87}                                                                                                       
{'loss': 0.759, 'learning_rate': 0.0002497905504140282, 'epoch': 1.87}                                                                                                        
{'loss': 0.8977, 'learning_rate': 0.0002496444227959084, 'epoch': 1.88}                                                                                                       
{'loss': 0.8355, 'learning_rate': 0.0002494982951777886, 'epoch': 1.88}                                                                                                       
{'loss': 0.766, 'learning_rate': 0.00024935216755966877, 'epoch': 1.89}                                                                                                       
{'loss': 0.7605, 'learning_rate': 0.0002492060399415489, 'epoch': 1.89}                                                                                                       
{'loss': 0.7933, 'learning_rate': 0.00024905991232342914, 'epoch': 1.9}                                                                                                       
{'loss': 0.9471, 'learning_rate': 0.00024891378470530927, 'epoch': 1.9}                                                                                                       
 19%|████████████████████████▎                                                                                                       | 4000/21030 [4:46:00<2:35:11,  1.83it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5723576545715332, 'eval_wer': 0.7586394892808779, 'eval_cer': 0.27917353401992334, 'eval_runtime': 361.3134, 'eval_samples_per_second': 28.889, 'eval_steps_per_second': 3.612, 'epoch': 1.9}                                                                                                                                               
{'loss': 0.8615, 'learning_rate': 0.00024876765708718945, 'epoch': 1.91}                                                                                                      
{'loss': 0.6936, 'learning_rate': 0.00024862152946906964, 'epoch': 1.91}                                                                                                      
{'loss': 0.7436, 'learning_rate': 0.0002484754018509498, 'epoch': 1.92}                                                                                                       
{'loss': 0.7935, 'learning_rate': 0.00024832927423282995, 'epoch': 1.92}                                                                                                      
{'loss': 0.8495, 'learning_rate': 0.00024818314661471014, 'epoch': 1.93}                                                                                                      
{'loss': 1.0551, 'learning_rate': 0.0002480370189965903, 'epoch': 1.93}                                                                                                       
{'loss': 0.7976, 'learning_rate': 0.0002478908913784705, 'epoch': 1.94}                                                                                                       
{'loss': 0.6597, 'learning_rate': 0.0002477447637603507, 'epoch': 1.94}                                                                                                       
{'loss': 0.706, 'learning_rate': 0.0002475986361422309, 'epoch': 1.94}                                                                                                        
{'loss': 0.8779, 'learning_rate': 0.00024745250852411106, 'epoch': 1.95}                                                                                                      
 19%|████████████████████████▉                                                                                                       | 4100/21030 [4:53:22<2:38:54,  1.78it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6564173102378845, 'eval_wer': 0.7734651166880638, 'eval_cer': 0.2826442725547135, 'eval_runtime': 362.3666, 'eval_samples_per_second': 28.805, 'eval_steps_per_second': 3.601, 'epoch': 1.95}                                                                                                                                               
{'loss': 0.9927, 'learning_rate': 0.0002473063809059912, 'epoch': 1.95}                                                                                                       
{'loss': 0.7925, 'learning_rate': 0.0002471602532878714, 'epoch': 1.96}                                                                                                       
{'loss': 0.7343, 'learning_rate': 0.00024701412566975156, 'epoch': 1.96}                                                                                                      
{'loss': 0.7467, 'learning_rate': 0.00024686799805163174, 'epoch': 1.97}                                                                                                      
{'loss': 0.8106, 'learning_rate': 0.00024672187043351193, 'epoch': 1.97}                                                                                                      
{'loss': 0.862, 'learning_rate': 0.0002465757428153921, 'epoch': 1.98}                                                                                                        
{'loss': 0.7179, 'learning_rate': 0.00024642961519727224, 'epoch': 1.98}                                                                                                      
{'loss': 0.7542, 'learning_rate': 0.00024628348757915243, 'epoch': 1.99}                                                                                                      
{'loss': 0.7673, 'learning_rate': 0.0002461373599610326, 'epoch': 1.99}                                                                                                       
{'loss': 0.935, 'learning_rate': 0.0002459912323429128, 'epoch': 2.0}                                                                                                         
 20%|█████████████████████████▌                                                                                                      | 4200/21030 [5:00:45<2:34:22,  1.82it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5619807839393616, 'eval_wer': 0.7352842897592374, 'eval_cer': 0.2770922536255265, 'eval_runtime': 362.8385, 'eval_samples_per_second': 28.768, 'eval_steps_per_second': 3.597, 'epoch': 2.0}                                                                                                                                                
 20%|█████████████████████████▍                                                                                                     | 4208/21030 [5:06:57<48:55:39, 10.47s/it]Saving model checkpoint to ./20_band/checkpoint-4208                                                                                                                          
Configuration saved in ./20_band/checkpoint-4208/config.json
Model weights saved in ./20_band/checkpoint-4208/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-4208/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 1.026, 'learning_rate': 0.0002458451047247929, 'epoch': 2.0}                                                                                                         
{'loss': 0.6624, 'learning_rate': 0.00024569897710667316, 'epoch': 2.01}                                                                                                      
{'loss': 0.7186, 'learning_rate': 0.00024555284948855335, 'epoch': 2.01}                                                                                                      
{'loss': 0.762, 'learning_rate': 0.0002454067218704335, 'epoch': 2.02}                                                                                                        
{'loss': 0.767, 'learning_rate': 0.00024526059425231366, 'epoch': 2.02}                                                                                                       
{'loss': 0.8407, 'learning_rate': 0.00024511446663419385, 'epoch': 2.03}                                                                                                      
{'loss': 0.6013, 'learning_rate': 0.00024496833901607403, 'epoch': 2.03}                                                                                                      
{'loss': 0.6373, 'learning_rate': 0.00024482221139795416, 'epoch': 2.04}                                                                                                      
{'loss': 0.6456, 'learning_rate': 0.00024467608377983435, 'epoch': 2.04}                                                                                                      
{'loss': 0.8113, 'learning_rate': 0.00024452995616171453, 'epoch': 2.04}                                                                                                      
 20%|██████████████████████████▏                                                                                                     | 4300/21030 [5:08:14<2:43:26,  1.71it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6098476052284241, 'eval_wer': 0.7630889361776965, 'eval_cer': 0.27578418633854523, 'eval_runtime': 362.373, 'eval_samples_per_second': 28.805, 'eval_steps_per_second': 3.601, 'epoch': 2.04}                                                                                                                                               
{'loss': 0.8083, 'learning_rate': 0.0002443838285435947, 'epoch': 2.05}                                                                                                       
{'loss': 0.7127, 'learning_rate': 0.0002442377009254749, 'epoch': 2.05}                                                                                                       
{'loss': 0.6844, 'learning_rate': 0.00024409157330735506, 'epoch': 2.06}                                                                                                      
{'loss': 0.716, 'learning_rate': 0.00024394544568923524, 'epoch': 2.06}                                                                                                       
{'loss': 0.7222, 'learning_rate': 0.0002437993180711154, 'epoch': 2.07}                                                                                                       
{'loss': 0.8542, 'learning_rate': 0.00024365319045299558, 'epoch': 2.07}                                                                                                      
{'loss': 0.6516, 'learning_rate': 0.00024350706283487574, 'epoch': 2.08}                                                                                                      
{'loss': 0.6443, 'learning_rate': 0.00024336093521675595, 'epoch': 2.08}                                                                                                      
{'loss': 0.6797, 'learning_rate': 0.00024321480759863614, 'epoch': 2.09}                                                                                                      
{'loss': 0.7545, 'learning_rate': 0.0002430686799805163, 'epoch': 2.09}                                                                                                       
 21%|██████████████████████████▊                                                                                                     | 4400/21030 [5:15:37<2:37:27,  1.76it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6724936366081238, 'eval_wer': 0.7949737078137915, 'eval_cer': 0.2910856667800325, 'eval_runtime': 360.5014, 'eval_samples_per_second': 28.954, 'eval_steps_per_second': 3.62, 'epoch': 2.09}                                                                                                                                                
{'loss': 0.8459, 'learning_rate': 0.00024292255236239648, 'epoch': 2.1}                                                                                                       
{'loss': 0.6944, 'learning_rate': 0.00024277642474427664, 'epoch': 2.1}                                                                                                       
{'loss': 0.6754, 'learning_rate': 0.00024263029712615682, 'epoch': 2.11}                                                                                                      
{'loss': 0.6889, 'learning_rate': 0.00024248416950803698, 'epoch': 2.11}                                                                                                      
{'loss': 0.7061, 'learning_rate': 0.0002423380418899172, 'epoch': 2.12}                                                                                                       
{'loss': 0.8155, 'learning_rate': 0.00024219191427179735, 'epoch': 2.12}                                                                                                      
{'loss': 0.7165, 'learning_rate': 0.00024204578665367753, 'epoch': 2.13}                                                                                                      
{'loss': 0.6814, 'learning_rate': 0.0002418996590355577, 'epoch': 2.13}                                                                                                       
{'loss': 0.6563, 'learning_rate': 0.00024175353141743787, 'epoch': 2.14}                                                                                                      
{'loss': 0.7359, 'learning_rate': 0.00024160740379931803, 'epoch': 2.14}                                                                                                      
 21%|███████████████████████████▍                                                                                                    | 4500/21030 [5:22:59<2:38:53,  1.73it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.57310950756073, 'eval_wer': 0.7453439088303054, 'eval_cer': 0.27196462986056, 'eval_runtime': 360.8431, 'eval_samples_per_second': 28.927, 'eval_steps_per_second': 3.617, 'epoch': 2.14}                                                                                                                                                   
{'loss': 0.8212, 'learning_rate': 0.00024146127618119822, 'epoch': 2.14}                                                                                                      
{'loss': 0.6995, 'learning_rate': 0.00024131514856307837, 'epoch': 2.15}                                                                                                      
{'loss': 0.6207, 'learning_rate': 0.00024116902094495858, 'epoch': 2.15}                                                                                                      
{'loss': 0.7534, 'learning_rate': 0.00024102289332683877, 'epoch': 2.16}                                                                                                      
{'loss': 0.6848, 'learning_rate': 0.00024087676570871893, 'epoch': 2.16}                                                                                                      
{'loss': 0.8946, 'learning_rate': 0.0002407306380905991, 'epoch': 2.17}                                                                                                       
{'loss': 0.6229, 'learning_rate': 0.00024058451047247927, 'epoch': 2.17}                                                                                                      
{'loss': 0.6197, 'learning_rate': 0.00024043838285435945, 'epoch': 2.18}                                                                                                      
{'loss': 0.7256, 'learning_rate': 0.0002402922552362396, 'epoch': 2.18}                                                                                                       
{'loss': 0.7557, 'learning_rate': 0.0002401461276181198, 'epoch': 2.19}                                                                                                       
 22%|███████████████████████████▉                                                                                                    | 4600/21030 [5:30:21<2:35:17,  1.76it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5794029831886292, 'eval_wer': 0.7688222155783402, 'eval_cer': 0.26940663161046335, 'eval_runtime': 363.476, 'eval_samples_per_second': 28.717, 'eval_steps_per_second': 3.59, 'epoch': 2.19}                                                                                                                                                
{'loss': 0.8803, 'learning_rate': 0.00023999999999999998, 'epoch': 2.19}                                                                                                      
{'loss': 0.6283, 'learning_rate': 0.00023985387238188016, 'epoch': 2.2}                                                                                                       
{'loss': 0.6214, 'learning_rate': 0.00023970774476376035, 'epoch': 2.2}                                                                                                       
{'loss': 0.7208, 'learning_rate': 0.0002395616171456405, 'epoch': 2.21}                                                                                                       
{'loss': 0.7925, 'learning_rate': 0.0002394154895275207, 'epoch': 2.21}                                                                                                       
{'loss': 0.9103, 'learning_rate': 0.00023926936190940085, 'epoch': 2.22}                                                                                                      
{'loss': 0.6127, 'learning_rate': 0.00023912323429128103, 'epoch': 2.22}                                                                                                      
{'loss': 0.6329, 'learning_rate': 0.00023897710667316122, 'epoch': 2.23}                                                                                                      
{'loss': 0.681, 'learning_rate': 0.0002388309790550414, 'epoch': 2.23}                                                                                                        
{'loss': 0.6652, 'learning_rate': 0.00023868485143692156, 'epoch': 2.23}                                                                                                      
 22%|████████████████████████████▌                                                                                                   | 4700/21030 [5:37:45<2:39:34,  1.71it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6490985155105591, 'eval_wer': 0.7734123564481806, 'eval_cer': 0.2870422854551638, 'eval_runtime': 360.0075, 'eval_samples_per_second': 28.994, 'eval_steps_per_second': 3.625, 'epoch': 2.23}                                                                                                                                               
{'loss': 0.8259, 'learning_rate': 0.00023853872381880174, 'epoch': 2.24}                                                                                                      
{'loss': 0.6476, 'learning_rate': 0.0002383925962006819, 'epoch': 2.24}                                                                                                       
{'loss': 0.6314, 'learning_rate': 0.00023824646858256208, 'epoch': 2.25}                                                                                                      
{'loss': 0.768, 'learning_rate': 0.00023810034096444224, 'epoch': 2.25}                                                                                                       
{'loss': 0.7873, 'learning_rate': 0.00023795421334632243, 'epoch': 2.26}                                                                                                      
{'loss': 0.8837, 'learning_rate': 0.00023780808572820264, 'epoch': 2.26}                                                                                                      
{'loss': 0.7144, 'learning_rate': 0.0002376619581100828, 'epoch': 2.27}                                                                                                       
{'loss': 0.6592, 'learning_rate': 0.00023751583049196298, 'epoch': 2.27}                                                                                                      
{'loss': 0.5735, 'learning_rate': 0.00023736970287384314, 'epoch': 2.28}                                                                                                      
{'loss': 0.6759, 'learning_rate': 0.00023722357525572332, 'epoch': 2.28}                                                                                                      
 23%|█████████████████████████████▏                                                                                                  | 4800/21030 [5:45:07<2:41:26,  1.68it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.641923189163208, 'eval_wer': 0.769033256537873, 'eval_cer': 0.28073449431572095, 'eval_runtime': 361.1977, 'eval_samples_per_second': 28.898, 'eval_steps_per_second': 3.613, 'epoch': 2.28}                                                                                                                                                
{'loss': 0.8202, 'learning_rate': 0.00023707744763760348, 'epoch': 2.29}                                                                                                      
{'loss': 0.6373, 'learning_rate': 0.00023693132001948366, 'epoch': 2.29}                                                                                                      
{'loss': 0.6486, 'learning_rate': 0.00023678519240136382, 'epoch': 2.3}                                                                                                       
{'loss': 0.6307, 'learning_rate': 0.00023663906478324403, 'epoch': 2.3}                                                                                                       
{'loss': 0.774, 'learning_rate': 0.0002364929371651242, 'epoch': 2.31}                                                                                                        
{'loss': 0.8099, 'learning_rate': 0.00023634680954700437, 'epoch': 2.31}                                                                                                      
{'loss': 0.7636, 'learning_rate': 0.00023620068192888453, 'epoch': 2.32}                                                                                                      
{'loss': 0.5895, 'learning_rate': 0.00023605455431076472, 'epoch': 2.32}                                                                                                      
{'loss': 0.6926, 'learning_rate': 0.00023590842669264487, 'epoch': 2.33}                                                                                                      
{'loss': 0.6976, 'learning_rate': 0.00023576229907452506, 'epoch': 2.33}                                                                                                      
 23%|█████████████████████████████▊                                                                                                  | 4900/21030 [5:52:28<2:35:57,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5809860825538635, 'eval_wer': 0.7469091292801745, 'eval_cer': 0.27916481357134343, 'eval_runtime': 362.79, 'eval_samples_per_second': 28.771, 'eval_steps_per_second': 3.597, 'epoch': 2.33}                                                                                                                                                
{'loss': 0.8524, 'learning_rate': 0.00023561617145640527, 'epoch': 2.33}                                                                                                      
{'loss': 0.6841, 'learning_rate': 0.00023547004383828543, 'epoch': 2.34}                                                                                                      
{'loss': 0.78, 'learning_rate': 0.0002353239162201656, 'epoch': 2.34}                                                                                                         
{'loss': 0.6907, 'learning_rate': 0.00023517778860204577, 'epoch': 2.35}                                                                                                      
{'loss': 0.7304, 'learning_rate': 0.00023503166098392595, 'epoch': 2.35}                                                                                                      
{'loss': 0.8124, 'learning_rate': 0.0002348855333658061, 'epoch': 2.36}                                                                                                       
{'loss': 0.7023, 'learning_rate': 0.0002347394057476863, 'epoch': 2.36}                                                                                                       
{'loss': 0.6829, 'learning_rate': 0.00023459327812956645, 'epoch': 2.37}                                                                                                      
{'loss': 0.6999, 'learning_rate': 0.00023444715051144666, 'epoch': 2.37}                                                                                                      
{'loss': 0.7317, 'learning_rate': 0.00023430102289332682, 'epoch': 2.38}                                                                                                      
 24%|██████████████████████████████▍                                                                                                 | 5000/21030 [5:59:51<2:34:36,  1.73it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5694189071655273, 'eval_wer': 0.740542727000932, 'eval_cer': 0.27865321392132414, 'eval_runtime': 362.2416, 'eval_samples_per_second': 28.815, 'eval_steps_per_second': 3.603, 'epoch': 2.38}                                                                                                                                               
{'loss': 0.8129, 'learning_rate': 0.000234154895275207, 'epoch': 2.38}                                                                                                        
{'loss': 0.6358, 'learning_rate': 0.00023400876765708716, 'epoch': 2.39}                                                                                                      
{'loss': 0.7413, 'learning_rate': 0.00023386264003896735, 'epoch': 2.39}                                                                                                      
{'loss': 0.6823, 'learning_rate': 0.0002337165124208475, 'epoch': 2.4}                                                                                                        
{'loss': 0.7226, 'learning_rate': 0.0002335703848027277, 'epoch': 2.4}                                                                                                        
{'loss': 0.8272, 'learning_rate': 0.00023342425718460785, 'epoch': 2.41}                                                                                                      
{'loss': 0.7037, 'learning_rate': 0.00023327812956648806, 'epoch': 2.41}                                                                                                      
{'loss': 0.6633, 'learning_rate': 0.00023313200194836824, 'epoch': 2.42}                                                                                                      
{'loss': 0.6071, 'learning_rate': 0.0002329858743302484, 'epoch': 2.42}                                                                                                       
{'loss': 0.6821, 'learning_rate': 0.00023283974671212858, 'epoch': 2.43}                                                                                                      
 24%|███████████████████████████████                                                                                                 | 5100/21030 [6:07:15<2:34:15,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6349197030067444, 'eval_wer': 0.7710733191466911, 'eval_cer': 0.28729517846398017, 'eval_runtime': 359.7925, 'eval_samples_per_second': 29.011, 'eval_steps_per_second': 3.627, 'epoch': 2.43}                                                                                                                                              
{'loss': 0.8519, 'learning_rate': 0.00023269361909400874, 'epoch': 2.43}                                                                                                      
{'loss': 0.6509, 'learning_rate': 0.00023254749147588893, 'epoch': 2.43}                                                                                                      
{'loss': 0.5825, 'learning_rate': 0.00023240136385776908, 'epoch': 2.44}                                                                                                      
{'loss': 0.5793, 'learning_rate': 0.0002322552362396493, 'epoch': 2.44}                                                                                                       
{'loss': 0.7696, 'learning_rate': 0.00023210910862152945, 'epoch': 2.45}                                                                                                      
{'loss': 0.8692, 'learning_rate': 0.00023196298100340964, 'epoch': 2.45}                                                                                                      
{'loss': 0.5934, 'learning_rate': 0.0002318168533852898, 'epoch': 2.46}                                                                                                       
{'loss': 0.6282, 'learning_rate': 0.00023167072576716998, 'epoch': 2.46}                                                                                                      
{'loss': 0.6028, 'learning_rate': 0.00023152459814905013, 'epoch': 2.47}                                                                                                      
{'loss': 0.7717, 'learning_rate': 0.00023137847053093032, 'epoch': 2.47}                                                                                                      
 25%|███████████████████████████████▋                                                                                                | 5200/21030 [6:14:36<2:30:10,  1.76it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5733516216278076, 'eval_wer': 0.7444645714989184, 'eval_cer': 0.2695752269496743, 'eval_runtime': 362.1, 'eval_samples_per_second': 28.826, 'eval_steps_per_second': 3.604, 'epoch': 2.47}                                                                                                                                                  
{'loss': 0.7907, 'learning_rate': 0.00023123234291281048, 'epoch': 2.48}                                                                                                      
{'loss': 0.7002, 'learning_rate': 0.0002310862152946907, 'epoch': 2.48}                                                                                                       
{'loss': 0.6741, 'learning_rate': 0.00023094008767657087, 'epoch': 2.49}                                                                                                      
{'loss': 0.6694, 'learning_rate': 0.00023079396005845103, 'epoch': 2.49}                                                                                                      
{'loss': 0.7234, 'learning_rate': 0.00023064783244033121, 'epoch': 2.5}                                                                                                       
{'loss': 0.9328, 'learning_rate': 0.00023050170482221137, 'epoch': 2.5}                                                                                                       
{'loss': 0.625, 'learning_rate': 0.00023035557720409156, 'epoch': 2.51}                                                                                                       
{'loss': 0.6004, 'learning_rate': 0.00023020944958597171, 'epoch': 2.51}                                                                                                      
{'loss': 0.6666, 'learning_rate': 0.0002300633219678519, 'epoch': 2.52}                                                                                                       
{'loss': 0.7093, 'learning_rate': 0.00022991719434973208, 'epoch': 2.52}                                                                                                      
 25%|████████████████████████████████▎                                                                                               | 5300/21030 [6:21:59<2:35:07,  1.69it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5664223432540894, 'eval_wer': 0.7314855524876454, 'eval_cer': 0.268874684247091, 'eval_runtime': 361.0321, 'eval_samples_per_second': 28.912, 'eval_steps_per_second': 3.615, 'epoch': 2.52}                                                                                                                                                
{'loss': 0.9107, 'learning_rate': 0.00022977106673161227, 'epoch': 2.52}                                                                                                      
{'loss': 0.6806, 'learning_rate': 0.00022962493911349242, 'epoch': 2.53}                                                                                                      
{'loss': 0.686, 'learning_rate': 0.0002294788114953726, 'epoch': 2.53}                                                                                                        
{'loss': 0.7203, 'learning_rate': 0.00022933268387725277, 'epoch': 2.54}                                                                                                      
{'loss': 0.6845, 'learning_rate': 0.00022918655625913295, 'epoch': 2.54}                                                                                                      
{'loss': 0.8647, 'learning_rate': 0.0002290404286410131, 'epoch': 2.55}                                                                                                       
{'loss': 0.6674, 'learning_rate': 0.00022889430102289332, 'epoch': 2.55}                                                                                                      
{'loss': 0.6266, 'learning_rate': 0.0002287481734047735, 'epoch': 2.56}                                                                                                       
{'loss': 0.6454, 'learning_rate': 0.00022860204578665366, 'epoch': 2.56}                                                                                                      
{'loss': 0.7984, 'learning_rate': 0.00022845591816853385, 'epoch': 2.57}                                                                                                      
 26%|████████████████████████████████▊                                                                                               | 5400/21030 [6:29:22<2:31:36,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.538486659526825, 'eval_wer': 0.7178030636112626, 'eval_cer': 0.26794740988143095, 'eval_runtime': 359.4542, 'eval_samples_per_second': 29.038, 'eval_steps_per_second': 3.631, 'epoch': 2.57}                                                                                                                                               
{'loss': 0.8031, 'learning_rate': 0.000228309790550414, 'epoch': 2.57}                                                                                                        
{'loss': 0.6458, 'learning_rate': 0.0002281636629322942, 'epoch': 2.58}                                                                                                       
{'loss': 0.6648, 'learning_rate': 0.00022801753531417434, 'epoch': 2.58}                                                                                                      
{'loss': 0.7212, 'learning_rate': 0.00022787140769605453, 'epoch': 2.59}                                                                                                      
{'loss': 0.7002, 'learning_rate': 0.00022772528007793471, 'epoch': 2.59}                                                                                                      
{'loss': 0.8506, 'learning_rate': 0.0002275791524598149, 'epoch': 2.6}                                                                                                        
{'loss': 0.6093, 'learning_rate': 0.00022743302484169506, 'epoch': 2.6}                                                                                                       
{'loss': 0.5576, 'learning_rate': 0.00022728689722357524, 'epoch': 2.61}                                                                                                      
{'loss': 0.6594, 'learning_rate': 0.0002271407696054554, 'epoch': 2.61}                                                                                                       
{'loss': 0.7156, 'learning_rate': 0.00022699464198733558, 'epoch': 2.62}                                                                                                      
 26%|█████████████████████████████████▍                                                                                              | 5500/21030 [6:36:42<2:25:23,  1.78it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5832907557487488, 'eval_wer': 0.7412110233727862, 'eval_cer': 0.2721448524645441, 'eval_runtime': 361.0244, 'eval_samples_per_second': 28.912, 'eval_steps_per_second': 3.615, 'epoch': 2.62}                                                                                                                                               
{'loss': 0.8687, 'learning_rate': 0.00022684851436921574, 'epoch': 2.62}                                                                                                      
{'loss': 0.6122, 'learning_rate': 0.00022670238675109592, 'epoch': 2.62}                                                                                                      
{'loss': 0.59, 'learning_rate': 0.00022655625913297614, 'epoch': 2.63}                                                                                                        
{'loss': 0.6275, 'learning_rate': 0.0002264101315148563, 'epoch': 2.63}                                                                                                       
{'loss': 0.8378, 'learning_rate': 0.00022626400389673648, 'epoch': 2.64}                                                                                                      
{'loss': 0.7487, 'learning_rate': 0.00022611787627861663, 'epoch': 2.64}                                                                                                      
{'loss': 0.6156, 'learning_rate': 0.00022597174866049682, 'epoch': 2.65}                                                                                                      
{'loss': 0.6029, 'learning_rate': 0.00022582562104237698, 'epoch': 2.65}                                                                                                      
{'loss': 0.6596, 'learning_rate': 0.00022567949342425716, 'epoch': 2.66}                                                                                                      
{'loss': 0.7036, 'learning_rate': 0.00022553336580613734, 'epoch': 2.66}                                                                                                      
 27%|██████████████████████████████████                                                                                              | 5600/21030 [6:44:04<2:28:49,  1.73it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5538769364356995, 'eval_wer': 0.7289178874799951, 'eval_cer': 0.2664009836665998, 'eval_runtime': 360.9882, 'eval_samples_per_second': 28.915, 'eval_steps_per_second': 3.615, 'epoch': 2.66}                                                                                                                                               
{'loss': 0.8199, 'learning_rate': 0.00022538723818801753, 'epoch': 2.67}                                                                                                      
{'loss': 0.6094, 'learning_rate': 0.0002252411105698977, 'epoch': 2.67}                                                                                                       
{'loss': 0.7286, 'learning_rate': 0.00022509498295177787, 'epoch': 2.68}                                                                                                      
{'loss': 0.6626, 'learning_rate': 0.00022494885533365803, 'epoch': 2.68}                                                                                                      
{'loss': 0.72, 'learning_rate': 0.0002248027277155382, 'epoch': 2.69}                                                                                                         
{'loss': 0.8628, 'learning_rate': 0.00022465660009741837, 'epoch': 2.69}                                                                                                      
{'loss': 0.6413, 'learning_rate': 0.00022451047247929855, 'epoch': 2.7}                                                                                                       
{'loss': 0.6574, 'learning_rate': 0.00022436434486117877, 'epoch': 2.7}                                                                                                       
{'loss': 0.6782, 'learning_rate': 0.00022421821724305892, 'epoch': 2.71}                                                                                                      
{'loss': 0.7913, 'learning_rate': 0.0002240720896249391, 'epoch': 2.71}                                                                                                       
 27%|██████████████████████████████████▋                                                                                             | 5700/21030 [6:51:26<2:27:06,  1.74it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5597501993179321, 'eval_wer': 0.7381509294595593, 'eval_cer': 0.26659283353535707, 'eval_runtime': 362.7997, 'eval_samples_per_second': 28.771, 'eval_steps_per_second': 3.597, 'epoch': 2.71}                                                                                                                                              
{'loss': 0.8706, 'learning_rate': 0.00022392596200681927, 'epoch': 2.71}                                                                                                      
{'loss': 0.6834, 'learning_rate': 0.00022377983438869945, 'epoch': 2.72}                                                                                                      
{'loss': 0.6558, 'learning_rate': 0.0002236337067705796, 'epoch': 2.72}                                                                                                       
{'loss': 0.648, 'learning_rate': 0.0002234875791524598, 'epoch': 2.73}                                                                                                        
{'loss': 0.5878, 'learning_rate': 0.00022334145153433995, 'epoch': 2.73}                                                                                                      
{'loss': 0.8474, 'learning_rate': 0.00022319532391622016, 'epoch': 2.74}                                                                                                      
{'loss': 0.617, 'learning_rate': 0.00022304919629810032, 'epoch': 2.74}                                                                                                       
{'loss': 0.5688, 'learning_rate': 0.0002229030686799805, 'epoch': 2.75}                                                                                                       
{'loss': 0.6828, 'learning_rate': 0.00022275694106186066, 'epoch': 2.75}                                                                                                      
{'loss': 0.6936, 'learning_rate': 0.00022261081344374084, 'epoch': 2.76}                                                                                                      
 28%|███████████████████████████████████▎                                                                                            | 5800/21030 [6:58:50<2:27:26,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.53705233335495, 'eval_wer': 0.7282144176148854, 'eval_cer': 0.2684531958990637, 'eval_runtime': 361.8059, 'eval_samples_per_second': 28.85, 'eval_steps_per_second': 3.607, 'epoch': 2.76}                                                                                                                                                  
{'loss': 0.7674, 'learning_rate': 0.000222464685825621, 'epoch': 2.76}                                                                                                        
{'loss': 0.6207, 'learning_rate': 0.00022231855820750119, 'epoch': 2.77}                                                                                                      
{'loss': 0.6454, 'learning_rate': 0.0002221724305893814, 'epoch': 2.77}                                                                                                       
{'loss': 0.6763, 'learning_rate': 0.00022202630297126155, 'epoch': 2.78}                                                                                                      
{'loss': 0.6696, 'learning_rate': 0.00022188017535314174, 'epoch': 2.78}                                                                                                      
{'loss': 0.8315, 'learning_rate': 0.0002217340477350219, 'epoch': 2.79}                                                                                                       
{'loss': 0.674, 'learning_rate': 0.00022158792011690208, 'epoch': 2.79}                                                                                                       
{'loss': 0.6404, 'learning_rate': 0.00022144179249878224, 'epoch': 2.8}                                                                                                       
{'loss': 0.661, 'learning_rate': 0.00022129566488066242, 'epoch': 2.8}                                                                                                        
{'loss': 0.764, 'learning_rate': 0.00022114953726254258, 'epoch': 2.81}                                                                                                       
 28%|███████████████████████████████████▉                                                                                            | 5900/21030 [7:06:12<2:25:31,  1.73it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5609722137451172, 'eval_wer': 0.7336838958161129, 'eval_cer': 0.2683921527590046, 'eval_runtime': 363.0677, 'eval_samples_per_second': 28.749, 'eval_steps_per_second': 3.594, 'epoch': 2.81}                                                                                                                                               
{'loss': 0.7939, 'learning_rate': 0.0002210034096444228, 'epoch': 2.81}                                                                                                       
{'loss': 0.618, 'learning_rate': 0.00022085728202630295, 'epoch': 2.81}                                                                                                       
{'loss': 0.6571, 'learning_rate': 0.00022071115440818313, 'epoch': 2.82}                                                                                                      
{'loss': 0.6761, 'learning_rate': 0.0002205650267900633, 'epoch': 2.82}                                                                                                       
{'loss': 0.7447, 'learning_rate': 0.00022041889917194348, 'epoch': 2.83}                                                                                                      
{'loss': 0.8026, 'learning_rate': 0.00022027277155382363, 'epoch': 2.83}                                                                                                      
{'loss': 0.589, 'learning_rate': 0.00022012664393570382, 'epoch': 2.84}                                                                                                       
{'loss': 0.7206, 'learning_rate': 0.00021998051631758397, 'epoch': 2.84}                                                                                                      
{'loss': 0.589, 'learning_rate': 0.00021983438869946419, 'epoch': 2.85}                                                                                                       
{'loss': 0.7139, 'learning_rate': 0.00021968826108134437, 'epoch': 2.85}                                                                                                      
 29%|████████████████████████████████████▌                                                                                           | 6000/21030 [7:13:36<2:25:32,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5554897785186768, 'eval_wer': 0.7379398885000263, 'eval_cer': 0.2638197308869568, 'eval_runtime': 361.4072, 'eval_samples_per_second': 28.882, 'eval_steps_per_second': 3.611, 'epoch': 2.85}                                                                                                                                               
{'loss': 0.8071, 'learning_rate': 0.00021954213346322453, 'epoch': 2.86}                                                                                                      
{'loss': 0.6127, 'learning_rate': 0.0002193960058451047, 'epoch': 2.86}                                                                                                       
{'loss': 0.641, 'learning_rate': 0.00021924987822698487, 'epoch': 2.87}                                                                                                       
{'loss': 0.6671, 'learning_rate': 0.00021910375060886505, 'epoch': 2.87}                                                                                                      
{'loss': 0.659, 'learning_rate': 0.0002189576229907452, 'epoch': 2.88}                                                                                                        
{'loss': 0.7407, 'learning_rate': 0.00021881149537262542, 'epoch': 2.88}                                                                                                      
{'loss': 0.6234, 'learning_rate': 0.00021866536775450558, 'epoch': 2.89}                                                                                                      
{'loss': 0.5669, 'learning_rate': 0.00021851924013638576, 'epoch': 2.89}                                                                                                      
{'loss': 0.6167, 'learning_rate': 0.00021837311251826595, 'epoch': 2.9}                                                                                                       
{'loss': 0.7249, 'learning_rate': 0.0002182269849001461, 'epoch': 2.9}                                                                                                        
 29%|█████████████████████████████████████▏                                                                                          | 6100/21030 [7:20:58<2:25:26,  1.71it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6850937008857727, 'eval_wer': 0.7520796327887305, 'eval_cer': 0.28953052011662145, 'eval_runtime': 362.3319, 'eval_samples_per_second': 28.808, 'eval_steps_per_second': 3.602, 'epoch': 2.9}                                                                                                                                               
{'loss': 0.832, 'learning_rate': 0.0002180808572820263, 'epoch': 2.91}                                                                                                        
{'loss': 0.6209, 'learning_rate': 0.00021793472966390645, 'epoch': 2.91}                                                                                                      
{'loss': 0.6466, 'learning_rate': 0.00021778860204578663, 'epoch': 2.91}                                                                                                      
{'loss': 0.627, 'learning_rate': 0.00021764247442766682, 'epoch': 2.92}                                                                                                       
{'loss': 0.6811, 'learning_rate': 0.000217496346809547, 'epoch': 2.92}                                                                                                        
{'loss': 0.817, 'learning_rate': 0.00021735021919142716, 'epoch': 2.93}                                                                                                       
{'loss': 0.611, 'learning_rate': 0.00021720409157330734, 'epoch': 2.93}                                                                                                       
{'loss': 0.6185, 'learning_rate': 0.0002170579639551875, 'epoch': 2.94}                                                                                                       
{'loss': 0.6665, 'learning_rate': 0.00021691183633706769, 'epoch': 2.94}                                                                                                      
{'loss': 0.7339, 'learning_rate': 0.00021676570871894784, 'epoch': 2.95}                                                                                                      
 29%|█████████████████████████████████████▋                                                                                          | 6200/21030 [7:28:22<2:24:15,  1.71it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5912635922431946, 'eval_wer': 0.7579887796556515, 'eval_cer': 0.2816443277842212, 'eval_runtime': 360.923, 'eval_samples_per_second': 28.92, 'eval_steps_per_second': 3.616, 'epoch': 2.95}                                                                                                                                                 
{'loss': 0.8523, 'learning_rate': 0.00021661958110082803, 'epoch': 2.95}                                                                                                      
{'loss': 0.6061, 'learning_rate': 0.00021647345348270824, 'epoch': 2.96}                                                                                                      
{'loss': 0.5462, 'learning_rate': 0.0002163273258645884, 'epoch': 2.96}                                                                                                       
{'loss': 0.6591, 'learning_rate': 0.00021618119824646858, 'epoch': 2.97}                                                                                                      
{'loss': 0.6668, 'learning_rate': 0.00021603507062834874, 'epoch': 2.97}                                                                                                      
{'loss': 0.7171, 'learning_rate': 0.00021588894301022892, 'epoch': 2.98}                                                                                                      
{'loss': 0.5991, 'learning_rate': 0.00021574281539210908, 'epoch': 2.98}                                                                                                      
{'loss': 0.621, 'learning_rate': 0.00021559668777398926, 'epoch': 2.99}                                                                                                       
{'loss': 0.7019, 'learning_rate': 0.00021545056015586945, 'epoch': 2.99}                                                                                                      
{'loss': 0.7175, 'learning_rate': 0.00021530443253774963, 'epoch': 3.0}                                                                                                       
 30%|██████████████████████████████████████▎                                                                                         | 6300/21030 [7:35:44<2:23:02,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5514283180236816, 'eval_wer': 0.7583053410949508, 'eval_cer': 0.270479246785788, 'eval_runtime': 363.684, 'eval_samples_per_second': 28.701, 'eval_steps_per_second': 3.588, 'epoch': 3.0}                                                                                                                                                  
{'loss': 0.8791, 'learning_rate': 0.0002151583049196298, 'epoch': 3.0}                                                                                                        
 30%|██████████████████████████████████████                                                                                         | 6312/21030 [7:41:59<14:30:28,  3.55s/it]Saving model checkpoint to ./20_band/checkpoint-6312
Configuration saved in ./20_band/checkpoint-6312/config.json
Model weights saved in ./20_band/checkpoint-6312/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-6312/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.5539, 'learning_rate': 0.00021501217730150997, 'epoch': 3.01}                                                                                                      
{'loss': 0.5578, 'learning_rate': 0.00021486604968339013, 'epoch': 3.01}                                                                                                      
{'loss': 0.5602, 'learning_rate': 0.00021471992206527032, 'epoch': 3.01}                                                                                                      
{'loss': 0.6292, 'learning_rate': 0.00021457379444715047, 'epoch': 3.02}                                                                                                      
{'loss': 0.693, 'learning_rate': 0.00021442766682903066, 'epoch': 3.02}                                                                                                       
{'loss': 0.545, 'learning_rate': 0.00021428153921091087, 'epoch': 3.03}                                                                                                       
{'loss': 0.5752, 'learning_rate': 0.00021413541159279103, 'epoch': 3.03}                                                                                                      
{'loss': 0.594, 'learning_rate': 0.0002139892839746712, 'epoch': 3.04}                                                                                                        
{'loss': 0.6777, 'learning_rate': 0.00021384315635655137, 'epoch': 3.04}                                                                                                      
 30%|██████████████████████████████████████▉                                                                                         | 6400/21030 [7:43:12<2:28:20,  1.64it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5695599317550659, 'eval_wer': 0.7377288475404935, 'eval_cer': 0.27581034768428486, 'eval_runtime': 365.0672, 'eval_samples_per_second': 28.592, 'eval_steps_per_second': 3.575, 'epoch': 3.04}                                                                                                                                              
{'loss': 0.7132, 'learning_rate': 0.00021369702873843155, 'epoch': 3.05}                                                                                                      
{'loss': 0.5716, 'learning_rate': 0.0002135509011203117, 'epoch': 3.05}                                                                                                       
{'loss': 0.5949, 'learning_rate': 0.0002134047735021919, 'epoch': 3.06}                                                                                                       
{'loss': 0.6103, 'learning_rate': 0.00021325864588407205, 'epoch': 3.06}                                                                                                      
{'loss': 0.5743, 'learning_rate': 0.00021311251826595226, 'epoch': 3.07}                                                                                                      
{'loss': 0.7708, 'learning_rate': 0.00021296639064783242, 'epoch': 3.07}                                                                                                      
{'loss': 0.5935, 'learning_rate': 0.0002128202630297126, 'epoch': 3.08}                                                                                                       
{'loss': 0.5781, 'learning_rate': 0.00021267413541159276, 'epoch': 3.08}                                                                                                      
{'loss': 0.5853, 'learning_rate': 0.00021252800779347295, 'epoch': 3.09}                                                                                                      
{'loss': 0.5979, 'learning_rate': 0.0002123818801753531, 'epoch': 3.09}                                                                                                       
 31%|███████████████████████████████████████▌                                                                                        | 6500/21030 [7:50:38<2:25:03,  1.67it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6034959554672241, 'eval_wer': 0.7626492675120029, 'eval_cer': 0.27428717599900004, 'eval_runtime': 364.4845, 'eval_samples_per_second': 28.638, 'eval_steps_per_second': 3.58, 'epoch': 3.09}                                                                                                                                               
{'loss': 0.7676, 'learning_rate': 0.0002122357525572333, 'epoch': 3.1}                                                                                                        
{'loss': 0.5888, 'learning_rate': 0.0002120896249391135, 'epoch': 3.1}                                                                                                        
{'loss': 0.5486, 'learning_rate': 0.00021194349732099366, 'epoch': 3.11}                                                                                                      
{'loss': 0.5905, 'learning_rate': 0.00021179736970287384, 'epoch': 3.11}                                                                                                      
{'loss': 0.5828, 'learning_rate': 0.000211651242084754, 'epoch': 3.11}                                                                                                        
{'loss': 0.7593, 'learning_rate': 0.00021150511446663418, 'epoch': 3.12}                                                                                                      
{'loss': 0.63, 'learning_rate': 0.00021135898684851434, 'epoch': 3.12}                                                                                                        
{'loss': 0.5496, 'learning_rate': 0.00021121285923039453, 'epoch': 3.13}                                                                                                      
{'loss': 0.5348, 'learning_rate': 0.00021106673161227468, 'epoch': 3.13}                                                                                                      
{'loss': 0.5964, 'learning_rate': 0.0002109206039941549, 'epoch': 3.14}                                                                                                       
 31%|████████████████████████████████████████▏                                                                                       | 6600/21030 [7:58:04<2:26:36,  1.64it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6077306270599365, 'eval_wer': 0.7498109424737518, 'eval_cer': 0.27480458928140594, 'eval_runtime': 362.0067, 'eval_samples_per_second': 28.834, 'eval_steps_per_second': 3.605, 'epoch': 3.14}                                                                                                                                              
{'loss': 0.7664, 'learning_rate': 0.00021077447637603505, 'epoch': 3.14}                                                                                                      
{'loss': 0.5423, 'learning_rate': 0.00021062834875791524, 'epoch': 3.15}                                                                                                      
{'loss': 0.5648, 'learning_rate': 0.0002104822211397954, 'epoch': 3.15}                                                                                                       
{'loss': 0.5541, 'learning_rate': 0.00021033609352167558, 'epoch': 3.16}                                                                                                      
{'loss': 0.6047, 'learning_rate': 0.00021018996590355574, 'epoch': 3.16}                                                                                                      
{'loss': 0.7472, 'learning_rate': 0.00021004383828543592, 'epoch': 3.17}                                                                                                      
{'loss': 0.5546, 'learning_rate': 0.00020989771066731608, 'epoch': 3.17}                                                                                                      
{'loss': 0.5687, 'learning_rate': 0.0002097515830491963, 'epoch': 3.18}                                                                                                       
{'loss': 0.557, 'learning_rate': 0.00020960545543107647, 'epoch': 3.18}                                                                                                       
{'loss': 0.6835, 'learning_rate': 0.00020945932781295663, 'epoch': 3.19}                                                                                                      
 32%|████████████████████████████████████████▊                                                                                       | 6700/21030 [8:05:28<2:22:29,  1.68it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6373036503791809, 'eval_wer': 0.7439369691000862, 'eval_cer': 0.2734412924867522, 'eval_runtime': 361.4931, 'eval_samples_per_second': 28.875, 'eval_steps_per_second': 3.61, 'epoch': 3.19}                                                                                                                                                
{'loss': 0.7974, 'learning_rate': 0.00020931320019483682, 'epoch': 3.19}                                                                                                      
{'loss': 0.5438, 'learning_rate': 0.00020916707257671697, 'epoch': 3.2}                                                                                                       
{'loss': 0.5737, 'learning_rate': 0.00020902094495859716, 'epoch': 3.2}                                                                                                       
{'loss': 0.5708, 'learning_rate': 0.00020887481734047732, 'epoch': 3.2}                                                                                                       
{'loss': 0.6229, 'learning_rate': 0.00020872868972235753, 'epoch': 3.21}                                                                                                      
{'loss': 0.8143, 'learning_rate': 0.00020858256210423768, 'epoch': 3.21}                                                                                                      
{'loss': 0.5526, 'learning_rate': 0.00020843643448611787, 'epoch': 3.22}                                                                                                      
{'loss': 0.5355, 'learning_rate': 0.00020829030686799803, 'epoch': 3.22}                                                                                                      
{'loss': 0.5718, 'learning_rate': 0.0002081441792498782, 'epoch': 3.23}                                                                                                       
{'loss': 0.6264, 'learning_rate': 0.00020799805163175837, 'epoch': 3.23}                                                                                                      
 32%|█████████████████████████████████████████▍                                                                                      | 6800/21030 [8:12:51<2:25:30,  1.63it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.6260249018669128, 'eval_wer': 0.7622447723395649, 'eval_cer': 0.27807475749885907, 'eval_runtime': 363.2787, 'eval_samples_per_second': 28.733, 'eval_steps_per_second': 3.592, 'epoch': 3.23}                                                                                                                                              
{'loss': 0.7367, 'learning_rate': 0.00020785192401363855, 'epoch': 3.24}                                                                                                      
{'loss': 0.5631, 'learning_rate': 0.0002077057963955187, 'epoch': 3.24}                                                                                                       
{'loss': 0.5341, 'learning_rate': 0.00020755966877739892, 'epoch': 3.25}                                                                                                      
{'loss': 0.5628, 'learning_rate': 0.0002074135411592791, 'epoch': 3.25}                                                                                                       
{'loss': 0.5311, 'learning_rate': 0.00020726741354115926, 'epoch': 3.26}                                                                                                      
{'loss': 0.6927, 'learning_rate': 0.00020712128592303945, 'epoch': 3.26}                                                                                                      
{'loss': 0.5311, 'learning_rate': 0.0002069751583049196, 'epoch': 3.27}                                                                                                       
{'loss': 0.5096, 'learning_rate': 0.0002068290306867998, 'epoch': 3.27}                                                                                                       
{'loss': 0.5994, 'learning_rate': 0.00020668290306867995, 'epoch': 3.28}                                                                                                      
{'loss': 0.5964, 'learning_rate': 0.00020653677545056013, 'epoch': 3.28}                                                                                                      
 33%|█████████████████████████████████████████▉                                                                                      | 6900/21030 [8:20:15<2:22:31,  1.65it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5734459161758423, 'eval_wer': 0.7288827139867395, 'eval_cer': 0.2726826134603031, 'eval_runtime': 363.1885, 'eval_samples_per_second': 28.74, 'eval_steps_per_second': 3.593, 'epoch': 3.28}                                                                                                                                                
{'loss': 0.7921, 'learning_rate': 0.00020639064783244032, 'epoch': 3.29}                                                                                                      
{'loss': 0.5688, 'learning_rate': 0.0002062445202143205, 'epoch': 3.29}                                                                                                       
{'loss': 0.5746, 'learning_rate': 0.00020609839259620066, 'epoch': 3.3}                                                                                                       
{'loss': 0.6148, 'learning_rate': 0.00020595226497808084, 'epoch': 3.3}                                                                                                       
{'loss': 0.5543, 'learning_rate': 0.000205806137359961, 'epoch': 3.3}                                                                                                         
{'loss': 0.7908, 'learning_rate': 0.00020566000974184118, 'epoch': 3.31}                                                                                                      
{'loss': 0.6156, 'learning_rate': 0.00020551388212372134, 'epoch': 3.31}                                                                                                      
{'loss': 0.592, 'learning_rate': 0.00020536775450560155, 'epoch': 3.32}                                                                                                       
{'loss': 0.6385, 'learning_rate': 0.00020522162688748174, 'epoch': 3.32}                                                                                                      
{'loss': 0.5728, 'learning_rate': 0.0002050754992693619, 'epoch': 3.33}                                                                                                       
 33%|██████████████████████████████████████████▌                                                                                     | 7000/21030 [8:27:39<2:20:02,  1.67it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5556291341781616, 'eval_wer': 0.7185768804628831, 'eval_cer': 0.26548242974951963, 'eval_runtime': 361.9441, 'eval_samples_per_second': 28.839, 'eval_steps_per_second': 3.606, 'epoch': 3.33}                                                                                                                                              
{'loss': 0.8046, 'learning_rate': 0.00020492937165124208, 'epoch': 3.33}                                                                                                      
{'loss': 0.5375, 'learning_rate': 0.00020478324403312224, 'epoch': 3.34}                                                                                                      
{'loss': 0.5017, 'learning_rate': 0.00020463711641500242, 'epoch': 3.34}                                                                                                      
{'loss': 0.6571, 'learning_rate': 0.00020449098879688258, 'epoch': 3.35}                                                                                                      
{'loss': 0.6061, 'learning_rate': 0.00020434486117876276, 'epoch': 3.35}                                                                                                      
{'loss': 0.7639, 'learning_rate': 0.00020419873356064295, 'epoch': 3.36}                                                                                                      
{'loss': 0.6084, 'learning_rate': 0.00020405260594252313, 'epoch': 3.36}                                                                                                      
{'loss': 0.5636, 'learning_rate': 0.0002039064783244033, 'epoch': 3.37}                                                                                                       
{'loss': 0.564, 'learning_rate': 0.00020376035070628347, 'epoch': 3.37}                                                                                                       
{'loss': 0.5992, 'learning_rate': 0.00020361422308816363, 'epoch': 3.38}                                                                                                      
 34%|███████████████████████████████████████████▏                                                                                    | 7100/21030 [8:35:02<2:18:35,  1.68it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5673547983169556, 'eval_wer': 0.728108897135119, 'eval_cer': 0.26603181801005177, 'eval_runtime': 363.2467, 'eval_samples_per_second': 28.735, 'eval_steps_per_second': 3.593, 'epoch': 3.38}                                                                                                                                               
{'loss': 0.7747, 'learning_rate': 0.00020346809547004381, 'epoch': 3.38}                                                                                                      
{'loss': 0.5549, 'learning_rate': 0.00020332196785192397, 'epoch': 3.39}                                                                                                      
{'loss': 0.5096, 'learning_rate': 0.00020317584023380416, 'epoch': 3.39}                                                                                                      
{'loss': 0.4819, 'learning_rate': 0.00020302971261568437, 'epoch': 3.4}                                                                                                       
{'loss': 0.4947, 'learning_rate': 0.00020288358499756453, 'epoch': 3.4}                                                                                                       
{'loss': 0.7664, 'learning_rate': 0.0002027374573794447, 'epoch': 3.4}                                                                                                        
{'loss': 0.604, 'learning_rate': 0.00020259132976132487, 'epoch': 3.41}                                                                                                       
{'loss': 0.5557, 'learning_rate': 0.00020244520214320505, 'epoch': 3.41}                                                                                                      
{'loss': 0.5432, 'learning_rate': 0.0002022990745250852, 'epoch': 3.42}                                                                                                       
{'loss': 0.5966, 'learning_rate': 0.0002021529469069654, 'epoch': 3.42}                                                                                                       
 34%|███████████████████████████████████████████▊                                                                                    | 7200/21030 [8:42:26<2:22:26,  1.62it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.590663492679596, 'eval_wer': 0.741158263132903, 'eval_cer': 0.27006647888634056, 'eval_runtime': 364.4362, 'eval_samples_per_second': 28.642, 'eval_steps_per_second': 3.581, 'epoch': 3.42}                                                                                                                                                
{'loss': 0.6892, 'learning_rate': 0.00020200681928884558, 'epoch': 3.43}                                                                                                      
{'loss': 0.5882, 'learning_rate': 0.00020186069167072576, 'epoch': 3.43}                                                                                                      
{'loss': 0.5428, 'learning_rate': 0.00020171456405260592, 'epoch': 3.44}                                                                                                      
{'loss': 0.5575, 'learning_rate': 0.0002015684364344861, 'epoch': 3.44}                                                                                                       
{'loss': 0.6278, 'learning_rate': 0.00020142230881636626, 'epoch': 3.45}                                                                                                      
{'loss': 0.8328, 'learning_rate': 0.00020127618119824645, 'epoch': 3.45}                                                                                                      
{'loss': 0.5472, 'learning_rate': 0.0002011300535801266, 'epoch': 3.46}                                                                                                       
{'loss': 0.5331, 'learning_rate': 0.0002009839259620068, 'epoch': 3.46}                                                                                                       
{'loss': 0.6156, 'learning_rate': 0.000200837798343887, 'epoch': 3.47}                                                                                                        
{'loss': 0.5699, 'learning_rate': 0.00020069167072576716, 'epoch': 3.47}                                                                                                      
 35%|████████████████████████████████████████████▍                                                                                   | 7300/21030 [8:49:52<2:19:14,  1.64it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5526074171066284, 'eval_wer': 0.7155871335361671, 'eval_cer': 0.2663951700342132, 'eval_runtime': 364.8827, 'eval_samples_per_second': 28.606, 'eval_steps_per_second': 3.576, 'epoch': 3.47}                                                                                                                                               
{'loss': 0.7306, 'learning_rate': 0.00020054554310764734, 'epoch': 3.48}                                                                                                      
{'loss': 0.5542, 'learning_rate': 0.0002003994154895275, 'epoch': 3.48}                                                                                                       
{'loss': 0.5586, 'learning_rate': 0.00020025328787140768, 'epoch': 3.49}                                                                                                      
{'loss': 0.6223, 'learning_rate': 0.00020010716025328784, 'epoch': 3.49}                                                                                                      
{'loss': 0.6598, 'learning_rate': 0.00019996103263516802, 'epoch': 3.49}                                                                                                      
{'loss': 0.7444, 'learning_rate': 0.00019981490501704818, 'epoch': 3.5}                                                                                                       
{'loss': 0.5272, 'learning_rate': 0.0001996687773989284, 'epoch': 3.5}                                                                                                        
{'loss': 0.5383, 'learning_rate': 0.00019952264978080855, 'epoch': 3.51}                                                                                                      
{'loss': 0.5457, 'learning_rate': 0.00019937652216268874, 'epoch': 3.51}                                                                                                      
{'loss': 0.6674, 'learning_rate': 0.0001992303945445689, 'epoch': 3.52}                                                                                                       
 35%|█████████████████████████████████████████████                                                                                   | 7400/21030 [8:57:18<2:12:55,  1.71it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5185566544532776, 'eval_wer': 0.6904732593517525, 'eval_cer': 0.2610088396280438, 'eval_runtime': 363.2511, 'eval_samples_per_second': 28.735, 'eval_steps_per_second': 3.593, 'epoch': 3.52}                                                                                                                                               
{'loss': 0.8152, 'learning_rate': 0.00019908426692644908, 'epoch': 3.52}                                                                                                      
{'loss': 0.5695, 'learning_rate': 0.00019893813930832923, 'epoch': 3.53}                                                                                                      
{'loss': 0.5027, 'learning_rate': 0.00019879201169020942, 'epoch': 3.53}                                                                                                      
{'loss': 0.5947, 'learning_rate': 0.00019864588407208963, 'epoch': 3.54}                                                                                                      
{'loss': 0.6648, 'learning_rate': 0.0001984997564539698, 'epoch': 3.54}                                                                                                       
{'loss': 0.7898, 'learning_rate': 0.00019835362883584997, 'epoch': 3.55}                                                                                                      
{'loss': 0.6459, 'learning_rate': 0.00019820750121773013, 'epoch': 3.55}                                                                                                      
{'loss': 0.5506, 'learning_rate': 0.00019806137359961031, 'epoch': 3.56}                                                                                                      
{'loss': 0.5964, 'learning_rate': 0.00019791524598149047, 'epoch': 3.56}                                                                                                      
{'loss': 0.6243, 'learning_rate': 0.00019776911836337066, 'epoch': 3.57}                                                                                                      
 36%|█████████████████████████████████████████████▋                                                                                  | 7500/21030 [9:04:43<2:17:06,  1.64it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5812639594078064, 'eval_wer': 0.7409120486801146, 'eval_cer': 0.2776038532755458, 'eval_runtime': 364.6065, 'eval_samples_per_second': 28.628, 'eval_steps_per_second': 3.579, 'epoch': 3.57}                                                                                                                                               
{'loss': 0.7648, 'learning_rate': 0.0001976229907452508, 'epoch': 3.57}                                                                                                       
{'loss': 0.5815, 'learning_rate': 0.00019747686312713102, 'epoch': 3.58}                                                                                                      
{'loss': 0.5469, 'learning_rate': 0.00019733073550901118, 'epoch': 3.58}                                                                                                      
{'loss': 0.5758, 'learning_rate': 0.00019718460789089137, 'epoch': 3.59}                                                                                                      
{'loss': 0.5449, 'learning_rate': 0.00019703848027277155, 'epoch': 3.59}                                                                                                      
{'loss': 0.8064, 'learning_rate': 0.0001968923526546517, 'epoch': 3.59}                                                                                                       
{'loss': 0.543, 'learning_rate': 0.0001967462250365319, 'epoch': 3.6}                                                                                                         
{'loss': 0.5554, 'learning_rate': 0.00019660009741841205, 'epoch': 3.6}                                                                                                       
{'loss': 0.4849, 'learning_rate': 0.00019645396980029226, 'epoch': 3.61}                                                                                                      
{'loss': 0.6435, 'learning_rate': 0.00019630784218217242, 'epoch': 3.61}                                                                                                      
 36%|██████████████████████████████████████████████▎                                                                                 | 7600/21030 [9:12:07<2:12:48,  1.69it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.586216390132904, 'eval_wer': 0.7382916234325813, 'eval_cer': 0.2781067324769853, 'eval_runtime': 364.2786, 'eval_samples_per_second': 28.654, 'eval_steps_per_second': 3.582, 'epoch': 3.61}                                                                                                                                                
{'loss': 0.7296, 'learning_rate': 0.0001961617145640526, 'epoch': 3.62}                                                                                                       
{'loss': 0.6002, 'learning_rate': 0.00019601558694593276, 'epoch': 3.62}                                                                                                      
{'loss': 0.4766, 'learning_rate': 0.00019586945932781295, 'epoch': 3.63}                                                                                                      
{'loss': 0.5683, 'learning_rate': 0.0001957233317096931, 'epoch': 3.63}                                                                                                       
{'loss': 0.5504, 'learning_rate': 0.0001955772040915733, 'epoch': 3.64}                                                                                                       
{'loss': 0.7382, 'learning_rate': 0.00019543107647345344, 'epoch': 3.64}                                                                                                      
{'loss': 0.5312, 'learning_rate': 0.00019528494885533366, 'epoch': 3.65}                                                                                                      
{'loss': 0.5802, 'learning_rate': 0.00019513882123721384, 'epoch': 3.65}                                                                                                      
{'loss': 0.4823, 'learning_rate': 0.000194992693619094, 'epoch': 3.66}                                                                                                        
{'loss': 0.6431, 'learning_rate': 0.00019484656600097418, 'epoch': 3.66}                                                                                                      
 37%|██████████████████████████████████████████████▊                                                                                 | 7700/21030 [9:19:32<2:14:07,  1.66it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5994246602058411, 'eval_wer': 0.7302017199838202, 'eval_cer': 0.27596731575872263, 'eval_runtime': 364.9163, 'eval_samples_per_second': 28.604, 'eval_steps_per_second': 3.576, 'epoch': 3.66}                                                                                                                                              
{'loss': 0.7475, 'learning_rate': 0.00019470043838285434, 'epoch': 3.67}                                                                                                      
{'loss': 0.5237, 'learning_rate': 0.00019455431076473452, 'epoch': 3.67}                                                                                                      
{'loss': 0.5098, 'learning_rate': 0.00019440818314661468, 'epoch': 3.68}                                                                                                      
{'loss': 0.5924, 'learning_rate': 0.00019426205552849487, 'epoch': 3.68}                                                                                                      
{'loss': 0.6389, 'learning_rate': 0.00019411592791037505, 'epoch': 3.69}                                                                                                      
{'loss': 0.6668, 'learning_rate': 0.00019396980029225523, 'epoch': 3.69}                                                                                                      
{'loss': 0.5831, 'learning_rate': 0.0001938236726741354, 'epoch': 3.69}                                                                                                       
{'loss': 0.5425, 'learning_rate': 0.00019367754505601558, 'epoch': 3.7}                                                                                                       
{'loss': 0.5324, 'learning_rate': 0.00019353141743789573, 'epoch': 3.7}                                                                                                       
{'loss': 0.6274, 'learning_rate': 0.00019338528981977592, 'epoch': 3.71}                                                                                                      
 37%|███████████████████████████████████████████████▍                                                                                | 7800/21030 [9:26:57<2:13:22,  1.65it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5760620832443237, 'eval_wer': 0.7528886231336065, 'eval_cer': 0.2758626703757641, 'eval_runtime': 364.218, 'eval_samples_per_second': 28.659, 'eval_steps_per_second': 3.583, 'epoch': 3.71}                                                                                                                                                
{'loss': 0.7278, 'learning_rate': 0.00019323916220165608, 'epoch': 3.71}                                                                                                      
{'loss': 0.5545, 'learning_rate': 0.0001930930345835363, 'epoch': 3.72}                                                                                                       
{'loss': 0.578, 'learning_rate': 0.00019294690696541647, 'epoch': 3.72}                                                                                                       
{'loss': 0.5826, 'learning_rate': 0.00019280077934729663, 'epoch': 3.73}                                                                                                      
{'loss': 0.6337, 'learning_rate': 0.0001926546517291768, 'epoch': 3.73}                                                                                                       
{'loss': 0.8338, 'learning_rate': 0.00019250852411105697, 'epoch': 3.74}                                                                                                      
{'loss': 0.5818, 'learning_rate': 0.00019236239649293716, 'epoch': 3.74}                                                                                                      
{'loss': 0.5158, 'learning_rate': 0.0001922162688748173, 'epoch': 3.75}                                                                                                       
{'loss': 0.5264, 'learning_rate': 0.0001920701412566975, 'epoch': 3.75}                                                                                                       
{'loss': 0.5735, 'learning_rate': 0.00019192401363857768, 'epoch': 3.76}                                                                                                      
 38%|████████████████████████████████████████████████                                                                                | 7900/21030 [9:34:23<2:11:52,  1.66it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5191118717193604, 'eval_wer': 0.7037512530556972, 'eval_cer': 0.26589229083277377, 'eval_runtime': 365.7854, 'eval_samples_per_second': 28.536, 'eval_steps_per_second': 3.568, 'epoch': 3.76}                                                                                                                                              
{'loss': 0.8241, 'learning_rate': 0.00019177788602045787, 'epoch': 3.76}                                                                                                      
{'loss': 0.5536, 'learning_rate': 0.00019163175840233802, 'epoch': 3.77}                                                                                                      
{'loss': 0.4954, 'learning_rate': 0.0001914856307842182, 'epoch': 3.77}                                                                                                       
{'loss': 0.6422, 'learning_rate': 0.00019133950316609836, 'epoch': 3.78}                                                                                                      
{'loss': 0.618, 'learning_rate': 0.00019119337554797855, 'epoch': 3.78}                                                                                                       
{'loss': 0.7424, 'learning_rate': 0.0001910472479298587, 'epoch': 3.78}                                                                                                       
{'loss': 0.5389, 'learning_rate': 0.0001909011203117389, 'epoch': 3.79}                                                                                                       
{'loss': 0.5155, 'learning_rate': 0.0001907549926936191, 'epoch': 3.79}                                                                                                       
{'loss': 0.5504, 'learning_rate': 0.00019060886507549926, 'epoch': 3.8}                                                                                                       
{'loss': 0.6326, 'learning_rate': 0.00019046273745737944, 'epoch': 3.8}                                                                                                       
 38%|████████████████████████████████████████████████▋                                                                               | 8000/21030 [9:41:50<2:10:52,  1.66it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5633363127708435, 'eval_wer': 0.7196848455004309, 'eval_cer': 0.2673834875399324, 'eval_runtime': 365.2508, 'eval_samples_per_second': 28.578, 'eval_steps_per_second': 3.573, 'epoch': 3.8}                                                                                                                                                
{'loss': 0.7199, 'learning_rate': 0.0001903166098392596, 'epoch': 3.81}                                                                                                       
{'loss': 0.5176, 'learning_rate': 0.00019017048222113979, 'epoch': 3.81}                                                                                                      
{'loss': 0.4994, 'learning_rate': 0.00019002435460301994, 'epoch': 3.82}                                                                                                      
{'loss': 0.575, 'learning_rate': 0.00018987822698490013, 'epoch': 3.82}                                                                                                       
{'loss': 0.6864, 'learning_rate': 0.0001897320993667803, 'epoch': 3.83}                                                                                                       
{'loss': 0.7729, 'learning_rate': 0.0001895859717486605, 'epoch': 3.83}                                                                                                       
{'loss': 0.5196, 'learning_rate': 0.00018943984413054065, 'epoch': 3.84}                                                                                                      
{'loss': 0.5178, 'learning_rate': 0.00018929371651242084, 'epoch': 3.84}                                                                                                      
{'loss': 0.6311, 'learning_rate': 0.000189147588894301, 'epoch': 3.85}                                                                                                        
{'loss': 0.6406, 'learning_rate': 0.00018900146127618118, 'epoch': 3.85}                                                                                                      
 39%|█████████████████████████████████████████████████▎                                                                              | 8100/21030 [9:49:16<2:11:43,  1.64it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5268277525901794, 'eval_wer': 0.7219359490687818, 'eval_cer': 0.2659794953185725, 'eval_runtime': 362.7673, 'eval_samples_per_second': 28.773, 'eval_steps_per_second': 3.597, 'epoch': 3.85}                                                                                                                                               
{'loss': 0.7753, 'learning_rate': 0.00018885533365806134, 'epoch': 3.86}                                                                                                      
{'loss': 0.5771, 'learning_rate': 0.00018870920603994152, 'epoch': 3.86}                                                                                                      
{'loss': 0.499, 'learning_rate': 0.00018856307842182173, 'epoch': 3.87}                                                                                                       
{'loss': 0.5736, 'learning_rate': 0.0001884169508037019, 'epoch': 3.87}                                                                                                       
{'loss': 0.5785, 'learning_rate': 0.00018827082318558208, 'epoch': 3.88}                                                                                                      
{'loss': 0.8541, 'learning_rate': 0.00018812469556746223, 'epoch': 3.88}                                                                                                      
{'loss': 0.5261, 'learning_rate': 0.00018797856794934242, 'epoch': 3.88}                                                                                                      
{'loss': 0.4846, 'learning_rate': 0.00018783244033122257, 'epoch': 3.89}                                                                                                      
{'loss': 0.5652, 'learning_rate': 0.00018768631271310276, 'epoch': 3.89}                                                                                                      
{'loss': 0.5885, 'learning_rate': 0.00018754018509498292, 'epoch': 3.9}                                                                                                       
 39%|█████████████████████████████████████████████████▉                                                                              | 8200/21030 [9:56:39<2:09:56,  1.65it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5478364825248718, 'eval_wer': 0.717504088918591, 'eval_cer': 0.26506384821768564, 'eval_runtime': 362.8181, 'eval_samples_per_second': 28.769, 'eval_steps_per_second': 3.597, 'epoch': 3.9}                                                                                                                                                
{'loss': 0.729, 'learning_rate': 0.00018739405747686313, 'epoch': 3.9}                                                                                                        
{'loss': 0.6387, 'learning_rate': 0.00018724792985874329, 'epoch': 3.91}                                                                                                      
{'loss': 0.5708, 'learning_rate': 0.00018710180224062347, 'epoch': 3.91}                                                                                                      
{'loss': 0.5043, 'learning_rate': 0.00018695567462250363, 'epoch': 3.92}                                                                                                      
{'loss': 0.5985, 'learning_rate': 0.0001868095470043838, 'epoch': 3.92}                                                                                                       
{'loss': 0.8018, 'learning_rate': 0.00018666341938626397, 'epoch': 3.93}                                                                                                      
{'loss': 0.5544, 'learning_rate': 0.00018651729176814415, 'epoch': 3.93}                                                                                                      
{'loss': 0.5341, 'learning_rate': 0.00018637116415002437, 'epoch': 3.94}                                                                                                      
{'loss': 0.5495, 'learning_rate': 0.00018622503653190452, 'epoch': 3.94}                                                                                                      
{'loss': 0.5942, 'learning_rate': 0.0001860789089137847, 'epoch': 3.95}                                                                                                       
 39%|██████████████████████████████████████████████████                                                                             | 8300/21030 [10:04:04<2:06:53,  1.67it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5698009133338928, 'eval_wer': 0.7154112660698897, 'eval_cer': 0.26566555916969703, 'eval_runtime': 365.3962, 'eval_samples_per_second': 28.566, 'eval_steps_per_second': 3.571, 'epoch': 3.95}                                                                                                                                              
{'loss': 0.6463, 'learning_rate': 0.00018593278129566486, 'epoch': 3.95}                                                                                                      
{'loss': 0.5733, 'learning_rate': 0.00018578665367754505, 'epoch': 3.96}                                                                                                      
{'loss': 0.5005, 'learning_rate': 0.0001856405260594252, 'epoch': 3.96}                                                                                                       
{'loss': 0.5458, 'learning_rate': 0.0001854943984413054, 'epoch': 3.97}                                                                                                       
{'loss': 0.5641, 'learning_rate': 0.00018534827082318555, 'epoch': 3.97}                                                                                                      
{'loss': 0.684, 'learning_rate': 0.00018520214320506576, 'epoch': 3.98}                                                                                                       
{'loss': 0.5205, 'learning_rate': 0.00018505601558694592, 'epoch': 3.98}                                                                                                      
{'loss': 0.6809, 'learning_rate': 0.0001849098879688261, 'epoch': 3.98}                                                                                                       
{'loss': 0.5096, 'learning_rate': 0.00018476376035070626, 'epoch': 3.99}                                                                                                      
{'loss': 0.5668, 'learning_rate': 0.00018461763273258644, 'epoch': 3.99}                                                                                                      
 40%|██████████████████████████████████████████████████▋                                                                            | 8400/21030 [10:11:29<2:02:25,  1.72it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5443992614746094, 'eval_wer': 0.7395578691897786, 'eval_cer': 0.2643603986989091, 'eval_runtime': 370.0173, 'eval_samples_per_second': 28.209, 'eval_steps_per_second': 3.527, 'epoch': 3.99}                                                                                                                                               
{'loss': 0.7669, 'learning_rate': 0.0001844715051144666, 'epoch': 4.0}                                                                                                        
 40%|██████████████████████████████████████████████████▊                                                                            | 8416/21030 [10:17:54<6:37:43,  1.89s/it]Saving model checkpoint to ./20_band/checkpoint-8416
Configuration saved in ./20_band/checkpoint-8416/config.json
Model weights saved in ./20_band/checkpoint-8416/pytorch_model.bin
Feature extractor saved in ./20_band/checkpoint-8416/preprocessor_config.json
/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:154: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.
  warnings.warn(
{'loss': 0.6447, 'learning_rate': 0.00018432537749634678, 'epoch': 4.0}                                                                                                       
{'loss': 0.4821, 'learning_rate': 0.00018417924987822694, 'epoch': 4.01}                                                                                                      
{'loss': 0.4664, 'learning_rate': 0.00018403312226010715, 'epoch': 4.01}                                                                                                      
{'loss': 0.4765, 'learning_rate': 0.00018388699464198734, 'epoch': 4.02}                                                                                                      
{'loss': 0.6877, 'learning_rate': 0.0001837408670238675, 'epoch': 4.02}                                                                                                       
{'loss': 0.585, 'learning_rate': 0.00018359473940574768, 'epoch': 4.03}                                                                                                       
{'loss': 0.4674, 'learning_rate': 0.00018344861178762784, 'epoch': 4.03}                                                                                                      
{'loss': 0.5146, 'learning_rate': 0.00018330248416950802, 'epoch': 4.04}                                                                                                      
{'loss': 0.4992, 'learning_rate': 0.00018315635655138818, 'epoch': 4.04}                                                                                                      
 40%|███████████████████████████████████████████████████▎                                                                           | 8500/21030 [10:19:06<2:14:48,  1.55it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.586575448513031, 'eval_wer': 0.728038550148608, 'eval_cer': 0.2676625418944884, 'eval_runtime': 373.9859, 'eval_samples_per_second': 27.91, 'eval_steps_per_second': 3.489, 'epoch': 4.04}                                                                                                                                                  
{'loss': 0.6304, 'learning_rate': 0.0001830102289332684, 'epoch': 4.05}                                                                                                       
{'loss': 0.5307, 'learning_rate': 0.00018286410131514855, 'epoch': 4.05}                                                                                                      
{'loss': 0.503, 'learning_rate': 0.00018271797369702873, 'epoch': 4.06}                                                                                                       
{'loss': 0.4874, 'learning_rate': 0.0001825718460789089, 'epoch': 4.06}                                                                                                       
{'loss': 0.5411, 'learning_rate': 0.00018242571846078907, 'epoch': 4.07}                                                                                                      
{'loss': 0.6208, 'learning_rate': 0.00018227959084266923, 'epoch': 4.07}                                                                                                      
{'loss': 0.6006, 'learning_rate': 0.00018213346322454942, 'epoch': 4.08}                                                                                                      
{'loss': 0.4992, 'learning_rate': 0.00018198733560642957, 'epoch': 4.08}                                                                                                      
{'loss': 0.496, 'learning_rate': 0.00018184120798830978, 'epoch': 4.08}                                                                                                       
{'loss': 0.4952, 'learning_rate': 0.00018169508037018997, 'epoch': 4.09}                                                                                                      
 41%|███████████████████████████████████████████████████▉                                                                           | 8600/21030 [10:26:42<2:16:40,  1.52it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5796331167221069, 'eval_wer': 0.7076555108070558, 'eval_cer': 0.262715140733506, 'eval_runtime': 366.2133, 'eval_samples_per_second': 28.503, 'eval_steps_per_second': 3.563, 'epoch': 4.09}                                                                                                                                                
{'loss': 0.6369, 'learning_rate': 0.00018154895275207013, 'epoch': 4.09}                                                                                                      
{'loss': 0.651, 'learning_rate': 0.0001814028251339503, 'epoch': 4.1}                                                                                                         
{'loss': 0.5289, 'learning_rate': 0.00018125669751583047, 'epoch': 4.1}                                                                                                       
{'loss': 0.5206, 'learning_rate': 0.00018111056989771065, 'epoch': 4.11}                                                                                                      
{'loss': 0.5172, 'learning_rate': 0.0001809644422795908, 'epoch': 4.11}                                                                                                       
{'loss': 0.617, 'learning_rate': 0.000180818314661471, 'epoch': 4.12}                                                                                                         
{'loss': 0.5569, 'learning_rate': 0.00018067218704335118, 'epoch': 4.12}                                                                                                      
{'loss': 0.4875, 'learning_rate': 0.00018052605942523136, 'epoch': 4.13}                                                                                                      
{'loss': 0.5286, 'learning_rate': 0.00018037993180711152, 'epoch': 4.13}                                                                                                      
{'loss': 0.5236, 'learning_rate': 0.0001802338041889917, 'epoch': 4.14}                                                                                                       
 41%|████████████████████████████████████████████████████▌                                                                          | 8700/21030 [10:34:09<2:11:26,  1.56it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8
{'eval_loss': 0.5607408285140991, 'eval_wer': 0.7115949420516698, 'eval_cer': 0.2598373927021473, 'eval_runtime': 374.0536, 'eval_samples_per_second': 27.905, 'eval_steps_per_second': 3.489, 'epoch': 4.14}                                                                                                                                               
{'loss': 0.5787, 'learning_rate': 0.00018008767657087186, 'epoch': 4.14}                                                                                                      
{'loss': 0.6068, 'learning_rate': 0.00017994154895275205, 'epoch': 4.15}                                                                                                      
{'loss': 0.4935, 'learning_rate': 0.0001797954213346322, 'epoch': 4.15}                                                                                                       
{'loss': 0.5237, 'learning_rate': 0.00017964929371651242, 'epoch': 4.16}                                                                                                      
{'loss': 0.5425, 'learning_rate': 0.0001795031660983926, 'epoch': 4.16}                                                                                                       
{'loss': 0.6307, 'learning_rate': 0.00017935703848027276, 'epoch': 4.17}                                                                                                      
{'loss': 0.6016, 'learning_rate': 0.00017921091086215294, 'epoch': 4.17}                                                                                                      
{'loss': 0.5016, 'learning_rate': 0.0001790647832440331, 'epoch': 4.17}                                                                                                       
{'loss': 0.4633, 'learning_rate': 0.00017891865562591328, 'epoch': 4.18}                                                                                                      
 42%|█████████████████████████████████████████████████████▏                                                                         | 8797/21030 [10:41:43<2:19:15,  1.46it/s]{'loss': 0.4947, 'learning_rate': 0.00017877252800779344, 'epoch': 4.18}                                                                                                      
 42%|█████████████████████████████████████████████████████▏                                                                         | 8800/21030 [10:41:45<2:10:44,  1.56it/s]***** Running Evaluation *****
  Num examples = 10438
  Batch size = 8

^A0%|▋                                                                                                                                       | 6/1305 [00:01<06:17,  3.44it/s]
^CTraceback (most recent call last):                                                                                                        | 14/1305 [00:04<07:34,  2.84it/s]
  File "/home/or/Desktop/wav2vec2/main_with_aug.py", line 314, in <module>
    trainer.train()
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1501, in train
    return inner_training_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 1826, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2089, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2796, in evaluate
    output = eval_loop(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2974, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 3217, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2540, in compute_loss
    outputs = model(**inputs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1677, in forward
    outputs = self.wav2vec2(
  File "/home/or/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1302, in forward
    attention_mask = self._get_feature_vector_attention_mask(
  File "/home/or/anaconda3/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py", line 1117, in _get_feature_vector_attention_mask
    attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1
KeyboardInterrupt
 42%|████████████████████████████████████████████████████▋                                                                         | 8800/21030 [10:41:51<14:52:01,  4.38s/it]

(base) or@anidjar:~/Desktop/wav2vec2$                                                                                                                                         


